loss = loss_target_total*0.2 + loss_exa_total



==========
Args:Namespace(alpha=0.999, arch='resnet50_AB_cosine', batch_size=16, ckp_prefix='ABMT_IC3', data_dir='K:\\\\ws\\\\project\\\\data\\\\', dataset_source='dukemtmc-reid', dataset_target='market1501', dropout=0.0, epochs=25, eval_step=1, features=0, height=256, init='logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/', iters=200, lamda=10.0, logs_dir='logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/', lr=0.00035, momentum=0.9, nb_cl=100, nb_protos=4, nb_runs=1, num_instances=4, num_phase=5, print_freq=20, rr_gpu=False, seed=1, soft_ce_weight=0.5, soft_tri_weight=0.8, weight_decay=0.0005, width=128, workers=8)
==========
=> Market1501 loaded
Dataset statistics:
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------
Order name:./checkpoint/seed_1_market1501_order.pkl
Loading orders
[690, 602, 389, 537, 90, 382, 654, 84, 425, 85, 608, 692, 334, 684, 19, 175, 285, 207, 60, 120, 364, 59, 512, 738, 448, 363, 47, 541, 445, 681, 432, 521, 311, 527, 214, 544, 601, 378, 111, 435, 101, 729, 8, 598, 484, 69, 560, 718, 523, 487, 734, 554, 202, 737, 154, 81, 329, 379, 412, 524, 631, 56, 148, 286, 599, 262, 23, 180, 720, 676, 624, 474, 314, 652, 494, 185, 310, 273, 696, 50, 717, 686, 194, 117, 386, 667, 674, 361, 323, 181, 436, 61, 134, 233, 589, 703, 3, 403, 159, 255, 551, 65, 366, 161, 104, 682, 603, 57, 607, 247, 739, 201, 446, 320, 540, 248, 35, 578, 402, 274, 373, 41, 626, 433, 16, 582, 108, 429, 748, 358, 0, 242, 74, 491, 66, 464, 345, 482, 414, 496, 687, 556, 480, 289, 305, 13, 459, 585, 257, 92, 473, 733, 672, 245, 530, 195, 507, 628, 132, 741, 371, 502, 529, 301, 40, 710, 68, 76, 223, 623, 139, 187, 372, 118, 299, 597, 34, 368, 189, 216, 744, 660, 699, 352, 498, 119, 82, 353, 355, 518, 107, 17, 307, 172, 404, 103, 656, 625, 374, 383, 408, 45, 241, 335, 218, 618, 635, 11, 88, 604, 538, 147, 658, 713, 224, 350, 570, 596, 619, 528, 594, 173, 333, 121, 695, 49, 31, 385, 268, 707, 479, 265, 747, 647, 685, 399, 116, 427, 670, 488, 95, 426, 740, 354, 318, 346, 277, 669, 62, 533, 160, 516, 689, 558, 483, 135, 711, 552, 331, 610, 29, 501, 592, 341, 146, 419, 657, 576, 349, 165, 80, 392, 259, 9, 746, 339, 591, 179, 638, 298, 422, 5, 197, 590, 437, 261, 664, 428, 434, 38, 644, 526, 293, 162, 67, 124, 39, 4, 33, 509, 280, 522, 99, 550, 567, 236, 421, 449, 622, 397, 732, 481, 186, 441, 680, 260, 415, 153, 517, 304, 200, 716, 78, 312, 679, 549, 42, 133, 394, 612, 250, 531, 439, 519, 192, 220, 495, 388, 563, 447, 511, 157, 539, 272, 722, 731, 306, 565, 102, 52, 156, 396, 400, 97, 573, 128, 177, 347, 110, 705, 510, 375, 343, 543, 142, 605, 6, 267, 743, 237, 486, 58, 232, 688, 617, 203, 649, 284, 443, 2, 736, 411, 217, 360, 46, 721, 225, 18, 168, 228, 292, 377, 227, 629, 620, 328, 283, 698, 579, 730, 546, 93, 213, 258, 337, 291, 493, 106, 115, 158, 745, 467, 36, 105, 395, 362, 452, 204, 244, 221, 246, 365, 504, 559, 723, 575, 351, 249, 122, 406, 640, 164, 642, 613, 697, 239, 83, 525, 295, 636, 70, 506, 457, 98, 614, 693, 359, 30, 340, 587, 94, 127, 630, 1, 472, 27, 89, 308, 73, 453, 184, 726, 171, 417, 226, 678, 646, 112, 91, 294, 370, 574, 650, 125, 423, 728, 455, 460, 410, 315, 376, 12, 651, 438, 553, 150, 750, 555, 577, 238, 114, 659, 442, 600, 309, 409, 143, 205, 14, 708, 463, 54, 430, 208, 344, 440, 131, 380, 174, 593, 191, 270, 407, 535, 123, 138, 51, 275, 727, 342, 256, 182, 300, 571, 240, 230, 326, 458, 465, 163, 167, 145, 206, 500, 188, 324, 637, 671, 79, 634, 677, 229, 581, 100, 290, 444, 712, 53, 391, 725, 749, 251, 499, 475, 271, 44, 113, 24, 211, 169, 520, 32, 109, 136, 330, 222, 28, 287, 55, 662, 675, 48, 735, 545, 477, 547, 63, 413, 384, 666, 322, 451, 584, 234, 557, 462, 424, 661, 303, 663, 219, 714, 296, 21, 420, 325, 199, 450, 137, 536, 212, 724, 702, 691, 505, 611, 231, 405, 568, 317, 183, 643, 641, 609, 485, 278, 20, 170, 401, 694, 566, 176, 327, 198, 470, 632, 266, 615, 492, 130, 338, 639, 489, 572, 606, 193, 140, 416, 476, 152, 673, 10, 269, 96, 210, 742, 569, 548, 709, 532, 701, 332, 75, 77, 263, 149, 700, 514, 706, 469, 564, 461, 253, 369, 321, 151, 302, 190, 586, 348, 243, 87, 655, 683, 418, 288, 648, 166, 595, 704, 155, 356, 381, 279, 126, 22, 616, 665, 282, 471, 367, 25, 196, 64, 15, 466, 297, 621, 336, 26, 588, 43, 497, 515, 719, 561, 454, 387, 71, 542, 456, 633, 431, 627, 653, 264, 209, 316, 513, 313, 534, 319, 7, 393, 141, 86, 478, 503, 215, 580, 562, 398, 668, 490, 252, 468, 357, 254, 276, 178, 281, 390, 508, 583, 129, 144, 645, 715, 72, 235, 37]


 phase 1 have 318 old classes
=> Loaded checkpoint 'logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase1_model_best.pth.tar'
phase:1 input id:100,input image:1887
Computing original distance...
Computing Jaccard distance...
Time cost: 13.786575317382812
eps for cluster: 0.071
Clustering and labeling...

 Clustered into 114 classes 

in phase 1:epoch 0 : in_features: 2048 out_features: 318
###############################
Lamda for less forget is set to  16.70171752907624
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [0][20/200]	Time 1.343 (10.561)	Data 0.000 (0.000)	Loss_ce 1.451 / 2.012	Loss_ce_soft 1.784 / 3.109	Loss_tri_soft 0.457 / 0.454	Loss_lf 4.085 / 2.935	Loss_lf_tri 0.360 / 0.356	Prec 86.25% / 81.56%	
Epoch: [0][40/200]	Time 1.402 (7.044)	Data 0.000 (1.075)	Loss_ce 1.497 / 2.122	Loss_ce_soft 1.816 / 3.244	Loss_tri_soft 0.442 / 0.444	Loss_lf 4.161 / 3.009	Loss_lf_tri 0.358 / 0.357	Prec 84.53% / 79.06%	
Epoch: [0][60/200]	Time 1.354 (5.873)	Data 0.000 (1.437)	Loss_ce 1.493 / 2.148	Loss_ce_soft 1.780 / 3.279	Loss_tri_soft 0.448 / 0.449	Loss_lf 4.175 / 3.000	Loss_lf_tri 0.366 / 0.358	Prec 85.21% / 78.96%	
Epoch: [0][80/200]	Time 1.231 (4.731)	Data 0.001 (1.078)	Loss_ce 1.485 / 2.163	Loss_ce_soft 1.785 / 3.281	Loss_tri_soft 0.439 / 0.451	Loss_lf 4.191 / 3.031	Loss_lf_tri 0.363 / 0.365	Prec 84.92% / 77.73%	
Epoch: [0][100/200]	Time 1.248 (4.459)	Data 0.000 (1.287)	Loss_ce 1.485 / 2.175	Loss_ce_soft 1.766 / 3.299	Loss_tri_soft 0.436 / 0.447	Loss_lf 4.202 / 3.044	Loss_lf_tri 0.365 / 0.367	Prec 85.06% / 76.56%	
Epoch: [0][120/200]	Time 1.322 (4.634)	Data 0.001 (1.425)	Loss_ce 1.473 / 2.159	Loss_ce_soft 1.755 / 3.284	Loss_tri_soft 0.429 / 0.440	Loss_lf 4.208 / 3.015	Loss_lf_tri 0.365 / 0.362	Prec 85.83% / 77.19%	
Epoch: [0][140/200]	Time 1.316 (4.163)	Data 0.001 (1.221)	Loss_ce 1.469 / 2.151	Loss_ce_soft 1.752 / 3.258	Loss_tri_soft 0.423 / 0.437	Loss_lf 4.220 / 3.022	Loss_lf_tri 0.366 / 0.363	Prec 86.21% / 77.41%	
Epoch: [0][160/200]	Time 1.347 (4.073)	Data 0.000 (1.339)	Loss_ce 1.464 / 2.155	Loss_ce_soft 1.737 / 3.233	Loss_tri_soft 0.421 / 0.438	Loss_lf 4.225 / 2.997	Loss_lf_tri 0.367 / 0.366	Prec 86.37% / 77.38%	
Epoch: [0][180/200]	Time 1.261 (4.003)	Data 0.000 (1.428)	Loss_ce 1.463 / 2.164	Loss_ce_soft 1.733 / 3.240	Loss_tri_soft 0.415 / 0.435	Loss_lf 4.228 / 3.025	Loss_lf_tri 0.366 / 0.367	Prec 86.28% / 77.33%	
Epoch: [0][200/200]	Time 1.310 (3.944)	Data 0.000 (1.499)	Loss_ce 1.454 / 2.161	Loss_ce_soft 1.718 / 3.227	Loss_tri_soft 0.417 / 0.427	Loss_lf 4.226 / 3.033	Loss_lf_tri 0.367 / 0.361	Prec 86.94% / 77.34%	
Extract Features: [50/302]	Time 0.186 (1.046)	Data 0.013 (0.867)	
Extract Features: [100/302]	Time 0.176 (0.613)	Data 0.002 (0.435)	
Extract Features: [150/302]	Time 0.176 (0.466)	Data 0.000 (0.291)	
Extract Features: [200/302]	Time 0.182 (0.394)	Data 0.002 (0.219)	
Extract Features: [250/302]	Time 0.183 (0.353)	Data 0.000 (0.175)	
Extract Features: [300/302]	Time 0.178 (0.325)	Data 0.000 (0.146)	
Mean AP: 57.7%

 * Finished phase   1 epoch   0  model no.1 mAP: 57.7%  best: 57.7% *

Computing original distance...
Computing Jaccard distance...
Time cost: 11.256386756896973
Clustering and labeling...

 Clustered into 123 classes 

###############################
Lamda for less forget is set to  16.079072901316597
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [1][20/200]	Time 1.330 (1.302)	Data 0.001 (0.000)	Loss_ce 1.593 / 2.071	Loss_ce_soft 1.750 / 3.053	Loss_tri_soft 0.401 / 0.412	Loss_lf 4.264 / 2.901	Loss_lf_tri 0.368 / 0.372	Prec 83.12% / 81.25%	
Epoch: [1][40/200]	Time 1.321 (2.366)	Data 0.000 (1.067)	Loss_ce 1.533 / 2.127	Loss_ce_soft 1.683 / 3.092	Loss_tri_soft 0.395 / 0.396	Loss_lf 4.282 / 3.046	Loss_lf_tri 0.361 / 0.364	Prec 84.38% / 78.59%	
Epoch: [1][60/200]	Time 1.349 (2.020)	Data 0.001 (0.711)	Loss_ce 1.525 / 2.182	Loss_ce_soft 1.690 / 3.154	Loss_tri_soft 0.373 / 0.396	Loss_lf 4.270 / 3.122	Loss_lf_tri 0.354 / 0.367	Prec 84.90% / 77.40%	
Epoch: [1][80/200]	Time 1.344 (2.411)	Data 0.000 (1.098)	Loss_ce 1.507 / 2.195	Loss_ce_soft 1.685 / 3.133	Loss_tri_soft 0.366 / 0.392	Loss_lf 4.279 / 3.143	Loss_lf_tri 0.350 / 0.361	Prec 85.70% / 77.27%	
Epoch: [1][100/200]	Time 1.348 (2.631)	Data 0.000 (1.315)	Loss_ce 1.498 / 2.216	Loss_ce_soft 1.685 / 3.152	Loss_tri_soft 0.370 / 0.397	Loss_lf 4.269 / 3.137	Loss_lf_tri 0.353 / 0.365	Prec 86.25% / 77.06%	
Epoch: [1][120/200]	Time 1.317 (2.771)	Data 0.000 (1.096)	Loss_ce 1.486 / 2.220	Loss_ce_soft 1.670 / 3.157	Loss_tri_soft 0.366 / 0.395	Loss_lf 4.265 / 3.140	Loss_lf_tri 0.351 / 0.364	Prec 86.61% / 77.19%	
Epoch: [1][140/200]	Time 1.339 (2.876)	Data 0.000 (1.250)	Loss_ce 1.481 / 2.190	Loss_ce_soft 1.661 / 3.132	Loss_tri_soft 0.361 / 0.391	Loss_lf 4.271 / 3.120	Loss_lf_tri 0.349 / 0.360	Prec 86.70% / 77.99%	
Epoch: [1][160/200]	Time 1.257 (2.946)	Data 0.000 (1.364)	Loss_ce 1.474 / 2.172	Loss_ce_soft 1.659 / 3.109	Loss_tri_soft 0.361 / 0.387	Loss_lf 4.270 / 3.103	Loss_lf_tri 0.349 / 0.357	Prec 86.84% / 78.24%	
Epoch: [1][180/200]	Time 1.329 (2.764)	Data 0.000 (1.212)	Loss_ce 1.469 / 2.167	Loss_ce_soft 1.652 / 3.104	Loss_tri_soft 0.355 / 0.387	Loss_lf 4.274 / 3.103	Loss_lf_tri 0.346 / 0.357	Prec 86.98% / 78.51%	
Epoch: [1][200/200]	Time 1.352 (2.846)	Data 0.000 (1.315)	Loss_ce 1.468 / 2.156	Loss_ce_soft 1.644 / 3.085	Loss_tri_soft 0.361 / 0.391	Loss_lf 4.265 / 3.105	Loss_lf_tri 0.352 / 0.361	Prec 86.97% / 78.78%	
Extract Features: [50/302]	Time 0.164 (1.059)	Data 0.000 (0.889)	
Extract Features: [100/302]	Time 0.165 (0.614)	Data 0.001 (0.445)	
Extract Features: [150/302]	Time 0.163 (0.464)	Data 0.001 (0.297)	
Extract Features: [200/302]	Time 0.180 (0.390)	Data 0.000 (0.223)	
Extract Features: [250/302]	Time 0.192 (0.346)	Data 0.016 (0.178)	
Extract Features: [300/302]	Time 0.156 (0.317)	Data 0.000 (0.149)	
Mean AP: 59.0%

 * Finished phase   1 epoch   1  model no.1 mAP: 59.0%  best: 59.0% *

Computing original distance...
Computing Jaccard distance...
Time cost: 11.191407203674316
Clustering and labeling...

 Clustered into 138 classes 

###############################
Lamda for less forget is set to  15.180078478344427
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [2][20/200]	Time 1.494 (1.335)	Data 0.000 (0.000)	Loss_ce 1.552 / 2.178	Loss_ce_soft 1.664 / 3.141	Loss_tri_soft 0.378 / 0.395	Loss_lf 4.256 / 3.135	Loss_lf_tri 0.356 / 0.385	Prec 84.69% / 79.06%	
Epoch: [2][40/200]	Time 1.308 (2.399)	Data 0.000 (1.067)	Loss_ce 1.527 / 2.111	Loss_ce_soft 1.661 / 3.075	Loss_tri_soft 0.376 / 0.359	Loss_lf 4.254 / 3.191	Loss_lf_tri 0.365 / 0.349	Prec 85.78% / 80.31%	
Epoch: [2][60/200]	Time 1.422 (2.061)	Data 0.000 (0.711)	Loss_ce 1.520 / 2.097	Loss_ce_soft 1.681 / 3.007	Loss_tri_soft 0.364 / 0.347	Loss_lf 4.246 / 3.111	Loss_lf_tri 0.364 / 0.337	Prec 86.04% / 81.04%	
Epoch: [2][80/200]	Time 1.295 (2.440)	Data 0.000 (1.086)	Loss_ce 1.542 / 2.112	Loss_ce_soft 1.722 / 3.024	Loss_tri_soft 0.361 / 0.353	Loss_lf 4.252 / 3.110	Loss_lf_tri 0.363 / 0.343	Prec 85.31% / 80.94%	
Epoch: [2][100/200]	Time 1.329 (2.215)	Data 0.001 (0.869)	Loss_ce 1.533 / 2.127	Loss_ce_soft 1.713 / 3.028	Loss_tri_soft 0.369 / 0.349	Loss_lf 4.255 / 3.108	Loss_lf_tri 0.374 / 0.341	Prec 85.75% / 80.25%	
Epoch: [2][120/200]	Time 1.264 (2.788)	Data 0.000 (1.093)	Loss_ce 1.524 / 2.104	Loss_ce_soft 1.691 / 2.996	Loss_tri_soft 0.361 / 0.348	Loss_lf 4.258 / 3.093	Loss_lf_tri 0.367 / 0.341	Prec 85.99% / 81.46%	
Epoch: [2][140/200]	Time 1.313 (2.874)	Data 0.000 (1.239)	Loss_ce 1.521 / 2.086	Loss_ce_soft 1.692 / 2.992	Loss_tri_soft 0.365 / 0.345	Loss_lf 4.253 / 3.079	Loss_lf_tri 0.370 / 0.343	Prec 86.34% / 81.52%	
Epoch: [2][160/200]	Time 1.311 (2.682)	Data 0.001 (1.084)	Loss_ce 1.517 / 2.080	Loss_ce_soft 1.691 / 2.984	Loss_tri_soft 0.361 / 0.343	Loss_lf 4.249 / 3.088	Loss_lf_tri 0.368 / 0.343	Prec 86.56% / 81.45%	
Epoch: [2][180/200]	Time 1.360 (2.764)	Data 0.001 (1.199)	Loss_ce 1.520 / 2.083	Loss_ce_soft 1.685 / 2.987	Loss_tri_soft 0.360 / 0.342	Loss_lf 4.252 / 3.082	Loss_lf_tri 0.370 / 0.343	Prec 86.56% / 81.01%	
Epoch: [2][200/200]	Time 1.311 (2.620)	Data 0.001 (1.079)	Loss_ce 1.522 / 2.082	Loss_ce_soft 1.697 / 2.988	Loss_tri_soft 0.364 / 0.340	Loss_lf 4.264 / 3.073	Loss_lf_tri 0.375 / 0.342	Prec 86.56% / 80.88%	
Extract Features: [50/302]	Time 0.184 (1.072)	Data 0.001 (0.898)	
Extract Features: [100/302]	Time 0.183 (0.619)	Data 0.000 (0.449)	
Extract Features: [150/302]	Time 0.164 (0.470)	Data 0.000 (0.299)	
Extract Features: [200/302]	Time 0.164 (0.394)	Data 0.000 (0.225)	
Extract Features: [250/302]	Time 0.164 (0.350)	Data 0.001 (0.180)	
Extract Features: [300/302]	Time 0.158 (0.319)	Data 0.000 (0.150)	
Mean AP: 59.3%

 * Finished phase   1 epoch   2  model no.1 mAP: 59.3%  best: 59.3% *

Computing original distance...
Computing Jaccard distance...
Time cost: 11.118431329727173
Clustering and labeling...

 Clustered into 139 classes 

###############################
Lamda for less forget is set to  15.125375314922476
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [3][20/200]	Time 1.288 (1.249)	Data 0.000 (0.000)	Loss_ce 1.624 / 2.019	Loss_ce_soft 1.829 / 2.882	Loss_tri_soft 0.368 / 0.325	Loss_lf 4.335 / 3.128	Loss_lf_tri 0.367 / 0.323	Prec 83.12% / 84.38%	
Epoch: [3][40/200]	Time 1.370 (2.402)	Data 0.001 (1.148)	Loss_ce 1.568 / 2.057	Loss_ce_soft 1.692 / 2.898	Loss_tri_soft 0.313 / 0.360	Loss_lf 4.300 / 3.197	Loss_lf_tri 0.334 / 0.361	Prec 84.06% / 81.09%	
Epoch: [3][60/200]	Time 1.360 (2.045)	Data 0.001 (0.766)	Loss_ce 1.543 / 2.082	Loss_ce_soft 1.668 / 2.943	Loss_tri_soft 0.317 / 0.352	Loss_lf 4.288 / 3.199	Loss_lf_tri 0.341 / 0.358	Prec 85.42% / 80.21%	
Epoch: [3][80/200]	Time 1.327 (2.498)	Data 0.001 (1.203)	Loss_ce 1.540 / 2.109	Loss_ce_soft 1.653 / 2.978	Loss_tri_soft 0.322 / 0.358	Loss_lf 4.291 / 3.198	Loss_lf_tri 0.346 / 0.356	Prec 85.70% / 80.08%	
Epoch: [3][100/200]	Time 1.317 (2.267)	Data 0.000 (0.962)	Loss_ce 1.559 / 2.123	Loss_ce_soft 1.690 / 2.978	Loss_tri_soft 0.332 / 0.360	Loss_lf 4.288 / 3.169	Loss_lf_tri 0.355 / 0.356	Prec 85.12% / 79.38%	
Epoch: [3][120/200]	Time 1.341 (2.858)	Data 0.000 (1.175)	Loss_ce 1.544 / 2.136	Loss_ce_soft 1.682 / 3.005	Loss_tri_soft 0.329 / 0.358	Loss_lf 4.288 / 3.156	Loss_lf_tri 0.354 / 0.355	Prec 85.57% / 78.96%	
Epoch: [3][140/200]	Time 1.228 (2.941)	Data 0.000 (1.311)	Loss_ce 1.537 / 2.128	Loss_ce_soft 1.694 / 3.013	Loss_tri_soft 0.329 / 0.355	Loss_lf 4.284 / 3.172	Loss_lf_tri 0.353 / 0.355	Prec 85.89% / 79.06%	
Epoch: [3][160/200]	Time 1.361 (2.739)	Data 0.000 (1.147)	Loss_ce 1.517 / 2.115	Loss_ce_soft 1.663 / 3.000	Loss_tri_soft 0.326 / 0.350	Loss_lf 4.273 / 3.161	Loss_lf_tri 0.354 / 0.351	Prec 86.48% / 79.57%	
Epoch: [3][180/200]	Time 1.325 (2.830)	Data 0.000 (1.267)	Loss_ce 1.512 / 2.109	Loss_ce_soft 1.662 / 2.998	Loss_tri_soft 0.331 / 0.342	Loss_lf 4.275 / 3.147	Loss_lf_tri 0.358 / 0.348	Prec 86.67% / 79.69%	
Epoch: [3][200/200]	Time 1.300 (2.680)	Data 0.000 (1.140)	Loss_ce 1.506 / 2.109	Loss_ce_soft 1.652 / 2.990	Loss_tri_soft 0.327 / 0.339	Loss_lf 4.268 / 3.157	Loss_lf_tri 0.357 / 0.346	Prec 86.72% / 79.78%	
Extract Features: [50/302]	Time 0.169 (1.090)	Data 0.000 (0.913)	
Extract Features: [100/302]	Time 0.190 (0.633)	Data 0.001 (0.457)	
Extract Features: [150/302]	Time 0.169 (0.481)	Data 0.000 (0.305)	
Extract Features: [200/302]	Time 0.170 (0.404)	Data 0.000 (0.229)	
Extract Features: [250/302]	Time 0.170 (0.358)	Data 0.000 (0.183)	
Extract Features: [300/302]	Time 0.156 (0.327)	Data 0.000 (0.152)	
Mean AP: 60.3%

 * Finished phase   1 epoch   3  model no.1 mAP: 60.3%  best: 60.3% *

Computing original distance...
Computing Jaccard distance...
Time cost: 11.229395151138306
Clustering and labeling...

 Clustered into 136 classes 

###############################
Lamda for less forget is set to  15.291289331242304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [4][20/200]	Time 1.339 (1.333)	Data 0.000 (0.000)	Loss_ce 1.484 / 2.122	Loss_ce_soft 1.591 / 2.866	Loss_tri_soft 0.357 / 0.408	Loss_lf 4.318 / 3.068	Loss_lf_tri 0.386 / 0.396	Prec 87.19% / 80.00%	
Epoch: [4][40/200]	Time 1.322 (2.421)	Data 0.000 (1.089)	Loss_ce 1.471 / 2.098	Loss_ce_soft 1.539 / 2.884	Loss_tri_soft 0.320 / 0.396	Loss_lf 4.300 / 3.133	Loss_lf_tri 0.349 / 0.387	Prec 87.50% / 81.25%	
Epoch: [4][60/200]	Time 1.272 (2.057)	Data 0.000 (0.726)	Loss_ce 1.475 / 2.105	Loss_ce_soft 1.531 / 2.890	Loss_tri_soft 0.333 / 0.371	Loss_lf 4.280 / 3.135	Loss_lf_tri 0.362 / 0.372	Prec 87.60% / 80.10%	
Epoch: [4][80/200]	Time 1.353 (2.415)	Data 0.001 (1.088)	Loss_ce 1.490 / 2.120	Loss_ce_soft 1.593 / 2.893	Loss_tri_soft 0.330 / 0.365	Loss_lf 4.281 / 3.089	Loss_lf_tri 0.364 / 0.366	Prec 87.73% / 78.98%	
Epoch: [4][100/200]	Time 1.296 (2.199)	Data 0.000 (0.871)	Loss_ce 1.488 / 2.098	Loss_ce_soft 1.605 / 2.874	Loss_tri_soft 0.348 / 0.346	Loss_lf 4.290 / 3.102	Loss_lf_tri 0.376 / 0.354	Prec 88.31% / 80.06%	
Epoch: [4][120/200]	Time 1.311 (2.766)	Data 0.001 (1.090)	Loss_ce 1.480 / 2.082	Loss_ce_soft 1.598 / 2.856	Loss_tri_soft 0.346 / 0.337	Loss_lf 4.272 / 3.079	Loss_lf_tri 0.378 / 0.351	Prec 88.49% / 80.26%	
Epoch: [4][140/200]	Time 1.250 (2.870)	Data 0.000 (1.246)	Loss_ce 1.487 / 2.076	Loss_ce_soft 1.608 / 2.861	Loss_tri_soft 0.344 / 0.338	Loss_lf 4.274 / 3.098	Loss_lf_tri 0.381 / 0.349	Prec 88.17% / 80.58%	
Epoch: [4][160/200]	Time 1.272 (2.673)	Data 0.000 (1.090)	Loss_ce 1.479 / 2.064	Loss_ce_soft 1.606 / 2.840	Loss_tri_soft 0.349 / 0.336	Loss_lf 4.278 / 3.096	Loss_lf_tri 0.383 / 0.349	Prec 88.48% / 81.09%	
Epoch: [4][180/200]	Time 1.223 (2.760)	Data 0.000 (1.208)	Loss_ce 1.472 / 2.050	Loss_ce_soft 1.606 / 2.833	Loss_tri_soft 0.347 / 0.331	Loss_lf 4.275 / 3.093	Loss_lf_tri 0.384 / 0.347	Prec 88.54% / 81.46%	
Epoch: [4][200/200]	Time 1.375 (2.616)	Data 0.000 (1.088)	Loss_ce 1.463 / 2.044	Loss_ce_soft 1.598 / 2.828	Loss_tri_soft 0.346 / 0.332	Loss_lf 4.271 / 3.080	Loss_lf_tri 0.383 / 0.346	Prec 88.84% / 81.62%	
Extract Features: [50/302]	Time 0.163 (1.073)	Data 0.000 (0.902)	
Extract Features: [100/302]	Time 0.164 (0.620)	Data 0.001 (0.451)	
Extract Features: [150/302]	Time 0.165 (0.471)	Data 0.000 (0.301)	
Extract Features: [200/302]	Time 0.169 (0.395)	Data 0.000 (0.226)	
Extract Features: [250/302]	Time 0.165 (0.351)	Data 0.000 (0.181)	
Extract Features: [300/302]	Time 0.178 (0.321)	Data 0.000 (0.151)	
Mean AP: 60.2%

 * Finished phase   1 epoch   4  model no.1 mAP: 60.2%  best: 60.3%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.159418106079102
Clustering and labeling...

 Clustered into 139 classes 

###############################
Lamda for less forget is set to  15.125375314922476
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [5][20/200]	Time 1.344 (1.316)	Data 0.000 (0.000)	Loss_ce 1.492 / 2.015	Loss_ce_soft 1.493 / 2.888	Loss_tri_soft 0.361 / 0.361	Loss_lf 4.247 / 3.132	Loss_lf_tri 0.399 / 0.380	Prec 85.62% / 82.50%	
Epoch: [5][40/200]	Time 1.323 (2.418)	Data 0.000 (1.098)	Loss_ce 1.478 / 2.015	Loss_ce_soft 1.519 / 2.846	Loss_tri_soft 0.329 / 0.331	Loss_lf 4.271 / 3.212	Loss_lf_tri 0.380 / 0.347	Prec 87.19% / 83.12%	
Epoch: [5][60/200]	Time 1.341 (2.055)	Data 0.000 (0.732)	Loss_ce 1.485 / 2.043	Loss_ce_soft 1.571 / 2.833	Loss_tri_soft 0.327 / 0.331	Loss_lf 4.266 / 3.174	Loss_lf_tri 0.370 / 0.346	Prec 87.92% / 82.29%	
Epoch: [5][80/200]	Time 1.354 (2.430)	Data 0.001 (1.101)	Loss_ce 1.472 / 2.059	Loss_ce_soft 1.564 / 2.840	Loss_tri_soft 0.327 / 0.334	Loss_lf 4.280 / 3.126	Loss_lf_tri 0.371 / 0.350	Prec 88.12% / 82.34%	
Epoch: [5][100/200]	Time 1.292 (2.209)	Data 0.001 (0.881)	Loss_ce 1.467 / 2.053	Loss_ce_soft 1.573 / 2.851	Loss_tri_soft 0.324 / 0.331	Loss_lf 4.265 / 3.115	Loss_lf_tri 0.371 / 0.350	Prec 88.44% / 82.56%	
Epoch: [5][120/200]	Time 1.320 (2.798)	Data 0.000 (1.096)	Loss_ce 1.472 / 2.060	Loss_ce_soft 1.579 / 2.860	Loss_tri_soft 0.330 / 0.333	Loss_lf 4.262 / 3.135	Loss_lf_tri 0.375 / 0.351	Prec 87.86% / 82.34%	
Epoch: [5][140/200]	Time 1.249 (2.891)	Data 0.000 (1.243)	Loss_ce 1.465 / 2.041	Loss_ce_soft 1.580 / 2.859	Loss_tri_soft 0.321 / 0.332	Loss_lf 4.267 / 3.138	Loss_lf_tri 0.369 / 0.350	Prec 88.08% / 82.23%	
Epoch: [5][160/200]	Time 1.325 (2.695)	Data 0.000 (1.088)	Loss_ce 1.466 / 2.036	Loss_ce_soft 1.580 / 2.861	Loss_tri_soft 0.319 / 0.335	Loss_lf 4.267 / 3.123	Loss_lf_tri 0.369 / 0.350	Prec 88.20% / 82.23%	
Epoch: [5][180/200]	Time 1.336 (2.787)	Data 0.000 (1.210)	Loss_ce 1.459 / 2.030	Loss_ce_soft 1.570 / 2.853	Loss_tri_soft 0.319 / 0.334	Loss_lf 4.264 / 3.104	Loss_lf_tri 0.367 / 0.350	Prec 88.30% / 82.22%	
Epoch: [5][200/200]	Time 1.320 (2.640)	Data 0.000 (1.089)	Loss_ce 1.455 / 2.029	Loss_ce_soft 1.568 / 2.851	Loss_tri_soft 0.318 / 0.336	Loss_lf 4.264 / 3.111	Loss_lf_tri 0.368 / 0.350	Prec 88.31% / 82.22%	
Extract Features: [50/302]	Time 0.181 (1.060)	Data 0.000 (0.885)	
Extract Features: [100/302]	Time 0.164 (0.613)	Data 0.000 (0.443)	
Extract Features: [150/302]	Time 0.162 (0.465)	Data 0.000 (0.295)	
Extract Features: [200/302]	Time 0.162 (0.392)	Data 0.001 (0.222)	
Extract Features: [250/302]	Time 0.166 (0.347)	Data 0.001 (0.177)	
Extract Features: [300/302]	Time 0.177 (0.317)	Data 0.000 (0.148)	
Mean AP: 60.7%

 * Finished phase   1 epoch   5  model no.1 mAP: 60.7%  best: 60.7% *

Computing original distance...
Computing Jaccard distance...
Time cost: 11.388343811035156
Clustering and labeling...

 Clustered into 143 classes 

###############################
Lamda for less forget is set to  14.912331218747202
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [6][20/200]	Time 1.339 (1.283)	Data 0.000 (0.000)	Loss_ce 1.518 / 2.059	Loss_ce_soft 1.646 / 2.877	Loss_tri_soft 0.374 / 0.336	Loss_lf 4.201 / 3.138	Loss_lf_tri 0.411 / 0.363	Prec 88.44% / 81.56%	
Epoch: [6][40/200]	Time 1.321 (2.366)	Data 0.000 (1.061)	Loss_ce 1.509 / 2.063	Loss_ce_soft 1.610 / 2.860	Loss_tri_soft 0.322 / 0.330	Loss_lf 4.270 / 3.137	Loss_lf_tri 0.383 / 0.358	Prec 89.06% / 81.25%	
Epoch: [6][60/200]	Time 1.291 (2.020)	Data 0.001 (0.708)	Loss_ce 1.507 / 2.067	Loss_ce_soft 1.602 / 2.856	Loss_tri_soft 0.329 / 0.336	Loss_lf 4.267 / 3.106	Loss_lf_tri 0.388 / 0.362	Prec 89.06% / 80.94%	
Epoch: [6][80/200]	Time 1.298 (2.368)	Data 0.000 (1.058)	Loss_ce 1.498 / 2.084	Loss_ce_soft 1.569 / 2.876	Loss_tri_soft 0.337 / 0.340	Loss_lf 4.254 / 3.146	Loss_lf_tri 0.391 / 0.368	Prec 88.67% / 80.31%	
Epoch: [6][100/200]	Time 1.367 (2.160)	Data 0.000 (0.846)	Loss_ce 1.491 / 2.090	Loss_ce_soft 1.583 / 2.866	Loss_tri_soft 0.331 / 0.328	Loss_lf 4.260 / 3.140	Loss_lf_tri 0.390 / 0.361	Prec 88.44% / 80.44%	
Epoch: [6][120/200]	Time 1.345 (2.760)	Data 0.001 (1.077)	Loss_ce 1.498 / 2.077	Loss_ce_soft 1.601 / 2.843	Loss_tri_soft 0.331 / 0.318	Loss_lf 4.272 / 3.118	Loss_lf_tri 0.391 / 0.353	Prec 87.97% / 81.25%	
Epoch: [6][140/200]	Time 1.326 (2.554)	Data 0.000 (0.923)	Loss_ce 1.495 / 2.068	Loss_ce_soft 1.598 / 2.852	Loss_tri_soft 0.329 / 0.320	Loss_lf 4.276 / 3.099	Loss_lf_tri 0.389 / 0.356	Prec 88.26% / 81.21%	
Epoch: [6][160/200]	Time 1.321 (2.677)	Data 0.000 (1.082)	Loss_ce 1.497 / 2.062	Loss_ce_soft 1.600 / 2.870	Loss_tri_soft 0.332 / 0.321	Loss_lf 4.271 / 3.091	Loss_lf_tri 0.393 / 0.357	Prec 88.09% / 81.33%	
Epoch: [6][180/200]	Time 1.222 (2.761)	Data 0.001 (1.196)	Loss_ce 1.501 / 2.048	Loss_ce_soft 1.604 / 2.858	Loss_tri_soft 0.332 / 0.320	Loss_lf 4.274 / 3.093	Loss_lf_tri 0.390 / 0.356	Prec 87.99% / 81.98%	
Epoch: [6][200/200]	Time 1.396 (2.618)	Data 0.000 (1.076)	Loss_ce 1.500 / 2.039	Loss_ce_soft 1.611 / 2.841	Loss_tri_soft 0.331 / 0.320	Loss_lf 4.268 / 3.094	Loss_lf_tri 0.389 / 0.356	Prec 87.88% / 82.00%	
Extract Features: [50/302]	Time 0.164 (1.064)	Data 0.001 (0.895)	
Extract Features: [100/302]	Time 0.164 (0.618)	Data 0.001 (0.448)	
Extract Features: [150/302]	Time 0.167 (0.468)	Data 0.000 (0.299)	
Extract Features: [200/302]	Time 0.167 (0.393)	Data 0.000 (0.224)	
Extract Features: [250/302]	Time 0.167 (0.349)	Data 0.001 (0.179)	
Extract Features: [300/302]	Time 0.156 (0.319)	Data 0.000 (0.150)	
Mean AP: 60.0%

 * Finished phase   1 epoch   6  model no.1 mAP: 60.0%  best: 60.7%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.18940806388855
Clustering and labeling...

 Clustered into 151 classes 

###############################
Lamda for less forget is set to  14.511927042611061
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [7][20/200]	Time 1.245 (1.269)	Data 0.000 (0.000)	Loss_ce 1.493 / 2.132	Loss_ce_soft 1.570 / 2.966	Loss_tri_soft 0.329 / 0.312	Loss_lf 4.273 / 3.330	Loss_lf_tri 0.371 / 0.355	Prec 88.75% / 81.56%	
Epoch: [7][40/200]	Time 1.327 (2.344)	Data 0.000 (1.076)	Loss_ce 1.462 / 2.101	Loss_ce_soft 1.533 / 2.905	Loss_tri_soft 0.318 / 0.337	Loss_lf 4.279 / 3.124	Loss_lf_tri 0.361 / 0.371	Prec 89.84% / 81.72%	
Epoch: [7][60/200]	Time 1.687 (2.009)	Data 0.001 (0.717)	Loss_ce 1.498 / 2.053	Loss_ce_soft 1.592 / 2.853	Loss_tri_soft 0.323 / 0.318	Loss_lf 4.302 / 3.101	Loss_lf_tri 0.370 / 0.354	Prec 88.44% / 82.29%	
Epoch: [7][80/200]	Time 1.316 (2.395)	Data 0.000 (1.094)	Loss_ce 1.502 / 2.069	Loss_ce_soft 1.618 / 2.884	Loss_tri_soft 0.327 / 0.318	Loss_lf 4.290 / 3.174	Loss_lf_tri 0.381 / 0.356	Prec 88.44% / 82.27%	
Epoch: [7][100/200]	Time 1.299 (2.185)	Data 0.000 (0.876)	Loss_ce 1.492 / 2.092	Loss_ce_soft 1.631 / 2.912	Loss_tri_soft 0.323 / 0.320	Loss_lf 4.285 / 3.182	Loss_lf_tri 0.383 / 0.357	Prec 88.81% / 81.50%	
Epoch: [7][120/200]	Time 1.319 (2.760)	Data 0.001 (1.083)	Loss_ce 1.494 / 2.069	Loss_ce_soft 1.635 / 2.868	Loss_tri_soft 0.321 / 0.315	Loss_lf 4.278 / 3.165	Loss_lf_tri 0.388 / 0.352	Prec 88.91% / 81.82%	
Epoch: [7][140/200]	Time 1.275 (2.555)	Data 0.001 (0.928)	Loss_ce 1.479 / 2.044	Loss_ce_soft 1.608 / 2.859	Loss_tri_soft 0.313 / 0.311	Loss_lf 4.265 / 3.168	Loss_lf_tri 0.385 / 0.349	Prec 89.29% / 82.54%	
Epoch: [7][160/200]	Time 1.329 (2.660)	Data 0.000 (1.075)	Loss_ce 1.477 / 2.021	Loss_ce_soft 1.610 / 2.841	Loss_tri_soft 0.310 / 0.307	Loss_lf 4.263 / 3.135	Loss_lf_tri 0.384 / 0.345	Prec 89.22% / 83.05%	
Epoch: [7][180/200]	Time 1.300 (2.513)	Data 0.000 (0.956)	Loss_ce 1.477 / 2.012	Loss_ce_soft 1.614 / 2.837	Loss_tri_soft 0.308 / 0.308	Loss_lf 4.263 / 3.142	Loss_lf_tri 0.382 / 0.346	Prec 89.41% / 83.37%	
Epoch: [7][200/200]	Time 1.329 (2.615)	Data 0.000 (1.079)	Loss_ce 1.477 / 2.014	Loss_ce_soft 1.608 / 2.841	Loss_tri_soft 0.307 / 0.309	Loss_lf 4.264 / 3.157	Loss_lf_tri 0.381 / 0.348	Prec 89.31% / 83.06%	
Extract Features: [50/302]	Time 0.164 (1.053)	Data 0.000 (0.882)	
Extract Features: [100/302]	Time 0.165 (0.613)	Data 0.000 (0.441)	
Extract Features: [150/302]	Time 0.163 (0.464)	Data 0.001 (0.294)	
Extract Features: [200/302]	Time 0.166 (0.390)	Data 0.001 (0.221)	
Extract Features: [250/302]	Time 0.181 (0.346)	Data 0.000 (0.177)	
Extract Features: [300/302]	Time 0.157 (0.316)	Data 0.000 (0.147)	
Mean AP: 61.0%

 * Finished phase   1 epoch   7  model no.1 mAP: 61.0%  best: 61.0% *

Computing original distance...
Computing Jaccard distance...
Time cost: 11.24539065361023
Clustering and labeling...

 Clustered into 151 classes 

###############################
Lamda for less forget is set to  14.511927042611061
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [8][20/200]	Time 1.346 (1.340)	Data 0.000 (0.000)	Loss_ce 1.473 / 1.976	Loss_ce_soft 1.547 / 2.805	Loss_tri_soft 0.247 / 0.349	Loss_lf 4.245 / 3.051	Loss_lf_tri 0.353 / 0.345	Prec 89.69% / 87.81%	
Epoch: [8][40/200]	Time 1.256 (2.411)	Data 0.001 (1.077)	Loss_ce 1.523 / 1.971	Loss_ce_soft 1.642 / 2.830	Loss_tri_soft 0.324 / 0.310	Loss_lf 4.256 / 3.207	Loss_lf_tri 0.400 / 0.339	Prec 87.97% / 86.72%	
Epoch: [8][60/200]	Time 1.287 (2.058)	Data 0.000 (0.718)	Loss_ce 1.489 / 1.990	Loss_ce_soft 1.623 / 2.810	Loss_tri_soft 0.333 / 0.290	Loss_lf 4.246 / 3.185	Loss_lf_tri 0.412 / 0.329	Prec 89.06% / 85.00%	
Epoch: [8][80/200]	Time 1.234 (2.406)	Data 0.001 (1.073)	Loss_ce 1.485 / 2.006	Loss_ce_soft 1.623 / 2.806	Loss_tri_soft 0.323 / 0.299	Loss_lf 4.252 / 3.147	Loss_lf_tri 0.409 / 0.339	Prec 89.06% / 84.77%	
Epoch: [8][100/200]	Time 1.409 (2.189)	Data 0.001 (0.858)	Loss_ce 1.483 / 2.035	Loss_ce_soft 1.629 / 2.811	Loss_tri_soft 0.313 / 0.306	Loss_lf 4.251 / 3.141	Loss_lf_tri 0.405 / 0.342	Prec 88.88% / 83.75%	
Epoch: [8][120/200]	Time 1.352 (2.788)	Data 0.000 (1.084)	Loss_ce 1.479 / 2.039	Loss_ce_soft 1.627 / 2.822	Loss_tri_soft 0.316 / 0.305	Loss_lf 4.247 / 3.143	Loss_lf_tri 0.407 / 0.341	Prec 89.38% / 83.33%	
Epoch: [8][140/200]	Time 1.338 (2.580)	Data 0.001 (0.930)	Loss_ce 1.471 / 2.034	Loss_ce_soft 1.619 / 2.830	Loss_tri_soft 0.313 / 0.309	Loss_lf 4.253 / 3.143	Loss_lf_tri 0.402 / 0.344	Prec 89.24% / 83.48%	
Epoch: [8][160/200]	Time 1.328 (2.697)	Data 0.000 (1.087)	Loss_ce 1.466 / 2.005	Loss_ce_soft 1.619 / 2.815	Loss_tri_soft 0.311 / 0.299	Loss_lf 4.257 / 3.153	Loss_lf_tri 0.398 / 0.341	Prec 89.38% / 84.30%	
Epoch: [8][180/200]	Time 1.399 (2.547)	Data 0.001 (0.966)	Loss_ce 1.465 / 2.002	Loss_ce_soft 1.615 / 2.823	Loss_tri_soft 0.311 / 0.298	Loss_lf 4.251 / 3.158	Loss_lf_tri 0.397 / 0.342	Prec 89.51% / 84.48%	
Epoch: [8][200/200]	Time 1.293 (2.646)	Data 0.000 (1.089)	Loss_ce 1.459 / 2.012	Loss_ce_soft 1.609 / 2.823	Loss_tri_soft 0.310 / 0.298	Loss_lf 4.250 / 3.149	Loss_lf_tri 0.397 / 0.344	Prec 89.41% / 84.06%	
Extract Features: [50/302]	Time 0.165 (1.070)	Data 0.000 (0.900)	
Extract Features: [100/302]	Time 0.165 (0.622)	Data 0.000 (0.450)	
Extract Features: [150/302]	Time 0.168 (0.470)	Data 0.000 (0.300)	
Extract Features: [200/302]	Time 0.166 (0.394)	Data 0.000 (0.225)	
Extract Features: [250/302]	Time 0.165 (0.349)	Data 0.000 (0.180)	
Extract Features: [300/302]	Time 0.157 (0.319)	Data 0.001 (0.150)	
Mean AP: 61.1%

 * Finished phase   1 epoch   8  model no.1 mAP: 61.1%  best: 61.1% *

Computing original distance...
Computing Jaccard distance...
Time cost: 11.167414665222168
Clustering and labeling...

 Clustered into 151 classes 

###############################
Lamda for less forget is set to  14.511927042611061
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [9][20/200]	Time 1.319 (1.305)	Data 0.000 (0.000)	Loss_ce 1.501 / 2.052	Loss_ce_soft 1.590 / 2.986	Loss_tri_soft 0.330 / 0.342	Loss_lf 4.239 / 3.263	Loss_lf_tri 0.393 / 0.372	Prec 88.12% / 80.94%	
Epoch: [9][40/200]	Time 1.351 (2.433)	Data 0.001 (1.123)	Loss_ce 1.454 / 1.995	Loss_ce_soft 1.523 / 2.833	Loss_tri_soft 0.321 / 0.323	Loss_lf 4.258 / 3.199	Loss_lf_tri 0.393 / 0.355	Prec 88.91% / 83.12%	
Epoch: [9][60/200]	Time 1.344 (2.068)	Data 0.000 (0.749)	Loss_ce 1.474 / 1.989	Loss_ce_soft 1.579 / 2.808	Loss_tri_soft 0.298 / 0.317	Loss_lf 4.277 / 3.186	Loss_lf_tri 0.378 / 0.355	Prec 88.54% / 82.81%	
Epoch: [9][80/200]	Time 1.368 (2.441)	Data 0.000 (1.119)	Loss_ce 1.475 / 1.967	Loss_ce_soft 1.603 / 2.783	Loss_tri_soft 0.296 / 0.300	Loss_lf 4.270 / 3.099	Loss_lf_tri 0.376 / 0.340	Prec 88.52% / 83.91%	
Epoch: [9][100/200]	Time 1.305 (2.218)	Data 0.000 (0.895)	Loss_ce 1.452 / 2.003	Loss_ce_soft 1.567 / 2.816	Loss_tri_soft 0.291 / 0.298	Loss_lf 4.244 / 3.143	Loss_lf_tri 0.375 / 0.343	Prec 89.00% / 83.06%	
Epoch: [9][120/200]	Time 1.234 (2.807)	Data 0.001 (1.116)	Loss_ce 1.456 / 2.003	Loss_ce_soft 1.583 / 2.785	Loss_tri_soft 0.291 / 0.307	Loss_lf 4.243 / 3.119	Loss_lf_tri 0.377 / 0.347	Prec 88.96% / 82.97%	
Epoch: [9][140/200]	Time 1.258 (2.585)	Data 0.001 (0.956)	Loss_ce 1.461 / 1.995	Loss_ce_soft 1.581 / 2.766	Loss_tri_soft 0.284 / 0.310	Loss_lf 4.244 / 3.120	Loss_lf_tri 0.377 / 0.351	Prec 88.48% / 83.21%	
Epoch: [9][160/200]	Time 1.349 (2.687)	Data 0.000 (1.101)	Loss_ce 1.464 / 2.010	Loss_ce_soft 1.581 / 2.798	Loss_tri_soft 0.292 / 0.310	Loss_lf 4.250 / 3.132	Loss_lf_tri 0.385 / 0.352	Prec 88.67% / 83.05%	
Epoch: [9][180/200]	Time 1.298 (2.536)	Data 0.000 (0.979)	Loss_ce 1.466 / 2.003	Loss_ce_soft 1.591 / 2.789	Loss_tri_soft 0.295 / 0.311	Loss_lf 4.252 / 3.135	Loss_lf_tri 0.386 / 0.352	Prec 88.47% / 83.23%	
Epoch: [9][200/200]	Time 1.322 (2.636)	Data 0.000 (1.100)	Loss_ce 1.464 / 1.989	Loss_ce_soft 1.587 / 2.773	Loss_tri_soft 0.297 / 0.307	Loss_lf 4.249 / 3.135	Loss_lf_tri 0.388 / 0.350	Prec 88.72% / 83.50%	
Extract Features: [50/302]	Time 0.173 (1.030)	Data 0.000 (0.859)	
Extract Features: [100/302]	Time 0.162 (0.599)	Data 0.000 (0.430)	
Extract Features: [150/302]	Time 0.167 (0.457)	Data 0.000 (0.287)	
Extract Features: [200/302]	Time 0.164 (0.385)	Data 0.000 (0.215)	
Extract Features: [250/302]	Time 0.163 (0.341)	Data 0.001 (0.172)	
Extract Features: [300/302]	Time 0.177 (0.313)	Data 0.000 (0.143)	
Mean AP: 60.9%

 * Finished phase   1 epoch   9  model no.1 mAP: 60.9%  best: 61.1%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.188408374786377
Clustering and labeling...

 Clustered into 150 classes 

###############################
Lamda for less forget is set to  14.560219778561036
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [10][20/200]	Time 1.321 (1.330)	Data 0.001 (0.000)	Loss_ce 1.500 / 2.012	Loss_ce_soft 1.572 / 2.848	Loss_tri_soft 0.286 / 0.288	Loss_lf 4.297 / 3.031	Loss_lf_tri 0.378 / 0.327	Prec 88.12% / 80.62%	
Epoch: [10][40/200]	Time 1.317 (2.403)	Data 0.001 (1.085)	Loss_ce 1.499 / 2.027	Loss_ce_soft 1.544 / 2.841	Loss_tri_soft 0.290 / 0.320	Loss_lf 4.264 / 3.164	Loss_lf_tri 0.378 / 0.350	Prec 87.66% / 79.38%	
Epoch: [10][60/200]	Time 1.319 (2.048)	Data 0.000 (0.723)	Loss_ce 1.506 / 2.037	Loss_ce_soft 1.576 / 2.829	Loss_tri_soft 0.289 / 0.309	Loss_lf 4.285 / 3.149	Loss_lf_tri 0.370 / 0.348	Prec 87.40% / 80.31%	
Epoch: [10][80/200]	Time 1.305 (2.428)	Data 0.000 (1.099)	Loss_ce 1.484 / 2.055	Loss_ce_soft 1.541 / 2.832	Loss_tri_soft 0.298 / 0.313	Loss_lf 4.259 / 3.172	Loss_lf_tri 0.380 / 0.354	Prec 87.81% / 80.47%	
Epoch: [10][100/200]	Time 1.318 (2.206)	Data 0.000 (0.879)	Loss_ce 1.473 / 2.055	Loss_ce_soft 1.542 / 2.821	Loss_tri_soft 0.293 / 0.306	Loss_lf 4.257 / 3.172	Loss_lf_tri 0.377 / 0.352	Prec 88.19% / 80.94%	
Epoch: [10][120/200]	Time 1.355 (2.797)	Data 0.000 (1.106)	Loss_ce 1.469 / 2.035	Loss_ce_soft 1.537 / 2.806	Loss_tri_soft 0.298 / 0.299	Loss_lf 4.252 / 3.171	Loss_lf_tri 0.383 / 0.350	Prec 88.28% / 81.93%	
Epoch: [10][140/200]	Time 1.342 (2.589)	Data 0.000 (0.948)	Loss_ce 1.458 / 2.033	Loss_ce_soft 1.530 / 2.805	Loss_tri_soft 0.291 / 0.309	Loss_lf 4.251 / 3.168	Loss_lf_tri 0.378 / 0.355	Prec 88.93% / 82.19%	
Epoch: [10][160/200]	Time 1.216 (2.689)	Data 0.001 (1.093)	Loss_ce 1.446 / 2.033	Loss_ce_soft 1.509 / 2.799	Loss_tri_soft 0.293 / 0.305	Loss_lf 4.248 / 3.173	Loss_lf_tri 0.380 / 0.354	Prec 89.18% / 82.03%	
Epoch: [10][180/200]	Time 1.230 (2.530)	Data 0.000 (0.971)	Loss_ce 1.442 / 2.023	Loss_ce_soft 1.508 / 2.792	Loss_tri_soft 0.286 / 0.306	Loss_lf 4.235 / 3.163	Loss_lf_tri 0.377 / 0.358	Prec 89.20% / 82.22%	
Epoch: [10][200/200]	Time 1.322 (2.611)	Data 0.000 (1.077)	Loss_ce 1.446 / 2.018	Loss_ce_soft 1.530 / 2.786	Loss_tri_soft 0.289 / 0.307	Loss_lf 4.240 / 3.164	Loss_lf_tri 0.381 / 0.359	Prec 89.28% / 82.41%	
Extract Features: [50/302]	Time 0.183 (0.996)	Data 0.001 (0.826)	
Extract Features: [100/302]	Time 0.165 (0.583)	Data 0.000 (0.413)	
Extract Features: [150/302]	Time 0.165 (0.445)	Data 0.000 (0.275)	
Extract Features: [200/302]	Time 0.164 (0.375)	Data 0.000 (0.207)	
Extract Features: [250/302]	Time 0.164 (0.336)	Data 0.000 (0.165)	
Extract Features: [300/302]	Time 0.181 (0.308)	Data 0.000 (0.138)	
Mean AP: 61.5%

 * Finished phase   1 epoch  10  model no.1 mAP: 61.5%  best: 61.5% *

Computing original distance...
Computing Jaccard distance...
Time cost: 11.020461082458496
Clustering and labeling...

 Clustered into 149 classes 

###############################
Lamda for less forget is set to  14.608997870899504
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [11][20/200]	Time 1.224 (1.246)	Data 0.000 (0.000)	Loss_ce 1.495 / 1.921	Loss_ce_soft 1.491 / 2.683	Loss_tri_soft 0.307 / 0.270	Loss_lf 4.234 / 3.019	Loss_lf_tri 0.400 / 0.337	Prec 88.44% / 85.94%	
Epoch: [11][40/200]	Time 1.312 (2.336)	Data 0.000 (1.077)	Loss_ce 1.548 / 1.973	Loss_ce_soft 1.559 / 2.731	Loss_tri_soft 0.307 / 0.302	Loss_lf 4.257 / 3.104	Loss_lf_tri 0.394 / 0.349	Prec 86.41% / 83.44%	
Epoch: [11][60/200]	Time 1.309 (2.009)	Data 0.001 (0.718)	Loss_ce 1.541 / 1.963	Loss_ce_soft 1.573 / 2.703	Loss_tri_soft 0.312 / 0.302	Loss_lf 4.252 / 3.103	Loss_lf_tri 0.399 / 0.350	Prec 86.98% / 83.33%	
Epoch: [11][80/200]	Time 1.242 (2.374)	Data 0.001 (1.079)	Loss_ce 1.518 / 2.000	Loss_ce_soft 1.563 / 2.724	Loss_tri_soft 0.308 / 0.317	Loss_lf 4.235 / 3.139	Loss_lf_tri 0.395 / 0.365	Prec 87.50% / 82.34%	
Epoch: [11][100/200]	Time 1.257 (2.154)	Data 0.000 (0.864)	Loss_ce 1.510 / 2.005	Loss_ce_soft 1.553 / 2.728	Loss_tri_soft 0.301 / 0.315	Loss_lf 4.240 / 3.145	Loss_lf_tri 0.387 / 0.368	Prec 87.88% / 82.44%	
Epoch: [11][120/200]	Time 1.313 (2.725)	Data 0.001 (1.075)	Loss_ce 1.505 / 1.996	Loss_ce_soft 1.555 / 2.722	Loss_tri_soft 0.301 / 0.308	Loss_lf 4.243 / 3.140	Loss_lf_tri 0.392 / 0.363	Prec 87.92% / 83.18%	
Epoch: [11][140/200]	Time 1.336 (2.525)	Data 0.000 (0.921)	Loss_ce 1.490 / 1.977	Loss_ce_soft 1.543 / 2.694	Loss_tri_soft 0.298 / 0.307	Loss_lf 4.239 / 3.124	Loss_lf_tri 0.391 / 0.362	Prec 88.26% / 83.93%	
Epoch: [11][160/200]	Time 1.224 (2.634)	Data 0.001 (1.072)	Loss_ce 1.480 / 1.960	Loss_ce_soft 1.535 / 2.683	Loss_tri_soft 0.291 / 0.306	Loss_lf 4.223 / 3.123	Loss_lf_tri 0.385 / 0.359	Prec 88.40% / 84.30%	
Epoch: [11][180/200]	Time 1.340 (2.484)	Data 0.000 (0.953)	Loss_ce 1.481 / 1.956	Loss_ce_soft 1.539 / 2.677	Loss_tri_soft 0.292 / 0.309	Loss_lf 4.226 / 3.113	Loss_lf_tri 0.387 / 0.359	Prec 88.44% / 84.51%	
Epoch: [11][200/200]	Time 1.311 (2.582)	Data 0.000 (1.072)	Loss_ce 1.481 / 1.952	Loss_ce_soft 1.545 / 2.673	Loss_tri_soft 0.292 / 0.305	Loss_lf 4.236 / 3.106	Loss_lf_tri 0.385 / 0.356	Prec 88.44% / 84.47%	
Extract Features: [50/302]	Time 0.164 (1.031)	Data 0.000 (0.861)	
Extract Features: [100/302]	Time 0.166 (0.600)	Data 0.001 (0.431)	
Extract Features: [150/302]	Time 0.163 (0.455)	Data 0.000 (0.287)	
Extract Features: [200/302]	Time 0.164 (0.383)	Data 0.000 (0.215)	
Extract Features: [250/302]	Time 0.162 (0.342)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.179 (0.313)	Data 0.000 (0.144)	
Mean AP: 61.8%

 * Finished phase   1 epoch  11  model no.1 mAP: 61.8%  best: 61.8% *

Computing original distance...
Computing Jaccard distance...
Time cost: 11.026459693908691
Clustering and labeling...

 Clustered into 147 classes 

###############################
Lamda for less forget is set to  14.708043058552857
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [12][20/200]	Time 1.355 (1.264)	Data 0.000 (0.000)	Loss_ce 1.409 / 2.005	Loss_ce_soft 1.452 / 2.727	Loss_tri_soft 0.323 / 0.302	Loss_lf 4.266 / 3.074	Loss_lf_tri 0.397 / 0.355	Prec 90.94% / 80.00%	
Epoch: [12][40/200]	Time 1.345 (2.324)	Data 0.000 (1.038)	Loss_ce 1.435 / 1.931	Loss_ce_soft 1.487 / 2.637	Loss_tri_soft 0.310 / 0.277	Loss_lf 4.262 / 3.117	Loss_lf_tri 0.395 / 0.335	Prec 90.78% / 82.81%	
Epoch: [12][60/200]	Time 1.235 (1.971)	Data 0.000 (0.692)	Loss_ce 1.444 / 1.948	Loss_ce_soft 1.536 / 2.670	Loss_tri_soft 0.296 / 0.282	Loss_lf 4.250 / 3.090	Loss_lf_tri 0.386 / 0.341	Prec 90.42% / 82.50%	
Epoch: [12][80/200]	Time 1.237 (2.305)	Data 0.000 (1.029)	Loss_ce 1.457 / 1.954	Loss_ce_soft 1.506 / 2.694	Loss_tri_soft 0.303 / 0.309	Loss_lf 4.254 / 3.110	Loss_lf_tri 0.386 / 0.353	Prec 90.08% / 83.28%	
Epoch: [12][100/200]	Time 1.218 (2.096)	Data 0.000 (0.823)	Loss_ce 1.451 / 1.951	Loss_ce_soft 1.510 / 2.678	Loss_tri_soft 0.303 / 0.310	Loss_lf 4.256 / 3.098	Loss_lf_tri 0.387 / 0.354	Prec 90.12% / 83.56%	
Epoch: [12][120/200]	Time 1.315 (2.680)	Data 0.000 (1.043)	Loss_ce 1.443 / 1.946	Loss_ce_soft 1.503 / 2.679	Loss_tri_soft 0.293 / 0.299	Loss_lf 4.257 / 3.101	Loss_lf_tri 0.380 / 0.352	Prec 89.95% / 84.06%	
Epoch: [12][140/200]	Time 1.294 (2.483)	Data 0.000 (0.894)	Loss_ce 1.440 / 1.932	Loss_ce_soft 1.507 / 2.665	Loss_tri_soft 0.296 / 0.293	Loss_lf 4.257 / 3.060	Loss_lf_tri 0.385 / 0.349	Prec 90.27% / 84.29%	
Epoch: [12][160/200]	Time 1.316 (2.604)	Data 0.000 (1.052)	Loss_ce 1.440 / 1.921	Loss_ce_soft 1.513 / 2.653	Loss_tri_soft 0.299 / 0.290	Loss_lf 4.256 / 3.056	Loss_lf_tri 0.387 / 0.349	Prec 90.51% / 84.77%	
Epoch: [12][180/200]	Time 1.278 (2.460)	Data 0.000 (0.935)	Loss_ce 1.440 / 1.917	Loss_ce_soft 1.504 / 2.643	Loss_tri_soft 0.299 / 0.290	Loss_lf 4.247 / 3.089	Loss_lf_tri 0.388 / 0.349	Prec 90.52% / 84.76%	
Epoch: [12][200/200]	Time 1.321 (2.564)	Data 0.001 (1.057)	Loss_ce 1.433 / 1.917	Loss_ce_soft 1.499 / 2.646	Loss_tri_soft 0.297 / 0.287	Loss_lf 4.243 / 3.096	Loss_lf_tri 0.387 / 0.345	Prec 90.62% / 84.88%	
Extract Features: [50/302]	Time 0.169 (1.032)	Data 0.000 (0.861)	
Extract Features: [100/302]	Time 0.167 (0.601)	Data 0.001 (0.431)	
Extract Features: [150/302]	Time 0.164 (0.456)	Data 0.000 (0.287)	
Extract Features: [200/302]	Time 0.166 (0.385)	Data 0.001 (0.215)	
Extract Features: [250/302]	Time 0.163 (0.344)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.180 (0.315)	Data 0.000 (0.144)	
Mean AP: 61.7%

 * Finished phase   1 epoch  12  model no.1 mAP: 61.7%  best: 61.8%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.271381855010986
Clustering and labeling...

 Clustered into 149 classes 

###############################
Lamda for less forget is set to  14.608997870899504
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [13][20/200]	Time 1.337 (1.276)	Data 0.000 (0.000)	Loss_ce 1.580 / 1.859	Loss_ce_soft 1.711 / 2.636	Loss_tri_soft 0.332 / 0.276	Loss_lf 4.239 / 3.061	Loss_lf_tri 0.431 / 0.341	Prec 86.25% / 84.06%	
Epoch: [13][40/200]	Time 1.234 (2.348)	Data 0.000 (1.054)	Loss_ce 1.532 / 1.951	Loss_ce_soft 1.588 / 2.726	Loss_tri_soft 0.305 / 0.320	Loss_lf 4.264 / 3.073	Loss_lf_tri 0.405 / 0.372	Prec 87.97% / 83.12%	
Epoch: [13][60/200]	Time 1.652 (1.990)	Data 0.000 (0.703)	Loss_ce 1.488 / 1.966	Loss_ce_soft 1.566 / 2.718	Loss_tri_soft 0.279 / 0.311	Loss_lf 4.267 / 3.081	Loss_lf_tri 0.380 / 0.366	Prec 88.75% / 82.81%	
Epoch: [13][80/200]	Time 1.274 (2.319)	Data 0.000 (1.038)	Loss_ce 1.477 / 1.958	Loss_ce_soft 1.533 / 2.694	Loss_tri_soft 0.296 / 0.302	Loss_lf 4.255 / 3.071	Loss_lf_tri 0.389 / 0.359	Prec 88.91% / 83.12%	
Epoch: [13][100/200]	Time 1.281 (2.107)	Data 0.001 (0.831)	Loss_ce 1.472 / 1.944	Loss_ce_soft 1.546 / 2.671	Loss_tri_soft 0.291 / 0.322	Loss_lf 4.257 / 3.056	Loss_lf_tri 0.387 / 0.372	Prec 88.88% / 83.69%	
Epoch: [13][120/200]	Time 1.282 (2.673)	Data 0.000 (1.048)	Loss_ce 1.460 / 1.940	Loss_ce_soft 1.546 / 2.671	Loss_tri_soft 0.294 / 0.327	Loss_lf 4.258 / 3.070	Loss_lf_tri 0.390 / 0.378	Prec 89.17% / 84.01%	
Epoch: [13][140/200]	Time 1.324 (2.480)	Data 0.000 (0.898)	Loss_ce 1.458 / 1.946	Loss_ce_soft 1.558 / 2.708	Loss_tri_soft 0.296 / 0.331	Loss_lf 4.252 / 3.111	Loss_lf_tri 0.395 / 0.381	Prec 89.29% / 83.62%	
Epoch: [13][160/200]	Time 1.224 (2.599)	Data 0.000 (1.053)	Loss_ce 1.461 / 1.938	Loss_ce_soft 1.563 / 2.706	Loss_tri_soft 0.292 / 0.327	Loss_lf 4.253 / 3.109	Loss_lf_tri 0.390 / 0.375	Prec 89.34% / 83.95%	
Epoch: [13][180/200]	Time 1.232 (2.449)	Data 0.000 (0.936)	Loss_ce 1.447 / 1.947	Loss_ce_soft 1.549 / 2.716	Loss_tri_soft 0.290 / 0.323	Loss_lf 4.251 / 3.105	Loss_lf_tri 0.387 / 0.373	Prec 89.76% / 83.72%	
Epoch: [13][200/200]	Time 1.227 (2.542)	Data 0.000 (1.055)	Loss_ce 1.444 / 1.941	Loss_ce_soft 1.537 / 2.711	Loss_tri_soft 0.294 / 0.315	Loss_lf 4.253 / 3.107	Loss_lf_tri 0.391 / 0.367	Prec 89.88% / 83.94%	
Extract Features: [50/302]	Time 0.167 (1.013)	Data 0.000 (0.840)	
Extract Features: [100/302]	Time 0.164 (0.591)	Data 0.000 (0.420)	
Extract Features: [150/302]	Time 0.642 (0.454)	Data 0.000 (0.280)	
Extract Features: [200/302]	Time 0.186 (0.383)	Data 0.000 (0.210)	
Extract Features: [250/302]	Time 0.183 (0.340)	Data 0.001 (0.168)	
Extract Features: [300/302]	Time 0.179 (0.312)	Data 0.000 (0.140)	
Mean AP: 61.3%

 * Finished phase   1 epoch  13  model no.1 mAP: 61.3%  best: 61.8%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.684248447418213
Clustering and labeling...

 Clustered into 148 classes 

###############################
Lamda for less forget is set to  14.65826950444236
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [14][20/200]	Time 1.290 (1.317)	Data 0.000 (0.000)	Loss_ce 1.502 / 1.899	Loss_ce_soft 1.640 / 2.600	Loss_tri_soft 0.337 / 0.262	Loss_lf 4.269 / 3.085	Loss_lf_tri 0.429 / 0.330	Prec 88.12% / 85.31%	
Epoch: [14][40/200]	Time 1.257 (2.390)	Data 0.000 (1.064)	Loss_ce 1.446 / 1.897	Loss_ce_soft 1.487 / 2.631	Loss_tri_soft 0.305 / 0.249	Loss_lf 4.253 / 3.155	Loss_lf_tri 0.396 / 0.314	Prec 89.84% / 85.16%	
Epoch: [14][60/200]	Time 1.360 (2.038)	Data 0.000 (0.710)	Loss_ce 1.483 / 1.931	Loss_ce_soft 1.530 / 2.664	Loss_tri_soft 0.311 / 0.265	Loss_lf 4.274 / 3.151	Loss_lf_tri 0.399 / 0.322	Prec 88.75% / 84.79%	
Epoch: [14][80/200]	Time 1.265 (2.401)	Data 0.000 (1.077)	Loss_ce 1.455 / 1.954	Loss_ce_soft 1.484 / 2.706	Loss_tri_soft 0.290 / 0.276	Loss_lf 4.246 / 3.146	Loss_lf_tri 0.386 / 0.332	Prec 89.77% / 84.38%	
Epoch: [14][100/200]	Time 1.290 (2.173)	Data 0.000 (0.861)	Loss_ce 1.444 / 1.950	Loss_ce_soft 1.472 / 2.711	Loss_tri_soft 0.298 / 0.288	Loss_lf 4.235 / 3.138	Loss_lf_tri 0.395 / 0.346	Prec 90.25% / 84.94%	
Epoch: [14][120/200]	Time 1.321 (2.733)	Data 0.001 (1.072)	Loss_ce 1.444 / 1.955	Loss_ce_soft 1.473 / 2.710	Loss_tri_soft 0.305 / 0.287	Loss_lf 4.223 / 3.132	Loss_lf_tri 0.399 / 0.348	Prec 89.95% / 84.64%	
Epoch: [14][140/200]	Time 1.336 (2.532)	Data 0.000 (0.919)	Loss_ce 1.435 / 1.951	Loss_ce_soft 1.467 / 2.708	Loss_tri_soft 0.304 / 0.286	Loss_lf 4.239 / 3.146	Loss_lf_tri 0.396 / 0.349	Prec 90.22% / 84.51%	
Epoch: [14][160/200]	Time 1.308 (2.641)	Data 0.000 (1.064)	Loss_ce 1.432 / 1.934	Loss_ce_soft 1.466 / 2.681	Loss_tri_soft 0.301 / 0.284	Loss_lf 4.241 / 3.119	Loss_lf_tri 0.393 / 0.348	Prec 90.12% / 85.12%	
Epoch: [14][180/200]	Time 1.350 (2.494)	Data 0.000 (0.945)	Loss_ce 1.425 / 1.927	Loss_ce_soft 1.471 / 2.669	Loss_tri_soft 0.304 / 0.279	Loss_lf 4.243 / 3.133	Loss_lf_tri 0.394 / 0.344	Prec 90.17% / 85.24%	
Epoch: [14][200/200]	Time 1.291 (2.583)	Data 0.000 (1.059)	Loss_ce 1.417 / 1.926	Loss_ce_soft 1.462 / 2.655	Loss_tri_soft 0.302 / 0.283	Loss_lf 4.242 / 3.116	Loss_lf_tri 0.395 / 0.345	Prec 90.56% / 85.00%	
Extract Features: [50/302]	Time 0.168 (1.041)	Data 0.001 (0.869)	
Extract Features: [100/302]	Time 0.165 (0.605)	Data 0.000 (0.435)	
Extract Features: [150/302]	Time 0.164 (0.460)	Data 0.000 (0.290)	
Extract Features: [200/302]	Time 0.166 (0.387)	Data 0.000 (0.217)	
Extract Features: [250/302]	Time 0.163 (0.343)	Data 0.000 (0.174)	
Extract Features: [300/302]	Time 0.178 (0.315)	Data 0.000 (0.145)	
Mean AP: 61.6%

 * Finished phase   1 epoch  14  model no.1 mAP: 61.6%  best: 61.8%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.102434158325195
Clustering and labeling...

 Clustered into 152 classes 

###############################
Lamda for less forget is set to  14.46411166701189
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [15][20/200]	Time 1.327 (1.345)	Data 0.000 (0.000)	Loss_ce 1.455 / 1.860	Loss_ce_soft 1.555 / 2.518	Loss_tri_soft 0.309 / 0.278	Loss_lf 4.327 / 2.999	Loss_lf_tri 0.419 / 0.333	Prec 88.75% / 84.06%	
Epoch: [15][40/200]	Time 1.315 (2.360)	Data 0.000 (1.029)	Loss_ce 1.466 / 1.923	Loss_ce_soft 1.503 / 2.628	Loss_tri_soft 0.309 / 0.269	Loss_lf 4.271 / 3.117	Loss_lf_tri 0.403 / 0.328	Prec 88.91% / 83.44%	
Epoch: [15][60/200]	Time 1.333 (2.012)	Data 0.001 (0.686)	Loss_ce 1.443 / 1.929	Loss_ce_soft 1.470 / 2.615	Loss_tri_soft 0.301 / 0.279	Loss_lf 4.262 / 3.095	Loss_lf_tri 0.409 / 0.339	Prec 89.79% / 84.27%	
Epoch: [15][80/200]	Time 1.316 (2.360)	Data 0.001 (1.034)	Loss_ce 1.426 / 1.933	Loss_ce_soft 1.441 / 2.592	Loss_tri_soft 0.301 / 0.279	Loss_lf 4.243 / 3.088	Loss_lf_tri 0.407 / 0.337	Prec 90.31% / 83.98%	
Epoch: [15][100/200]	Time 1.344 (2.153)	Data 0.000 (0.827)	Loss_ce 1.427 / 1.930	Loss_ce_soft 1.451 / 2.602	Loss_tri_soft 0.300 / 0.275	Loss_lf 4.250 / 3.086	Loss_lf_tri 0.406 / 0.336	Prec 90.06% / 84.31%	
Epoch: [15][120/200]	Time 1.210 (2.722)	Data 0.000 (1.041)	Loss_ce 1.423 / 1.935	Loss_ce_soft 1.455 / 2.620	Loss_tri_soft 0.302 / 0.275	Loss_lf 4.243 / 3.104	Loss_lf_tri 0.408 / 0.339	Prec 90.26% / 84.32%	
Epoch: [15][140/200]	Time 1.344 (2.516)	Data 0.000 (0.892)	Loss_ce 1.429 / 1.908	Loss_ce_soft 1.467 / 2.602	Loss_tri_soft 0.303 / 0.269	Loss_lf 4.251 / 3.095	Loss_lf_tri 0.404 / 0.336	Prec 90.13% / 85.00%	
Epoch: [15][160/200]	Time 1.244 (2.630)	Data 0.000 (1.048)	Loss_ce 1.425 / 1.910	Loss_ce_soft 1.481 / 2.602	Loss_tri_soft 0.298 / 0.274	Loss_lf 4.252 / 3.108	Loss_lf_tri 0.402 / 0.341	Prec 90.12% / 85.00%	
Epoch: [15][180/200]	Time 1.219 (2.476)	Data 0.000 (0.931)	Loss_ce 1.415 / 1.907	Loss_ce_soft 1.474 / 2.605	Loss_tri_soft 0.300 / 0.277	Loss_lf 4.249 / 3.113	Loss_lf_tri 0.402 / 0.343	Prec 90.28% / 85.35%	
Epoch: [15][200/200]	Time 1.232 (2.570)	Data 0.001 (1.052)	Loss_ce 1.406 / 1.906	Loss_ce_soft 1.474 / 2.604	Loss_tri_soft 0.306 / 0.272	Loss_lf 4.254 / 3.093	Loss_lf_tri 0.404 / 0.341	Prec 90.78% / 85.25%	
Extract Features: [50/302]	Time 0.165 (1.028)	Data 0.000 (0.858)	
Extract Features: [100/302]	Time 0.165 (0.599)	Data 0.000 (0.429)	
Extract Features: [150/302]	Time 0.167 (0.456)	Data 0.000 (0.286)	
Extract Features: [200/302]	Time 0.166 (0.384)	Data 0.000 (0.215)	
Extract Features: [250/302]	Time 0.166 (0.341)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.181 (0.313)	Data 0.000 (0.143)	
Mean AP: 62.0%

 * Finished phase   1 epoch  15  model no.1 mAP: 62.0%  best: 62.0% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.93848967552185
Clustering and labeling...

 Clustered into 153 classes 

###############################
Lamda for less forget is set to  14.416765838942586
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [16][20/200]	Time 1.217 (1.284)	Data 0.001 (0.000)	Loss_ce 1.377 / 1.841	Loss_ce_soft 1.348 / 2.495	Loss_tri_soft 0.246 / 0.282	Loss_lf 4.290 / 3.152	Loss_lf_tri 0.352 / 0.340	Prec 92.50% / 85.62%	
Epoch: [16][40/200]	Time 1.376 (2.368)	Data 0.000 (1.074)	Loss_ce 1.426 / 1.866	Loss_ce_soft 1.427 / 2.553	Loss_tri_soft 0.264 / 0.280	Loss_lf 4.240 / 3.138	Loss_lf_tri 0.366 / 0.342	Prec 91.41% / 85.78%	
Epoch: [16][60/200]	Time 1.323 (2.013)	Data 0.000 (0.716)	Loss_ce 1.427 / 1.870	Loss_ce_soft 1.443 / 2.563	Loss_tri_soft 0.256 / 0.278	Loss_lf 4.261 / 3.121	Loss_lf_tri 0.356 / 0.343	Prec 90.62% / 85.94%	
Epoch: [16][80/200]	Time 1.262 (2.383)	Data 0.000 (1.076)	Loss_ce 1.445 / 1.877	Loss_ce_soft 1.470 / 2.550	Loss_tri_soft 0.265 / 0.268	Loss_lf 4.249 / 3.073	Loss_lf_tri 0.370 / 0.336	Prec 90.23% / 86.09%	
Epoch: [16][100/200]	Time 1.332 (2.165)	Data 0.000 (0.861)	Loss_ce 1.435 / 1.891	Loss_ce_soft 1.470 / 2.567	Loss_tri_soft 0.282 / 0.279	Loss_lf 4.252 / 3.104	Loss_lf_tri 0.384 / 0.344	Prec 90.44% / 85.62%	
Epoch: [16][120/200]	Time 1.309 (2.723)	Data 0.000 (1.073)	Loss_ce 1.436 / 1.913	Loss_ce_soft 1.452 / 2.603	Loss_tri_soft 0.284 / 0.285	Loss_lf 4.251 / 3.119	Loss_lf_tri 0.382 / 0.347	Prec 90.10% / 85.36%	
Epoch: [16][140/200]	Time 1.275 (2.514)	Data 0.000 (0.919)	Loss_ce 1.433 / 1.919	Loss_ce_soft 1.458 / 2.612	Loss_tri_soft 0.290 / 0.291	Loss_lf 4.248 / 3.123	Loss_lf_tri 0.385 / 0.348	Prec 90.22% / 84.96%	
Epoch: [16][160/200]	Time 1.292 (2.629)	Data 0.000 (1.072)	Loss_ce 1.436 / 1.912	Loss_ce_soft 1.465 / 2.614	Loss_tri_soft 0.291 / 0.296	Loss_lf 4.249 / 3.128	Loss_lf_tri 0.388 / 0.349	Prec 90.12% / 85.20%	
Epoch: [16][180/200]	Time 1.281 (2.479)	Data 0.000 (0.953)	Loss_ce 1.419 / 1.911	Loss_ce_soft 1.440 / 2.609	Loss_tri_soft 0.284 / 0.295	Loss_lf 4.249 / 3.127	Loss_lf_tri 0.376 / 0.348	Prec 90.49% / 85.38%	
Epoch: [16][200/200]	Time 1.315 (2.569)	Data 0.001 (1.069)	Loss_ce 1.415 / 1.915	Loss_ce_soft 1.443 / 2.623	Loss_tri_soft 0.286 / 0.295	Loss_lf 4.251 / 3.134	Loss_lf_tri 0.378 / 0.351	Prec 90.41% / 85.31%	
Extract Features: [50/302]	Time 0.166 (1.030)	Data 0.001 (0.858)	
Extract Features: [100/302]	Time 0.164 (0.599)	Data 0.000 (0.429)	
Extract Features: [150/302]	Time 0.165 (0.455)	Data 0.000 (0.286)	
Extract Features: [200/302]	Time 0.165 (0.383)	Data 0.000 (0.215)	
Extract Features: [250/302]	Time 0.168 (0.341)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.182 (0.314)	Data 0.000 (0.143)	
Mean AP: 62.7%

 * Finished phase   1 epoch  16  model no.1 mAP: 62.7%  best: 62.7% *

Computing original distance...
Computing Jaccard distance...
Time cost: 11.243390798568726
Clustering and labeling...

 Clustered into 147 classes 

###############################
Lamda for less forget is set to  14.708043058552857
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [17][20/200]	Time 1.318 (1.288)	Data 0.000 (0.000)	Loss_ce 1.412 / 1.854	Loss_ce_soft 1.430 / 2.640	Loss_tri_soft 0.322 / 0.347	Loss_lf 4.270 / 2.974	Loss_lf_tri 0.424 / 0.388	Prec 89.38% / 85.94%	
Epoch: [17][40/200]	Time 1.263 (2.356)	Data 0.001 (1.061)	Loss_ce 1.457 / 1.920	Loss_ce_soft 1.471 / 2.673	Loss_tri_soft 0.313 / 0.288	Loss_lf 4.266 / 3.041	Loss_lf_tri 0.415 / 0.349	Prec 87.50% / 84.84%	
Epoch: [17][60/200]	Time 1.234 (1.989)	Data 0.000 (0.708)	Loss_ce 1.451 / 1.918	Loss_ce_soft 1.492 / 2.650	Loss_tri_soft 0.302 / 0.288	Loss_lf 4.266 / 3.025	Loss_lf_tri 0.408 / 0.350	Prec 88.54% / 84.79%	
Epoch: [17][80/200]	Time 1.257 (2.342)	Data 0.001 (1.066)	Loss_ce 1.450 / 1.911	Loss_ce_soft 1.489 / 2.659	Loss_tri_soft 0.300 / 0.289	Loss_lf 4.264 / 3.063	Loss_lf_tri 0.410 / 0.351	Prec 88.83% / 85.23%	
Epoch: [17][100/200]	Time 1.288 (2.138)	Data 0.000 (0.852)	Loss_ce 1.451 / 1.907	Loss_ce_soft 1.496 / 2.640	Loss_tri_soft 0.296 / 0.274	Loss_lf 4.254 / 3.083	Loss_lf_tri 0.404 / 0.340	Prec 88.62% / 85.06%	
Epoch: [17][120/200]	Time 1.262 (2.698)	Data 0.000 (1.057)	Loss_ce 1.440 / 1.894	Loss_ce_soft 1.475 / 2.609	Loss_tri_soft 0.302 / 0.267	Loss_lf 4.257 / 3.087	Loss_lf_tri 0.402 / 0.338	Prec 89.17% / 85.89%	
Epoch: [17][140/200]	Time 1.219 (2.492)	Data 0.000 (0.906)	Loss_ce 1.442 / 1.880	Loss_ce_soft 1.497 / 2.606	Loss_tri_soft 0.299 / 0.266	Loss_lf 4.261 / 3.080	Loss_lf_tri 0.400 / 0.335	Prec 89.06% / 86.38%	
Epoch: [17][160/200]	Time 1.285 (2.609)	Data 0.000 (1.061)	Loss_ce 1.427 / 1.874	Loss_ce_soft 1.473 / 2.603	Loss_tri_soft 0.295 / 0.271	Loss_lf 4.252 / 3.081	Loss_lf_tri 0.397 / 0.341	Prec 89.49% / 86.52%	
Epoch: [17][180/200]	Time 1.235 (2.466)	Data 0.000 (0.943)	Loss_ce 1.417 / 1.873	Loss_ce_soft 1.447 / 2.602	Loss_tri_soft 0.297 / 0.278	Loss_lf 4.249 / 3.085	Loss_lf_tri 0.401 / 0.346	Prec 89.72% / 86.63%	
Epoch: [17][200/200]	Time 1.228 (2.565)	Data 0.000 (1.068)	Loss_ce 1.415 / 1.871	Loss_ce_soft 1.449 / 2.591	Loss_tri_soft 0.297 / 0.274	Loss_lf 4.253 / 3.084	Loss_lf_tri 0.403 / 0.344	Prec 89.78% / 86.59%	
Extract Features: [50/302]	Time 0.165 (1.027)	Data 0.001 (0.856)	
Extract Features: [100/302]	Time 0.184 (0.598)	Data 0.000 (0.428)	
Extract Features: [150/302]	Time 0.164 (0.454)	Data 0.000 (0.285)	
Extract Features: [200/302]	Time 0.169 (0.383)	Data 0.000 (0.214)	
Extract Features: [250/302]	Time 0.165 (0.340)	Data 0.000 (0.171)	
Extract Features: [300/302]	Time 0.179 (0.311)	Data 0.000 (0.143)	
Mean AP: 61.9%

 * Finished phase   1 epoch  17  model no.1 mAP: 61.9%  best: 62.7%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.935487508773804
Clustering and labeling...

 Clustered into 152 classes 

###############################
Lamda for less forget is set to  14.46411166701189
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [18][20/200]	Time 1.297 (1.278)	Data 0.000 (0.000)	Loss_ce 1.432 / 1.827	Loss_ce_soft 1.374 / 2.525	Loss_tri_soft 0.227 / 0.268	Loss_lf 4.360 / 2.959	Loss_lf_tri 0.342 / 0.343	Prec 90.00% / 86.56%	
Epoch: [18][40/200]	Time 1.279 (2.355)	Data 0.000 (1.076)	Loss_ce 1.429 / 1.876	Loss_ce_soft 1.391 / 2.630	Loss_tri_soft 0.236 / 0.290	Loss_lf 4.278 / 3.087	Loss_lf_tri 0.352 / 0.351	Prec 89.84% / 86.09%	
Epoch: [18][60/200]	Time 1.361 (1.999)	Data 0.000 (0.718)	Loss_ce 1.437 / 1.893	Loss_ce_soft 1.422 / 2.595	Loss_tri_soft 0.250 / 0.290	Loss_lf 4.268 / 3.067	Loss_lf_tri 0.360 / 0.347	Prec 90.21% / 84.90%	
Epoch: [18][80/200]	Time 1.230 (2.378)	Data 0.001 (1.092)	Loss_ce 1.420 / 1.891	Loss_ce_soft 1.419 / 2.580	Loss_tri_soft 0.258 / 0.276	Loss_lf 4.258 / 3.078	Loss_lf_tri 0.364 / 0.341	Prec 90.47% / 84.45%	
Epoch: [18][100/200]	Time 1.264 (2.165)	Data 0.000 (0.874)	Loss_ce 1.408 / 1.906	Loss_ce_soft 1.409 / 2.595	Loss_tri_soft 0.259 / 0.293	Loss_lf 4.261 / 3.112	Loss_lf_tri 0.363 / 0.351	Prec 90.88% / 84.56%	
Epoch: [18][120/200]	Time 1.307 (2.734)	Data 0.000 (1.086)	Loss_ce 1.423 / 1.912	Loss_ce_soft 1.426 / 2.593	Loss_tri_soft 0.267 / 0.295	Loss_lf 4.262 / 3.090	Loss_lf_tri 0.368 / 0.351	Prec 90.31% / 84.58%	
Epoch: [18][140/200]	Time 1.292 (2.523)	Data 0.000 (0.931)	Loss_ce 1.412 / 1.884	Loss_ce_soft 1.402 / 2.578	Loss_tri_soft 0.275 / 0.294	Loss_lf 4.257 / 3.094	Loss_lf_tri 0.373 / 0.350	Prec 90.45% / 85.58%	
Epoch: [18][160/200]	Time 1.300 (2.640)	Data 0.000 (1.086)	Loss_ce 1.409 / 1.877	Loss_ce_soft 1.399 / 2.579	Loss_tri_soft 0.271 / 0.293	Loss_lf 4.248 / 3.116	Loss_lf_tri 0.372 / 0.354	Prec 90.43% / 86.02%	
Epoch: [18][180/200]	Time 1.350 (2.491)	Data 0.000 (0.965)	Loss_ce 1.408 / 1.873	Loss_ce_soft 1.411 / 2.568	Loss_tri_soft 0.272 / 0.293	Loss_lf 4.257 / 3.125	Loss_lf_tri 0.373 / 0.356	Prec 90.45% / 86.25%	
Epoch: [18][200/200]	Time 1.272 (2.592)	Data 0.000 (1.085)	Loss_ce 1.402 / 1.871	Loss_ce_soft 1.411 / 2.568	Loss_tri_soft 0.269 / 0.287	Loss_lf 4.253 / 3.128	Loss_lf_tri 0.374 / 0.353	Prec 90.66% / 86.25%	
Extract Features: [50/302]	Time 0.164 (1.024)	Data 0.001 (0.854)	
Extract Features: [100/302]	Time 0.167 (0.598)	Data 0.000 (0.427)	
Extract Features: [150/302]	Time 0.165 (0.455)	Data 0.000 (0.285)	
Extract Features: [200/302]	Time 0.164 (0.384)	Data 0.000 (0.214)	
Extract Features: [250/302]	Time 0.184 (0.341)	Data 0.000 (0.171)	
Extract Features: [300/302]	Time 0.179 (0.312)	Data 0.000 (0.143)	
Mean AP: 62.5%

 * Finished phase   1 epoch  18  model no.1 mAP: 62.5%  best: 62.7%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.028457164764404
Clustering and labeling...

 Clustered into 149 classes 

###############################
Lamda for less forget is set to  14.608997870899504
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [19][20/200]	Time 1.242 (1.287)	Data 0.000 (0.000)	Loss_ce 1.386 / 1.903	Loss_ce_soft 1.364 / 2.750	Loss_tri_soft 0.272 / 0.274	Loss_lf 4.296 / 3.162	Loss_lf_tri 0.340 / 0.338	Prec 90.62% / 81.88%	
Epoch: [19][40/200]	Time 1.259 (2.304)	Data 0.000 (1.029)	Loss_ce 1.425 / 1.961	Loss_ce_soft 1.397 / 2.755	Loss_tri_soft 0.271 / 0.290	Loss_lf 4.241 / 3.191	Loss_lf_tri 0.363 / 0.346	Prec 89.38% / 82.34%	
Epoch: [19][60/200]	Time 1.249 (1.956)	Data 0.001 (0.686)	Loss_ce 1.423 / 1.935	Loss_ce_soft 1.411 / 2.681	Loss_tri_soft 0.277 / 0.277	Loss_lf 4.258 / 3.160	Loss_lf_tri 0.372 / 0.344	Prec 89.06% / 82.81%	
Epoch: [19][80/200]	Time 1.260 (2.308)	Data 0.000 (1.041)	Loss_ce 1.418 / 1.973	Loss_ce_soft 1.413 / 2.708	Loss_tri_soft 0.303 / 0.288	Loss_lf 4.230 / 3.143	Loss_lf_tri 0.392 / 0.351	Prec 89.77% / 81.56%	
Epoch: [19][100/200]	Time 1.284 (2.106)	Data 0.000 (0.833)	Loss_ce 1.402 / 1.976	Loss_ce_soft 1.399 / 2.692	Loss_tri_soft 0.284 / 0.293	Loss_lf 4.236 / 3.137	Loss_lf_tri 0.379 / 0.356	Prec 89.94% / 82.00%	
Epoch: [19][120/200]	Time 1.224 (2.656)	Data 0.001 (1.042)	Loss_ce 1.410 / 1.964	Loss_ce_soft 1.412 / 2.672	Loss_tri_soft 0.285 / 0.289	Loss_lf 4.239 / 3.132	Loss_lf_tri 0.386 / 0.352	Prec 89.95% / 82.76%	
Epoch: [19][140/200]	Time 1.327 (2.465)	Data 0.000 (0.893)	Loss_ce 1.405 / 1.966	Loss_ce_soft 1.401 / 2.694	Loss_tri_soft 0.276 / 0.282	Loss_lf 4.234 / 3.145	Loss_lf_tri 0.378 / 0.348	Prec 89.96% / 82.86%	
Epoch: [19][160/200]	Time 1.210 (2.577)	Data 0.000 (1.041)	Loss_ce 1.406 / 1.948	Loss_ce_soft 1.410 / 2.675	Loss_tri_soft 0.278 / 0.290	Loss_lf 4.239 / 3.152	Loss_lf_tri 0.381 / 0.355	Prec 89.92% / 83.44%	
Epoch: [19][180/200]	Time 1.219 (2.430)	Data 0.000 (0.926)	Loss_ce 1.404 / 1.938	Loss_ce_soft 1.406 / 2.672	Loss_tri_soft 0.283 / 0.291	Loss_lf 4.241 / 3.163	Loss_lf_tri 0.383 / 0.358	Prec 90.17% / 83.72%	
Epoch: [19][200/200]	Time 1.259 (2.523)	Data 0.001 (1.043)	Loss_ce 1.399 / 1.924	Loss_ce_soft 1.405 / 2.658	Loss_tri_soft 0.284 / 0.289	Loss_lf 4.240 / 3.140	Loss_lf_tri 0.383 / 0.358	Prec 90.28% / 84.28%	
Extract Features: [50/302]	Time 0.169 (1.024)	Data 0.000 (0.843)	
Extract Features: [100/302]	Time 0.163 (0.596)	Data 0.000 (0.422)	
Extract Features: [150/302]	Time 0.183 (0.454)	Data 0.000 (0.281)	
Extract Features: [200/302]	Time 0.163 (0.383)	Data 0.000 (0.211)	
Extract Features: [250/302]	Time 0.164 (0.339)	Data 0.000 (0.169)	
Extract Features: [300/302]	Time 0.195 (0.311)	Data 0.000 (0.141)	
Mean AP: 62.3%

 * Finished phase   1 epoch  19  model no.1 mAP: 62.3%  best: 62.7%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.02346158027649
Clustering and labeling...

 Clustered into 152 classes 

###############################
Lamda for less forget is set to  14.46411166701189
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [20][20/200]	Time 1.245 (1.289)	Data 0.000 (0.000)	Loss_ce 1.472 / 1.782	Loss_ce_soft 1.499 / 2.473	Loss_tri_soft 0.271 / 0.260	Loss_lf 4.235 / 3.165	Loss_lf_tri 0.389 / 0.325	Prec 89.06% / 88.75%	
Epoch: [20][40/200]	Time 1.242 (2.316)	Data 0.000 (1.041)	Loss_ce 1.492 / 1.864	Loss_ce_soft 1.488 / 2.548	Loss_tri_soft 0.299 / 0.289	Loss_lf 4.217 / 3.176	Loss_lf_tri 0.399 / 0.353	Prec 87.97% / 85.94%	
Epoch: [20][60/200]	Time 1.264 (1.962)	Data 0.000 (0.694)	Loss_ce 1.487 / 1.873	Loss_ce_soft 1.498 / 2.569	Loss_tri_soft 0.295 / 0.264	Loss_lf 4.236 / 3.154	Loss_lf_tri 0.398 / 0.335	Prec 87.81% / 85.21%	
Epoch: [20][80/200]	Time 1.223 (2.314)	Data 0.000 (1.053)	Loss_ce 1.471 / 1.901	Loss_ce_soft 1.467 / 2.616	Loss_tri_soft 0.301 / 0.270	Loss_lf 4.233 / 3.205	Loss_lf_tri 0.400 / 0.346	Prec 88.44% / 85.23%	
Epoch: [20][100/200]	Time 1.249 (2.101)	Data 0.000 (0.843)	Loss_ce 1.448 / 1.925	Loss_ce_soft 1.454 / 2.650	Loss_tri_soft 0.292 / 0.277	Loss_lf 4.236 / 3.130	Loss_lf_tri 0.393 / 0.347	Prec 89.00% / 85.00%	
Epoch: [20][120/200]	Time 1.307 (2.658)	Data 0.000 (1.048)	Loss_ce 1.466 / 1.914	Loss_ce_soft 1.482 / 2.623	Loss_tri_soft 0.293 / 0.275	Loss_lf 4.251 / 3.129	Loss_lf_tri 0.393 / 0.345	Prec 88.28% / 85.16%	
Epoch: [20][140/200]	Time 1.231 (2.456)	Data 0.001 (0.899)	Loss_ce 1.447 / 1.912	Loss_ce_soft 1.470 / 2.628	Loss_tri_soft 0.301 / 0.273	Loss_lf 4.251 / 3.136	Loss_lf_tri 0.398 / 0.344	Prec 89.06% / 85.13%	
Epoch: [20][160/200]	Time 1.253 (2.564)	Data 0.000 (1.043)	Loss_ce 1.440 / 1.909	Loss_ce_soft 1.482 / 2.642	Loss_tri_soft 0.301 / 0.273	Loss_lf 4.245 / 3.146	Loss_lf_tri 0.402 / 0.348	Prec 89.45% / 84.92%	
Epoch: [20][180/200]	Time 1.288 (2.420)	Data 0.000 (0.927)	Loss_ce 1.433 / 1.907	Loss_ce_soft 1.474 / 2.641	Loss_tri_soft 0.296 / 0.273	Loss_lf 4.244 / 3.136	Loss_lf_tri 0.397 / 0.346	Prec 89.55% / 85.03%	
Epoch: [20][200/200]	Time 1.264 (2.516)	Data 0.001 (1.044)	Loss_ce 1.431 / 1.903	Loss_ce_soft 1.479 / 2.636	Loss_tri_soft 0.295 / 0.272	Loss_lf 4.246 / 3.140	Loss_lf_tri 0.395 / 0.345	Prec 89.88% / 85.06%	
Extract Features: [50/302]	Time 0.164 (0.999)	Data 0.000 (0.827)	
Extract Features: [100/302]	Time 0.163 (0.583)	Data 0.000 (0.414)	
Extract Features: [150/302]	Time 0.165 (0.444)	Data 0.001 (0.276)	
Extract Features: [200/302]	Time 0.168 (0.379)	Data 0.001 (0.207)	
Extract Features: [250/302]	Time 0.165 (0.337)	Data 0.000 (0.166)	
Extract Features: [300/302]	Time 0.180 (0.309)	Data 0.001 (0.138)	
Mean AP: 62.1%

 * Finished phase   1 epoch  20  model no.1 mAP: 62.1%  best: 62.7%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.939487934112549
Clustering and labeling...

 Clustered into 152 classes 

###############################
Lamda for less forget is set to  14.46411166701189
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [21][20/200]	Time 1.274 (1.308)	Data 0.000 (0.000)	Loss_ce 1.439 / 1.962	Loss_ce_soft 1.484 / 2.685	Loss_tri_soft 0.256 / 0.291	Loss_lf 4.258 / 3.119	Loss_lf_tri 0.359 / 0.350	Prec 90.31% / 82.81%	
Epoch: [21][40/200]	Time 1.317 (2.374)	Data 0.001 (1.059)	Loss_ce 1.431 / 1.912	Loss_ce_soft 1.491 / 2.628	Loss_tri_soft 0.249 / 0.297	Loss_lf 4.253 / 3.176	Loss_lf_tri 0.363 / 0.342	Prec 91.25% / 85.94%	
Epoch: [21][60/200]	Time 1.253 (2.006)	Data 0.001 (0.706)	Loss_ce 1.436 / 1.944	Loss_ce_soft 1.497 / 2.687	Loss_tri_soft 0.264 / 0.305	Loss_lf 4.258 / 3.235	Loss_lf_tri 0.374 / 0.348	Prec 90.52% / 85.00%	
Epoch: [21][80/200]	Time 1.303 (2.362)	Data 0.001 (1.063)	Loss_ce 1.415 / 1.922	Loss_ce_soft 1.464 / 2.652	Loss_tri_soft 0.258 / 0.305	Loss_lf 4.247 / 3.167	Loss_lf_tri 0.372 / 0.354	Prec 91.25% / 85.16%	
Epoch: [21][100/200]	Time 1.291 (2.143)	Data 0.001 (0.850)	Loss_ce 1.421 / 1.937	Loss_ce_soft 1.465 / 2.667	Loss_tri_soft 0.265 / 0.294	Loss_lf 4.247 / 3.211	Loss_lf_tri 0.381 / 0.355	Prec 91.00% / 84.75%	
Epoch: [21][120/200]	Time 1.250 (2.707)	Data 0.000 (1.065)	Loss_ce 1.416 / 1.914	Loss_ce_soft 1.461 / 2.644	Loss_tri_soft 0.263 / 0.286	Loss_lf 4.257 / 3.163	Loss_lf_tri 0.379 / 0.348	Prec 91.04% / 85.47%	
Epoch: [21][140/200]	Time 1.275 (2.502)	Data 0.000 (0.913)	Loss_ce 1.418 / 1.907	Loss_ce_soft 1.472 / 2.650	Loss_tri_soft 0.269 / 0.283	Loss_lf 4.267 / 3.159	Loss_lf_tri 0.386 / 0.350	Prec 90.98% / 85.45%	
Epoch: [21][160/200]	Time 1.230 (2.610)	Data 0.000 (1.058)	Loss_ce 1.425 / 1.894	Loss_ce_soft 1.478 / 2.645	Loss_tri_soft 0.263 / 0.279	Loss_lf 4.261 / 3.169	Loss_lf_tri 0.381 / 0.346	Prec 90.78% / 85.70%	
Epoch: [21][180/200]	Time 1.268 (2.463)	Data 0.000 (0.941)	Loss_ce 1.434 / 1.890	Loss_ce_soft 1.490 / 2.635	Loss_tri_soft 0.265 / 0.278	Loss_lf 4.260 / 3.167	Loss_lf_tri 0.385 / 0.348	Prec 90.21% / 86.04%	
Epoch: [21][200/200]	Time 1.209 (2.554)	Data 0.001 (1.059)	Loss_ce 1.436 / 1.883	Loss_ce_soft 1.490 / 2.625	Loss_tri_soft 0.272 / 0.275	Loss_lf 4.253 / 3.155	Loss_lf_tri 0.390 / 0.346	Prec 89.97% / 86.19%	
Extract Features: [50/302]	Time 0.172 (1.002)	Data 0.000 (0.833)	
Extract Features: [100/302]	Time 0.163 (0.585)	Data 0.000 (0.417)	
Extract Features: [150/302]	Time 0.168 (0.446)	Data 0.000 (0.278)	
Extract Features: [200/302]	Time 0.176 (0.377)	Data 0.001 (0.209)	
Extract Features: [250/302]	Time 0.162 (0.335)	Data 0.000 (0.167)	
Extract Features: [300/302]	Time 0.182 (0.307)	Data 0.000 (0.139)	
Mean AP: 62.0%

 * Finished phase   1 epoch  21  model no.1 mAP: 62.0%  best: 62.7%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.151419401168823
Clustering and labeling...

 Clustered into 157 classes 

###############################
Lamda for less forget is set to  14.231927863105438
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [22][20/200]	Time 1.293 (1.324)	Data 0.000 (0.000)	Loss_ce 1.482 / 1.902	Loss_ce_soft 1.456 / 2.651	Loss_tri_soft 0.254 / 0.345	Loss_lf 4.258 / 3.132	Loss_lf_tri 0.345 / 0.400	Prec 89.06% / 86.25%	
Epoch: [22][40/200]	Time 43.675 (2.374)	Data 42.334 (1.059)	Loss_ce 1.451 / 1.931	Loss_ce_soft 1.446 / 2.698	Loss_tri_soft 0.273 / 0.313	Loss_lf 4.242 / 3.090	Loss_lf_tri 0.370 / 0.380	Prec 91.25% / 84.53%	
Epoch: [22][60/200]	Time 1.224 (2.000)	Data 0.000 (0.706)	Loss_ce 1.416 / 1.946	Loss_ce_soft 1.393 / 2.709	Loss_tri_soft 0.268 / 0.305	Loss_lf 4.242 / 3.126	Loss_lf_tri 0.370 / 0.374	Prec 91.88% / 85.21%	
Epoch: [22][80/200]	Time 1.286 (2.353)	Data 0.000 (1.068)	Loss_ce 1.426 / 1.936	Loss_ce_soft 1.452 / 2.657	Loss_tri_soft 0.274 / 0.292	Loss_lf 4.246 / 3.095	Loss_lf_tri 0.380 / 0.364	Prec 91.64% / 85.31%	
Epoch: [22][100/200]	Time 1.340 (2.141)	Data 0.000 (0.855)	Loss_ce 1.422 / 1.962	Loss_ce_soft 1.442 / 2.669	Loss_tri_soft 0.265 / 0.297	Loss_lf 4.234 / 3.116	Loss_lf_tri 0.373 / 0.369	Prec 91.56% / 85.00%	
Epoch: [22][120/200]	Time 1.223 (2.724)	Data 0.000 (1.071)	Loss_ce 1.410 / 1.955	Loss_ce_soft 1.421 / 2.651	Loss_tri_soft 0.264 / 0.299	Loss_lf 4.245 / 3.147	Loss_lf_tri 0.369 / 0.371	Prec 92.03% / 85.05%	
Epoch: [22][140/200]	Time 1.261 (2.514)	Data 0.001 (0.918)	Loss_ce 1.403 / 1.945	Loss_ce_soft 1.422 / 2.654	Loss_tri_soft 0.259 / 0.296	Loss_lf 4.247 / 3.148	Loss_lf_tri 0.364 / 0.367	Prec 92.10% / 85.04%	
Epoch: [22][160/200]	Time 1.226 (2.627)	Data 0.000 (1.070)	Loss_ce 1.402 / 1.924	Loss_ce_soft 1.425 / 2.647	Loss_tri_soft 0.260 / 0.287	Loss_lf 4.252 / 3.151	Loss_lf_tri 0.366 / 0.360	Prec 91.76% / 85.35%	
Epoch: [22][180/200]	Time 1.273 (2.474)	Data 0.000 (0.951)	Loss_ce 1.403 / 1.911	Loss_ce_soft 1.434 / 2.642	Loss_tri_soft 0.262 / 0.285	Loss_lf 4.259 / 3.143	Loss_lf_tri 0.367 / 0.360	Prec 91.67% / 86.04%	
Epoch: [22][200/200]	Time 1.277 (2.564)	Data 0.001 (1.069)	Loss_ce 1.394 / 1.916	Loss_ce_soft 1.426 / 2.651	Loss_tri_soft 0.261 / 0.289	Loss_lf 4.251 / 3.138	Loss_lf_tri 0.369 / 0.361	Prec 91.75% / 85.78%	
Extract Features: [50/302]	Time 0.167 (1.033)	Data 0.000 (0.862)	
Extract Features: [100/302]	Time 0.164 (0.608)	Data 0.000 (0.431)	
Extract Features: [150/302]	Time 0.164 (0.461)	Data 0.001 (0.288)	
Extract Features: [200/302]	Time 0.165 (0.387)	Data 0.001 (0.216)	
Extract Features: [250/302]	Time 0.166 (0.343)	Data 0.001 (0.173)	
Extract Features: [300/302]	Time 0.164 (0.314)	Data 0.000 (0.144)	
Mean AP: 62.4%

 * Finished phase   1 epoch  22  model no.1 mAP: 62.4%  best: 62.7%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.50630497932434
Clustering and labeling...

 Clustered into 149 classes 

###############################
Lamda for less forget is set to  14.608997870899504
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [23][20/200]	Time 1.250 (1.254)	Data 0.001 (0.000)	Loss_ce 1.452 / 2.030	Loss_ce_soft 1.433 / 2.731	Loss_tri_soft 0.297 / 0.297	Loss_lf 4.291 / 3.264	Loss_lf_tri 0.411 / 0.382	Prec 87.19% / 82.81%	
Epoch: [23][40/200]	Time 1.238 (2.304)	Data 0.000 (1.047)	Loss_ce 1.488 / 2.020	Loss_ce_soft 1.486 / 2.703	Loss_tri_soft 0.306 / 0.308	Loss_lf 4.287 / 3.156	Loss_lf_tri 0.416 / 0.386	Prec 87.03% / 82.50%	
Epoch: [23][60/200]	Time 1.293 (1.968)	Data 0.000 (0.698)	Loss_ce 1.433 / 1.960	Loss_ce_soft 1.416 / 2.641	Loss_tri_soft 0.284 / 0.315	Loss_lf 4.271 / 3.214	Loss_lf_tri 0.395 / 0.385	Prec 88.96% / 84.27%	
Epoch: [23][80/200]	Time 1.243 (2.339)	Data 0.001 (1.066)	Loss_ce 1.422 / 1.925	Loss_ce_soft 1.390 / 2.599	Loss_tri_soft 0.292 / 0.303	Loss_lf 4.264 / 3.208	Loss_lf_tri 0.398 / 0.369	Prec 89.53% / 85.70%	
Epoch: [23][100/200]	Time 1.205 (2.121)	Data 0.001 (0.853)	Loss_ce 1.424 / 1.927	Loss_ce_soft 1.407 / 2.595	Loss_tri_soft 0.302 / 0.301	Loss_lf 4.260 / 3.178	Loss_lf_tri 0.403 / 0.366	Prec 89.62% / 85.69%	
Epoch: [23][120/200]	Time 1.250 (2.696)	Data 0.000 (1.068)	Loss_ce 1.429 / 1.925	Loss_ce_soft 1.419 / 2.605	Loss_tri_soft 0.302 / 0.289	Loss_lf 4.271 / 3.140	Loss_lf_tri 0.404 / 0.356	Prec 89.84% / 85.47%	
Epoch: [23][140/200]	Time 1.305 (2.495)	Data 0.000 (0.916)	Loss_ce 1.422 / 1.919	Loss_ce_soft 1.408 / 2.624	Loss_tri_soft 0.300 / 0.284	Loss_lf 4.268 / 3.159	Loss_lf_tri 0.404 / 0.354	Prec 89.69% / 85.89%	
Epoch: [23][160/200]	Time 1.252 (2.607)	Data 0.001 (1.064)	Loss_ce 1.414 / 1.910	Loss_ce_soft 1.421 / 2.624	Loss_tri_soft 0.298 / 0.279	Loss_lf 4.273 / 3.160	Loss_lf_tri 0.404 / 0.351	Prec 89.92% / 85.90%	
Epoch: [23][180/200]	Time 1.323 (2.462)	Data 0.001 (0.946)	Loss_ce 1.414 / 1.904	Loss_ce_soft 1.425 / 2.616	Loss_tri_soft 0.297 / 0.273	Loss_lf 4.265 / 3.166	Loss_lf_tri 0.402 / 0.351	Prec 90.03% / 85.62%	
Epoch: [23][200/200]	Time 1.228 (2.558)	Data 0.000 (1.062)	Loss_ce 1.403 / 1.899	Loss_ce_soft 1.410 / 2.597	Loss_tri_soft 0.294 / 0.272	Loss_lf 4.260 / 3.148	Loss_lf_tri 0.401 / 0.349	Prec 90.34% / 85.66%	
Extract Features: [50/302]	Time 0.165 (1.020)	Data 0.001 (0.850)	
Extract Features: [100/302]	Time 0.165 (0.594)	Data 0.000 (0.425)	
Extract Features: [150/302]	Time 0.169 (0.452)	Data 0.001 (0.284)	
Extract Features: [200/302]	Time 0.165 (0.382)	Data 0.000 (0.213)	
Extract Features: [250/302]	Time 0.166 (0.339)	Data 0.000 (0.170)	
Extract Features: [300/302]	Time 0.179 (0.312)	Data 0.000 (0.142)	
Mean AP: 62.3%

 * Finished phase   1 epoch  23  model no.1 mAP: 62.3%  best: 62.7%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.25338888168335
Clustering and labeling...

 Clustered into 158 classes 

###############################
Lamda for less forget is set to  14.186818628036736
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [24][20/200]	Time 1.227 (1.282)	Data 0.000 (0.000)	Loss_ce 1.548 / 2.003	Loss_ce_soft 1.575 / 2.826	Loss_tri_soft 0.343 / 0.268	Loss_lf 4.277 / 3.261	Loss_lf_tri 0.427 / 0.329	Prec 86.25% / 81.25%	
Epoch: [24][40/200]	Time 44.268 (2.366)	Data 42.935 (1.074)	Loss_ce 1.510 / 1.928	Loss_ce_soft 1.547 / 2.614	Loss_tri_soft 0.338 / 0.273	Loss_lf 4.292 / 3.220	Loss_lf_tri 0.424 / 0.338	Prec 88.44% / 85.00%	
Epoch: [24][60/200]	Time 1.245 (1.993)	Data 0.000 (0.716)	Loss_ce 1.478 / 1.923	Loss_ce_soft 1.538 / 2.596	Loss_tri_soft 0.301 / 0.295	Loss_lf 4.269 / 3.161	Loss_lf_tri 0.399 / 0.354	Prec 89.38% / 84.79%	
Epoch: [24][80/200]	Time 1.313 (2.347)	Data 0.001 (1.068)	Loss_ce 1.456 / 1.922	Loss_ce_soft 1.549 / 2.588	Loss_tri_soft 0.301 / 0.286	Loss_lf 4.278 / 3.138	Loss_lf_tri 0.408 / 0.353	Prec 90.23% / 84.69%	
Epoch: [24][100/200]	Time 1.218 (2.128)	Data 0.001 (0.854)	Loss_ce 1.465 / 1.937	Loss_ce_soft 1.570 / 2.641	Loss_tri_soft 0.302 / 0.287	Loss_lf 4.275 / 3.166	Loss_lf_tri 0.410 / 0.358	Prec 89.50% / 84.75%	
Epoch: [24][120/200]	Time 43.881 (2.706)	Data 0.000 (1.071)	Loss_ce 1.467 / 1.923	Loss_ce_soft 1.558 / 2.613	Loss_tri_soft 0.301 / 0.284	Loss_lf 4.284 / 3.176	Loss_lf_tri 0.408 / 0.352	Prec 89.32% / 85.16%	
Epoch: [24][140/200]	Time 1.232 (2.499)	Data 0.001 (0.918)	Loss_ce 1.463 / 1.911	Loss_ce_soft 1.547 / 2.604	Loss_tri_soft 0.295 / 0.282	Loss_lf 4.269 / 3.163	Loss_lf_tri 0.403 / 0.349	Prec 89.33% / 85.22%	
Epoch: [24][160/200]	Time 1.222 (2.622)	Data 0.000 (1.073)	Loss_ce 1.449 / 1.903	Loss_ce_soft 1.533 / 2.609	Loss_tri_soft 0.295 / 0.279	Loss_lf 4.263 / 3.160	Loss_lf_tri 0.405 / 0.345	Prec 89.65% / 85.62%	
Epoch: [24][180/200]	Time 1.293 (2.470)	Data 0.000 (0.954)	Loss_ce 1.438 / 1.905	Loss_ce_soft 1.518 / 2.605	Loss_tri_soft 0.290 / 0.285	Loss_lf 4.253 / 3.155	Loss_lf_tri 0.402 / 0.350	Prec 89.97% / 85.38%	
Epoch: [24][200/200]	Time 1.136 (2.560)	Data 0.001 (1.073)	Loss_ce 1.436 / 1.902	Loss_ce_soft 1.510 / 2.619	Loss_tri_soft 0.293 / 0.287	Loss_lf 4.256 / 3.164	Loss_lf_tri 0.405 / 0.353	Prec 89.72% / 85.56%	
Extract Features: [50/302]	Time 0.185 (1.034)	Data 0.001 (0.860)	
Extract Features: [100/302]	Time 0.166 (0.601)	Data 0.001 (0.430)	
Extract Features: [150/302]	Time 0.165 (0.457)	Data 0.000 (0.287)	
Extract Features: [200/302]	Time 0.183 (0.385)	Data 0.000 (0.215)	
Extract Features: [250/302]	Time 0.165 (0.342)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.178 (0.316)	Data 0.000 (0.144)	
Mean AP: 62.1%

 * Finished phase   1 epoch  24  model no.1 mAP: 62.1%  best: 62.7%

update proto_dataset
=> Loaded checkpoint 'logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase2_model_best.pth.tar'
mismatch: module.classifier.fc2.weight torch.Size([153, 2048]) torch.Size([158, 2048])
mismatch: module.classifier_max.fc2.weight torch.Size([153, 2048]) torch.Size([158, 2048])
missing keys in state_dict: {'module.classifier_max.fc2.weight', 'module.classifier.fc2.weight'}
Computing original distance...
Computing Jaccard distance...
Time cost: 11.189408302307129
Clustering and labeling...
phase 1 Clustered into 147 example classes 
delete by camera is 35
delete by cluster distance is 5,total num is 112
NMI of phase1 is 0.951483419304936 
pickle into logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase1_proto.pkl


 phase 2 have 425 old classes
=> Loaded checkpoint 'logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase2_model_best.pth.tar'
phase:2 input id:100,input image:1713
Computing original distance...
Computing Jaccard distance...
Time cost: 9.945807218551636
eps for cluster: 0.054
Clustering and labeling...

 Clustered into 123 classes 

in_features: 2048 out_features1: 318 out_features2: 107
###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [0][20/200]	Time 1.241 (1.273)	Data 0.000 (0.000)	Loss_ce 1.456 / 1.944	Loss_ce_soft 1.493 / 2.965	Loss_tri_soft 0.360 / 0.294	Loss_lf 4.297 / 2.647	Loss_lf_tri 0.323 / 0.278	Prec 89.06% / 85.31%	
Epoch: [0][40/200]	Time 1.220 (2.324)	Data 0.000 (1.053)	Loss_ce 1.468 / 1.949	Loss_ce_soft 1.548 / 2.947	Loss_tri_soft 0.356 / 0.293	Loss_lf 4.355 / 2.780	Loss_lf_tri 0.322 / 0.278	Prec 87.97% / 86.41%	
Epoch: [0][60/200]	Time 1.348 (1.982)	Data 0.000 (0.702)	Loss_ce 1.431 / 1.968	Loss_ce_soft 1.522 / 2.951	Loss_tri_soft 0.348 / 0.298	Loss_lf 4.368 / 2.830	Loss_lf_tri 0.319 / 0.281	Prec 88.23% / 85.73%	
Epoch: [0][80/200]	Time 1.237 (2.325)	Data 0.000 (1.049)	Loss_ce 1.427 / 1.961	Loss_ce_soft 1.513 / 2.980	Loss_tri_soft 0.336 / 0.297	Loss_lf 4.369 / 2.840	Loss_lf_tri 0.309 / 0.280	Prec 88.20% / 85.94%	
Epoch: [0][100/200]	Time 1.286 (2.541)	Data 0.001 (1.264)	Loss_ce 1.415 / 1.990	Loss_ce_soft 1.501 / 3.022	Loss_tri_soft 0.331 / 0.297	Loss_lf 4.386 / 2.845	Loss_lf_tri 0.307 / 0.280	Prec 88.81% / 85.00%	
Epoch: [0][120/200]	Time 1.278 (2.335)	Data 0.000 (1.053)	Loss_ce 1.412 / 2.003	Loss_ce_soft 1.513 / 3.035	Loss_tri_soft 0.339 / 0.305	Loss_lf 4.394 / 2.841	Loss_lf_tri 0.315 / 0.284	Prec 88.96% / 84.27%	
Epoch: [0][140/200]	Time 1.148 (2.784)	Data 0.000 (1.208)	Loss_ce 1.411 / 1.996	Loss_ce_soft 1.502 / 3.027	Loss_tri_soft 0.333 / 0.295	Loss_lf 4.393 / 2.809	Loss_lf_tri 0.312 / 0.276	Prec 89.20% / 84.29%	
Epoch: [0][160/200]	Time 1.257 (2.859)	Data 0.000 (1.322)	Loss_ce 1.403 / 1.987	Loss_ce_soft 1.490 / 3.015	Loss_tri_soft 0.325 / 0.295	Loss_lf 4.399 / 2.814	Loss_lf_tri 0.307 / 0.278	Prec 89.38% / 84.22%	
Epoch: [0][180/200]	Time 1.235 (2.681)	Data 0.000 (1.176)	Loss_ce 1.399 / 1.984	Loss_ce_soft 1.476 / 3.008	Loss_tri_soft 0.326 / 0.295	Loss_lf 4.395 / 2.804	Loss_lf_tri 0.309 / 0.279	Prec 89.38% / 84.38%	
Epoch: [0][200/200]	Time 1.225 (2.757)	Data 0.001 (1.277)	Loss_ce 1.399 / 1.968	Loss_ce_soft 1.479 / 2.985	Loss_tri_soft 0.321 / 0.290	Loss_lf 4.397 / 2.794	Loss_lf_tri 0.306 / 0.275	Prec 89.44% / 84.94%	
Extract Features: [50/302]	Time 0.170 (1.017)	Data 0.000 (0.842)	
Extract Features: [100/302]	Time 0.183 (0.594)	Data 0.000 (0.421)	
Extract Features: [150/302]	Time 0.185 (0.453)	Data 0.001 (0.281)	
Extract Features: [200/302]	Time 0.167 (0.383)	Data 0.000 (0.211)	
Extract Features: [250/302]	Time 0.187 (0.341)	Data 0.000 (0.169)	
Extract Features: [300/302]	Time 0.162 (0.315)	Data 0.000 (0.141)	
Mean AP: 61.9%

 * Finished phase   2 epoch   0  model no.1 mAP: 61.9%  best: 61.9% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.277699947357178
Clustering and labeling...

 Clustered into 128 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [1][20/200]	Time 1.244 (1.262)	Data 0.000 (0.000)	Loss_ce 1.479 / 1.961	Loss_ce_soft 1.413 / 2.962	Loss_tri_soft 0.313 / 0.231	Loss_lf 4.435 / 2.802	Loss_lf_tri 0.302 / 0.231	Prec 86.25% / 83.75%	
Epoch: [1][40/200]	Time 1.318 (2.305)	Data 0.001 (1.045)	Loss_ce 1.447 / 1.977	Loss_ce_soft 1.383 / 2.932	Loss_tri_soft 0.330 / 0.257	Loss_lf 4.448 / 2.886	Loss_lf_tri 0.318 / 0.251	Prec 85.62% / 84.22%	
Epoch: [1][60/200]	Time 1.297 (1.980)	Data 0.000 (0.697)	Loss_ce 1.427 / 1.967	Loss_ce_soft 1.386 / 2.882	Loss_tri_soft 0.326 / 0.262	Loss_lf 4.433 / 2.809	Loss_lf_tri 0.312 / 0.256	Prec 87.08% / 84.38%	
Epoch: [1][80/200]	Time 1.335 (2.333)	Data 0.000 (1.047)	Loss_ce 1.409 / 1.971	Loss_ce_soft 1.371 / 2.886	Loss_tri_soft 0.302 / 0.264	Loss_lf 4.418 / 2.819	Loss_lf_tri 0.296 / 0.258	Prec 87.89% / 84.38%	
Epoch: [1][100/200]	Time 1.221 (2.552)	Data 0.000 (1.261)	Loss_ce 1.405 / 2.003	Loss_ce_soft 1.363 / 2.914	Loss_tri_soft 0.300 / 0.259	Loss_lf 4.414 / 2.835	Loss_lf_tri 0.296 / 0.258	Prec 88.00% / 83.56%	
Epoch: [1][120/200]	Time 1.308 (2.351)	Data 0.000 (1.051)	Loss_ce 1.395 / 2.031	Loss_ce_soft 1.365 / 2.926	Loss_tri_soft 0.299 / 0.259	Loss_lf 4.422 / 2.850	Loss_lf_tri 0.297 / 0.258	Prec 88.49% / 82.92%	
Epoch: [1][140/200]	Time 1.312 (2.809)	Data 0.000 (1.201)	Loss_ce 1.407 / 2.021	Loss_ce_soft 1.378 / 2.934	Loss_tri_soft 0.297 / 0.263	Loss_lf 4.417 / 2.869	Loss_lf_tri 0.297 / 0.264	Prec 88.12% / 83.62%	
Epoch: [1][160/200]	Time 1.339 (2.617)	Data 0.001 (1.051)	Loss_ce 1.394 / 2.019	Loss_ce_soft 1.363 / 2.938	Loss_tri_soft 0.287 / 0.264	Loss_lf 4.418 / 2.879	Loss_lf_tri 0.291 / 0.267	Prec 88.79% / 83.52%	
Epoch: [1][180/200]	Time 1.318 (2.710)	Data 0.001 (1.174)	Loss_ce 1.383 / 2.010	Loss_ce_soft 1.357 / 2.930	Loss_tri_soft 0.286 / 0.266	Loss_lf 4.414 / 2.886	Loss_lf_tri 0.293 / 0.270	Prec 89.27% / 83.75%	
Epoch: [1][200/200]	Time 1.265 (2.783)	Data 0.000 (1.269)	Loss_ce 1.385 / 2.009	Loss_ce_soft 1.355 / 2.933	Loss_tri_soft 0.282 / 0.271	Loss_lf 4.417 / 2.897	Loss_lf_tri 0.291 / 0.275	Prec 89.25% / 84.00%	
Extract Features: [50/302]	Time 0.166 (1.014)	Data 0.000 (0.840)	
Extract Features: [100/302]	Time 0.185 (0.601)	Data 0.000 (0.420)	
Extract Features: [150/302]	Time 0.168 (0.458)	Data 0.000 (0.280)	
Extract Features: [200/302]	Time 0.166 (0.386)	Data 0.000 (0.210)	
Extract Features: [250/302]	Time 0.188 (0.343)	Data 0.000 (0.168)	
Extract Features: [300/302]	Time 0.180 (0.314)	Data 0.000 (0.140)	
Mean AP: 62.1%

 * Finished phase   2 epoch   1  model no.1 mAP: 62.1%  best: 62.1% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.056771993637085
Clustering and labeling...

 Clustered into 122 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [2][20/200]	Time 1.266 (1.278)	Data 0.000 (0.000)	Loss_ce 1.425 / 1.906	Loss_ce_soft 1.377 / 2.795	Loss_tri_soft 0.305 / 0.322	Loss_lf 4.394 / 2.831	Loss_lf_tri 0.317 / 0.319	Prec 88.44% / 84.69%	
Epoch: [2][40/200]	Time 1.237 (2.353)	Data 0.000 (1.083)	Loss_ce 1.395 / 2.000	Loss_ce_soft 1.379 / 2.852	Loss_tri_soft 0.318 / 0.304	Loss_lf 4.459 / 2.879	Loss_lf_tri 0.315 / 0.305	Prec 88.75% / 82.66%	
Epoch: [2][60/200]	Time 1.283 (2.001)	Data 0.001 (0.722)	Loss_ce 1.372 / 1.988	Loss_ce_soft 1.344 / 2.845	Loss_tri_soft 0.295 / 0.290	Loss_lf 4.430 / 2.806	Loss_lf_tri 0.301 / 0.296	Prec 89.58% / 84.48%	
Epoch: [2][80/200]	Time 1.232 (2.369)	Data 0.001 (1.090)	Loss_ce 1.357 / 1.997	Loss_ce_soft 1.323 / 2.856	Loss_tri_soft 0.294 / 0.285	Loss_lf 4.413 / 2.832	Loss_lf_tri 0.303 / 0.292	Prec 90.55% / 84.45%	
Epoch: [2][100/200]	Time 1.235 (2.593)	Data 0.000 (1.300)	Loss_ce 1.350 / 2.012	Loss_ce_soft 1.316 / 2.885	Loss_tri_soft 0.285 / 0.285	Loss_lf 4.430 / 2.847	Loss_lf_tri 0.298 / 0.292	Prec 91.06% / 84.38%	
Epoch: [2][120/200]	Time 1.242 (2.372)	Data 0.000 (1.084)	Loss_ce 1.341 / 2.010	Loss_ce_soft 1.304 / 2.872	Loss_tri_soft 0.277 / 0.280	Loss_lf 4.431 / 2.831	Loss_lf_tri 0.294 / 0.285	Prec 91.51% / 84.17%	
Epoch: [2][140/200]	Time 1.236 (2.824)	Data 0.000 (1.233)	Loss_ce 1.343 / 2.014	Loss_ce_soft 1.298 / 2.890	Loss_tri_soft 0.279 / 0.285	Loss_lf 4.422 / 2.818	Loss_lf_tri 0.297 / 0.288	Prec 91.56% / 84.38%	
Epoch: [2][160/200]	Time 1.378 (2.899)	Data 0.000 (1.346)	Loss_ce 1.338 / 1.994	Loss_ce_soft 1.294 / 2.864	Loss_tri_soft 0.279 / 0.272	Loss_lf 4.415 / 2.820	Loss_lf_tri 0.296 / 0.278	Prec 91.64% / 84.73%	
Epoch: [2][180/200]	Time 1.320 (2.725)	Data 0.001 (1.196)	Loss_ce 1.331 / 1.983	Loss_ce_soft 1.292 / 2.853	Loss_tri_soft 0.271 / 0.268	Loss_lf 4.415 / 2.835	Loss_lf_tri 0.292 / 0.277	Prec 91.81% / 84.90%	
Epoch: [2][200/200]	Time 1.303 (2.791)	Data 0.000 (1.288)	Loss_ce 1.321 / 1.977	Loss_ce_soft 1.278 / 2.862	Loss_tri_soft 0.267 / 0.274	Loss_lf 4.419 / 2.843	Loss_lf_tri 0.289 / 0.283	Prec 92.03% / 84.97%	
Extract Features: [50/302]	Time 0.172 (1.023)	Data 0.001 (0.852)	
Extract Features: [100/302]	Time 0.993 (0.605)	Data 0.001 (0.426)	
Extract Features: [150/302]	Time 0.182 (0.460)	Data 0.000 (0.284)	
Extract Features: [200/302]	Time 0.165 (0.388)	Data 0.000 (0.213)	
Extract Features: [250/302]	Time 0.165 (0.344)	Data 0.000 (0.171)	
Extract Features: [300/302]	Time 0.181 (0.315)	Data 0.000 (0.142)	
Mean AP: 63.0%

 * Finished phase   2 epoch   2  model no.1 mAP: 63.0%  best: 63.0% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.503628253936768
Clustering and labeling...

 Clustered into 127 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [3][20/200]	Time 1.356 (1.280)	Data 0.000 (0.000)	Loss_ce 1.412 / 1.931	Loss_ce_soft 1.356 / 2.725	Loss_tri_soft 0.275 / 0.263	Loss_lf 4.524 / 2.773	Loss_lf_tri 0.321 / 0.267	Prec 88.75% / 86.88%	
Epoch: [3][40/200]	Time 1.352 (2.332)	Data 0.001 (1.033)	Loss_ce 1.435 / 1.948	Loss_ce_soft 1.329 / 2.830	Loss_tri_soft 0.274 / 0.258	Loss_lf 4.516 / 2.787	Loss_lf_tri 0.314 / 0.264	Prec 89.06% / 86.09%	
Epoch: [3][60/200]	Time 1.327 (2.013)	Data 0.000 (0.689)	Loss_ce 1.408 / 1.958	Loss_ce_soft 1.335 / 2.853	Loss_tri_soft 0.276 / 0.243	Loss_lf 4.492 / 2.811	Loss_lf_tri 0.313 / 0.262	Prec 89.69% / 86.98%	
Epoch: [3][80/200]	Time 1.366 (2.355)	Data 0.000 (1.034)	Loss_ce 1.385 / 1.954	Loss_ce_soft 1.325 / 2.834	Loss_tri_soft 0.281 / 0.233	Loss_lf 4.487 / 2.848	Loss_lf_tri 0.315 / 0.257	Prec 90.47% / 86.09%	
Epoch: [3][100/200]	Time 1.345 (2.569)	Data 0.000 (1.249)	Loss_ce 1.375 / 1.976	Loss_ce_soft 1.315 / 2.878	Loss_tri_soft 0.273 / 0.234	Loss_lf 4.481 / 2.858	Loss_lf_tri 0.306 / 0.259	Prec 90.62% / 86.31%	
Epoch: [3][120/200]	Time 1.246 (2.349)	Data 0.000 (1.041)	Loss_ce 1.380 / 1.987	Loss_ce_soft 1.324 / 2.878	Loss_tri_soft 0.277 / 0.240	Loss_lf 4.478 / 2.830	Loss_lf_tri 0.307 / 0.264	Prec 90.42% / 86.15%	
Epoch: [3][140/200]	Time 1.226 (2.795)	Data 0.000 (1.196)	Loss_ce 1.377 / 2.000	Loss_ce_soft 1.322 / 2.884	Loss_tri_soft 0.285 / 0.240	Loss_lf 4.476 / 2.873	Loss_lf_tri 0.316 / 0.261	Prec 90.71% / 85.54%	
Epoch: [3][160/200]	Time 1.250 (2.868)	Data 0.001 (1.307)	Loss_ce 1.370 / 1.984	Loss_ce_soft 1.316 / 2.881	Loss_tri_soft 0.284 / 0.243	Loss_lf 4.466 / 2.867	Loss_lf_tri 0.315 / 0.261	Prec 90.70% / 85.98%	
Epoch: [3][180/200]	Time 1.388 (2.698)	Data 0.000 (1.162)	Loss_ce 1.362 / 2.000	Loss_ce_soft 1.305 / 2.908	Loss_tri_soft 0.281 / 0.250	Loss_lf 4.467 / 2.889	Loss_lf_tri 0.311 / 0.266	Prec 90.87% / 85.66%	
Epoch: [3][200/200]	Time 1.248 (2.769)	Data 0.000 (1.256)	Loss_ce 1.358 / 1.996	Loss_ce_soft 1.296 / 2.913	Loss_tri_soft 0.274 / 0.248	Loss_lf 4.458 / 2.907	Loss_lf_tri 0.305 / 0.265	Prec 90.78% / 85.88%	
Extract Features: [50/302]	Time 0.168 (1.024)	Data 0.000 (0.853)	
Extract Features: [100/302]	Time 0.164 (0.596)	Data 0.001 (0.427)	
Extract Features: [150/302]	Time 0.167 (0.453)	Data 0.001 (0.285)	
Extract Features: [200/302]	Time 0.166 (0.382)	Data 0.000 (0.214)	
Extract Features: [250/302]	Time 0.169 (0.340)	Data 0.000 (0.171)	
Extract Features: [300/302]	Time 0.157 (0.310)	Data 0.000 (0.142)	
Mean AP: 62.9%

 * Finished phase   2 epoch   3  model no.1 mAP: 62.9%  best: 63.0%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.841840505599976
Clustering and labeling...

 Clustered into 122 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [4][20/200]	Time 1.356 (1.292)	Data 0.001 (0.000)	Loss_ce 1.375 / 1.978	Loss_ce_soft 1.313 / 2.855	Loss_tri_soft 0.228 / 0.250	Loss_lf 4.500 / 2.690	Loss_lf_tri 0.285 / 0.273	Prec 90.62% / 86.56%	
Epoch: [4][40/200]	Time 1.295 (2.425)	Data 0.000 (1.085)	Loss_ce 1.369 / 1.982	Loss_ce_soft 1.288 / 2.855	Loss_tri_soft 0.252 / 0.233	Loss_lf 4.478 / 2.837	Loss_lf_tri 0.299 / 0.261	Prec 90.47% / 86.25%	
Epoch: [4][60/200]	Time 1.300 (2.063)	Data 0.000 (0.724)	Loss_ce 1.368 / 1.998	Loss_ce_soft 1.273 / 2.876	Loss_tri_soft 0.247 / 0.243	Loss_lf 4.487 / 2.833	Loss_lf_tri 0.290 / 0.267	Prec 90.73% / 85.73%	
Epoch: [4][80/200]	Time 1.320 (2.437)	Data 0.000 (1.098)	Loss_ce 1.341 / 1.999	Loss_ce_soft 1.245 / 2.839	Loss_tri_soft 0.242 / 0.239	Loss_lf 4.483 / 2.851	Loss_lf_tri 0.288 / 0.268	Prec 91.88% / 85.62%	
Epoch: [4][100/200]	Time 1.278 (2.660)	Data 0.000 (1.318)	Loss_ce 1.336 / 2.007	Loss_ce_soft 1.244 / 2.829	Loss_tri_soft 0.251 / 0.226	Loss_lf 4.474 / 2.848	Loss_lf_tri 0.296 / 0.255	Prec 91.94% / 85.56%	
Epoch: [4][120/200]	Time 1.325 (2.440)	Data 0.000 (1.099)	Loss_ce 1.343 / 2.018	Loss_ce_soft 1.263 / 2.845	Loss_tri_soft 0.265 / 0.226	Loss_lf 4.481 / 2.865	Loss_lf_tri 0.307 / 0.257	Prec 91.56% / 85.47%	
Epoch: [4][140/200]	Time 1.412 (2.902)	Data 0.000 (1.257)	Loss_ce 1.334 / 2.004	Loss_ce_soft 1.253 / 2.815	Loss_tri_soft 0.260 / 0.229	Loss_lf 4.477 / 2.868	Loss_lf_tri 0.302 / 0.259	Prec 91.79% / 85.94%	
Epoch: [4][160/200]	Time 1.308 (2.984)	Data 0.001 (1.378)	Loss_ce 1.332 / 1.993	Loss_ce_soft 1.251 / 2.826	Loss_tri_soft 0.257 / 0.225	Loss_lf 4.475 / 2.864	Loss_lf_tri 0.301 / 0.256	Prec 92.11% / 86.41%	
Epoch: [4][180/200]	Time 1.367 (2.802)	Data 0.001 (1.225)	Loss_ce 1.330 / 1.976	Loss_ce_soft 1.252 / 2.811	Loss_tri_soft 0.254 / 0.229	Loss_lf 4.467 / 2.854	Loss_lf_tri 0.300 / 0.259	Prec 92.19% / 86.84%	
Epoch: [4][200/200]	Time 1.325 (2.875)	Data 0.000 (1.323)	Loss_ce 1.330 / 1.957	Loss_ce_soft 1.257 / 2.785	Loss_tri_soft 0.256 / 0.226	Loss_lf 4.464 / 2.857	Loss_lf_tri 0.303 / 0.255	Prec 92.12% / 87.22%	
Extract Features: [50/302]	Time 0.173 (1.074)	Data 0.000 (0.889)	
Extract Features: [100/302]	Time 0.171 (0.625)	Data 0.001 (0.445)	
Extract Features: [150/302]	Time 0.167 (0.472)	Data 0.000 (0.296)	
Extract Features: [200/302]	Time 0.164 (0.396)	Data 0.001 (0.222)	
Extract Features: [250/302]	Time 0.183 (0.351)	Data 0.001 (0.178)	
Extract Features: [300/302]	Time 0.181 (0.321)	Data 0.001 (0.148)	
Mean AP: 62.9%

 * Finished phase   2 epoch   4  model no.1 mAP: 62.9%  best: 63.0%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.803853273391724
Clustering and labeling...

 Clustered into 125 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [5][20/200]	Time 1.349 (1.405)	Data 0.000 (0.000)	Loss_ce 1.392 / 1.927	Loss_ce_soft 1.299 / 2.799	Loss_tri_soft 0.224 / 0.289	Loss_lf 4.379 / 2.783	Loss_lf_tri 0.279 / 0.301	Prec 90.00% / 86.56%	
Epoch: [5][40/200]	Time 1.334 (2.465)	Data 0.000 (1.095)	Loss_ce 1.378 / 1.915	Loss_ce_soft 1.283 / 2.751	Loss_tri_soft 0.269 / 0.257	Loss_lf 4.434 / 2.740	Loss_lf_tri 0.309 / 0.281	Prec 91.25% / 86.25%	
Epoch: [5][60/200]	Time 1.312 (2.089)	Data 0.000 (0.730)	Loss_ce 1.364 / 1.971	Loss_ce_soft 1.278 / 2.824	Loss_tri_soft 0.268 / 0.249	Loss_lf 4.399 / 2.806	Loss_lf_tri 0.312 / 0.274	Prec 91.46% / 86.04%	
Epoch: [5][80/200]	Time 1.334 (2.434)	Data 0.000 (1.090)	Loss_ce 1.347 / 1.976	Loss_ce_soft 1.275 / 2.814	Loss_tri_soft 0.262 / 0.246	Loss_lf 4.424 / 2.830	Loss_lf_tri 0.307 / 0.273	Prec 91.80% / 86.09%	
Epoch: [5][100/200]	Time 1.306 (2.672)	Data 0.001 (1.324)	Loss_ce 1.347 / 1.998	Loss_ce_soft 1.257 / 2.839	Loss_tri_soft 0.252 / 0.254	Loss_lf 4.414 / 2.843	Loss_lf_tri 0.301 / 0.284	Prec 91.62% / 85.12%	
Epoch: [5][120/200]	Time 1.363 (2.450)	Data 0.000 (1.104)	Loss_ce 1.344 / 2.003	Loss_ce_soft 1.262 / 2.829	Loss_tri_soft 0.245 / 0.247	Loss_lf 4.409 / 2.864	Loss_lf_tri 0.298 / 0.278	Prec 91.41% / 84.95%	
Epoch: [5][140/200]	Time 1.240 (2.896)	Data 0.000 (1.250)	Loss_ce 1.341 / 1.998	Loss_ce_soft 1.265 / 2.821	Loss_tri_soft 0.247 / 0.243	Loss_lf 4.406 / 2.871	Loss_lf_tri 0.303 / 0.275	Prec 91.52% / 85.13%	
Epoch: [5][160/200]	Time 1.371 (2.984)	Data 0.000 (1.371)	Loss_ce 1.342 / 1.974	Loss_ce_soft 1.272 / 2.796	Loss_tri_soft 0.252 / 0.247	Loss_lf 4.416 / 2.871	Loss_lf_tri 0.307 / 0.277	Prec 91.45% / 85.78%	
Epoch: [5][180/200]	Time 1.364 (2.801)	Data 0.000 (1.219)	Loss_ce 1.343 / 1.980	Loss_ce_soft 1.281 / 2.812	Loss_tri_soft 0.255 / 0.251	Loss_lf 4.419 / 2.878	Loss_lf_tri 0.309 / 0.281	Prec 91.25% / 85.38%	
Epoch: [5][200/200]	Time 1.300 (2.880)	Data 0.000 (1.320)	Loss_ce 1.338 / 1.970	Loss_ce_soft 1.283 / 2.819	Loss_tri_soft 0.248 / 0.246	Loss_lf 4.420 / 2.888	Loss_lf_tri 0.303 / 0.277	Prec 91.38% / 85.62%	
Extract Features: [50/302]	Time 0.168 (1.039)	Data 0.001 (0.867)	
Extract Features: [100/302]	Time 0.169 (0.604)	Data 0.001 (0.434)	
Extract Features: [150/302]	Time 0.165 (0.459)	Data 0.000 (0.289)	
Extract Features: [200/302]	Time 0.164 (0.386)	Data 0.000 (0.217)	
Extract Features: [250/302]	Time 0.166 (0.343)	Data 0.001 (0.174)	
Extract Features: [300/302]	Time 0.161 (0.313)	Data 0.000 (0.145)	
Mean AP: 63.5%

 * Finished phase   2 epoch   5  model no.1 mAP: 63.5%  best: 63.5% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.764544486999512
Clustering and labeling...

 Clustered into 122 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [6][20/200]	Time 1.337 (1.390)	Data 0.000 (0.000)	Loss_ce 1.447 / 1.972	Loss_ce_soft 1.329 / 2.763	Loss_tri_soft 0.301 / 0.237	Loss_lf 4.496 / 2.857	Loss_lf_tri 0.341 / 0.268	Prec 88.44% / 85.00%	
Epoch: [6][40/200]	Time 1.379 (2.456)	Data 0.000 (1.094)	Loss_ce 1.423 / 1.948	Loss_ce_soft 1.289 / 2.761	Loss_tri_soft 0.286 / 0.226	Loss_lf 4.454 / 2.796	Loss_lf_tri 0.324 / 0.257	Prec 89.84% / 85.00%	
Epoch: [6][60/200]	Time 1.337 (2.084)	Data 0.000 (0.730)	Loss_ce 1.406 / 2.004	Loss_ce_soft 1.281 / 2.822	Loss_tri_soft 0.265 / 0.235	Loss_lf 4.445 / 2.848	Loss_lf_tri 0.311 / 0.268	Prec 89.69% / 84.58%	
Epoch: [6][80/200]	Time 1.354 (2.454)	Data 0.000 (1.100)	Loss_ce 1.396 / 2.018	Loss_ce_soft 1.273 / 2.815	Loss_tri_soft 0.269 / 0.235	Loss_lf 4.431 / 2.871	Loss_lf_tri 0.315 / 0.270	Prec 89.92% / 84.92%	
Epoch: [6][100/200]	Time 1.233 (2.655)	Data 0.000 (1.310)	Loss_ce 1.394 / 2.043	Loss_ce_soft 1.293 / 2.858	Loss_tri_soft 0.256 / 0.246	Loss_lf 4.432 / 2.914	Loss_lf_tri 0.308 / 0.279	Prec 90.06% / 84.94%	
Epoch: [6][120/200]	Time 1.325 (2.423)	Data 0.000 (1.092)	Loss_ce 1.380 / 2.022	Loss_ce_soft 1.285 / 2.830	Loss_tri_soft 0.254 / 0.237	Loss_lf 4.441 / 2.921	Loss_lf_tri 0.307 / 0.271	Prec 90.57% / 85.62%	
Epoch: [6][140/200]	Time 1.350 (2.902)	Data 0.000 (1.249)	Loss_ce 1.374 / 2.023	Loss_ce_soft 1.287 / 2.829	Loss_tri_soft 0.255 / 0.244	Loss_lf 4.444 / 2.925	Loss_lf_tri 0.308 / 0.277	Prec 90.36% / 85.31%	
Epoch: [6][160/200]	Time 1.318 (2.970)	Data 0.000 (1.358)	Loss_ce 1.366 / 2.011	Loss_ce_soft 1.282 / 2.836	Loss_tri_soft 0.250 / 0.249	Loss_lf 4.448 / 2.903	Loss_lf_tri 0.307 / 0.281	Prec 90.70% / 85.20%	
Epoch: [6][180/200]	Time 1.352 (2.795)	Data 0.000 (1.208)	Loss_ce 1.360 / 1.994	Loss_ce_soft 1.275 / 2.818	Loss_tri_soft 0.249 / 0.252	Loss_lf 4.444 / 2.889	Loss_lf_tri 0.306 / 0.285	Prec 90.97% / 85.66%	
Epoch: [6][200/200]	Time 1.351 (2.865)	Data 0.000 (1.302)	Loss_ce 1.359 / 1.996	Loss_ce_soft 1.269 / 2.818	Loss_tri_soft 0.247 / 0.254	Loss_lf 4.444 / 2.892	Loss_lf_tri 0.304 / 0.285	Prec 90.78% / 85.75%	
Extract Features: [50/302]	Time 0.168 (1.066)	Data 0.001 (0.896)	
Extract Features: [100/302]	Time 0.169 (0.617)	Data 0.001 (0.448)	
Extract Features: [150/302]	Time 0.170 (0.468)	Data 0.000 (0.299)	
Extract Features: [200/302]	Time 0.175 (0.394)	Data 0.001 (0.224)	
Extract Features: [250/302]	Time 0.168 (0.349)	Data 0.000 (0.179)	
Extract Features: [300/302]	Time 0.185 (0.319)	Data 0.000 (0.150)	
Mean AP: 63.2%

 * Finished phase   2 epoch   6  model no.1 mAP: 63.2%  best: 63.5%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.739873886108398
Clustering and labeling...

 Clustered into 129 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [7][20/200]	Time 1.305 (1.316)	Data 0.000 (0.000)	Loss_ce 1.398 / 2.050	Loss_ce_soft 1.381 / 2.980	Loss_tri_soft 0.236 / 0.314	Loss_lf 4.417 / 3.158	Loss_lf_tri 0.303 / 0.340	Prec 89.06% / 82.19%	
Epoch: [7][40/200]	Time 1.360 (2.415)	Data 0.000 (1.075)	Loss_ce 1.388 / 2.017	Loss_ce_soft 1.331 / 2.842	Loss_tri_soft 0.197 / 0.284	Loss_lf 4.418 / 3.035	Loss_lf_tri 0.272 / 0.306	Prec 89.38% / 83.44%	
Epoch: [7][60/200]	Time 1.381 (2.059)	Data 0.000 (0.717)	Loss_ce 1.372 / 2.030	Loss_ce_soft 1.334 / 2.873	Loss_tri_soft 0.182 / 0.269	Loss_lf 4.429 / 2.961	Loss_lf_tri 0.258 / 0.295	Prec 90.10% / 84.06%	
Epoch: [7][80/200]	Time 1.343 (2.410)	Data 0.000 (1.075)	Loss_ce 1.358 / 2.015	Loss_ce_soft 1.324 / 2.847	Loss_tri_soft 0.189 / 0.246	Loss_lf 4.417 / 2.899	Loss_lf_tri 0.263 / 0.277	Prec 90.39% / 84.53%	
Epoch: [7][100/200]	Time 1.233 (2.620)	Data 0.001 (1.291)	Loss_ce 1.361 / 2.005	Loss_ce_soft 1.341 / 2.826	Loss_tri_soft 0.207 / 0.239	Loss_lf 4.420 / 2.866	Loss_lf_tri 0.274 / 0.275	Prec 90.38% / 84.75%	
Epoch: [7][120/200]	Time 1.373 (2.397)	Data 0.001 (1.076)	Loss_ce 1.359 / 2.005	Loss_ce_soft 1.329 / 2.813	Loss_tri_soft 0.216 / 0.233	Loss_lf 4.413 / 2.881	Loss_lf_tri 0.284 / 0.272	Prec 90.31% / 84.79%	
Epoch: [7][140/200]	Time 1.270 (2.858)	Data 0.000 (1.232)	Loss_ce 1.360 / 2.014	Loss_ce_soft 1.335 / 2.818	Loss_tri_soft 0.222 / 0.230	Loss_lf 4.421 / 2.872	Loss_lf_tri 0.292 / 0.270	Prec 90.31% / 84.69%	
Epoch: [7][160/200]	Time 1.229 (2.665)	Data 0.000 (1.078)	Loss_ce 1.345 / 1.995	Loss_ce_soft 1.312 / 2.815	Loss_tri_soft 0.225 / 0.225	Loss_lf 4.407 / 2.882	Loss_lf_tri 0.292 / 0.265	Prec 90.70% / 84.77%	
Epoch: [7][180/200]	Time 1.332 (2.752)	Data 0.000 (1.195)	Loss_ce 1.354 / 1.992	Loss_ce_soft 1.320 / 2.817	Loss_tri_soft 0.229 / 0.227	Loss_lf 4.418 / 2.887	Loss_lf_tri 0.296 / 0.269	Prec 90.49% / 85.07%	
Epoch: [7][200/200]	Time 1.323 (2.825)	Data 0.000 (1.292)	Loss_ce 1.352 / 1.985	Loss_ce_soft 1.322 / 2.819	Loss_tri_soft 0.226 / 0.228	Loss_lf 4.413 / 2.878	Loss_lf_tri 0.294 / 0.271	Prec 90.53% / 85.16%	
Extract Features: [50/302]	Time 0.166 (1.031)	Data 0.000 (0.862)	
Extract Features: [100/302]	Time 0.173 (0.601)	Data 0.001 (0.431)	
Extract Features: [150/302]	Time 0.167 (0.457)	Data 0.000 (0.288)	
Extract Features: [200/302]	Time 0.167 (0.386)	Data 0.000 (0.216)	
Extract Features: [250/302]	Time 0.170 (0.342)	Data 0.000 (0.173)	
Extract Features: [300/302]	Time 0.181 (0.314)	Data 0.001 (0.144)	
Mean AP: 63.9%

 * Finished phase   2 epoch   7  model no.1 mAP: 63.9%  best: 63.9% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.706883430480957
Clustering and labeling...

 Clustered into 136 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [8][20/200]	Time 1.368 (1.332)	Data 0.000 (0.000)	Loss_ce 1.392 / 1.881	Loss_ce_soft 1.324 / 2.735	Loss_tri_soft 0.194 / 0.198	Loss_lf 4.406 / 2.873	Loss_lf_tri 0.262 / 0.238	Prec 90.31% / 85.31%	
Epoch: [8][40/200]	Time 1.268 (2.360)	Data 0.000 (1.050)	Loss_ce 1.369 / 1.914	Loss_ce_soft 1.313 / 2.712	Loss_tri_soft 0.220 / 0.212	Loss_lf 4.435 / 2.880	Loss_lf_tri 0.289 / 0.258	Prec 91.56% / 86.09%	
Epoch: [8][60/200]	Time 1.307 (1.994)	Data 0.000 (0.700)	Loss_ce 1.386 / 1.916	Loss_ce_soft 1.330 / 2.729	Loss_tri_soft 0.217 / 0.207	Loss_lf 4.450 / 2.887	Loss_lf_tri 0.282 / 0.259	Prec 90.62% / 86.77%	
Epoch: [8][80/200]	Time 1.293 (2.362)	Data 0.000 (1.054)	Loss_ce 1.392 / 1.944	Loss_ce_soft 1.332 / 2.774	Loss_tri_soft 0.215 / 0.211	Loss_lf 4.437 / 2.907	Loss_lf_tri 0.279 / 0.264	Prec 90.62% / 86.48%	
Epoch: [8][100/200]	Time 1.293 (2.151)	Data 0.000 (0.843)	Loss_ce 1.378 / 1.971	Loss_ce_soft 1.320 / 2.796	Loss_tri_soft 0.220 / 0.211	Loss_lf 4.431 / 2.904	Loss_lf_tri 0.282 / 0.263	Prec 90.69% / 85.88%	
Epoch: [8][120/200]	Time 1.240 (2.362)	Data 0.001 (1.060)	Loss_ce 1.372 / 1.985	Loss_ce_soft 1.319 / 2.784	Loss_tri_soft 0.225 / 0.205	Loss_lf 4.429 / 2.896	Loss_lf_tri 0.287 / 0.256	Prec 91.20% / 85.31%	
Epoch: [8][140/200]	Time 1.230 (2.508)	Data 0.000 (1.210)	Loss_ce 1.363 / 1.998	Loss_ce_soft 1.317 / 2.784	Loss_tri_soft 0.218 / 0.209	Loss_lf 4.434 / 2.909	Loss_lf_tri 0.282 / 0.258	Prec 91.65% / 85.04%	
Epoch: [8][160/200]	Time 1.324 (2.627)	Data 0.000 (1.059)	Loss_ce 1.358 / 1.989	Loss_ce_soft 1.308 / 2.790	Loss_tri_soft 0.213 / 0.214	Loss_lf 4.429 / 2.919	Loss_lf_tri 0.278 / 0.264	Prec 91.76% / 85.43%	
Epoch: [8][180/200]	Time 1.234 (2.714)	Data 0.001 (1.177)	Loss_ce 1.360 / 1.990	Loss_ce_soft 1.313 / 2.794	Loss_tri_soft 0.217 / 0.212	Loss_lf 4.432 / 2.908	Loss_lf_tri 0.282 / 0.262	Prec 91.46% / 85.24%	
Epoch: [8][200/200]	Time 1.362 (2.580)	Data 0.001 (1.059)	Loss_ce 1.356 / 1.973	Loss_ce_soft 1.308 / 2.782	Loss_tri_soft 0.217 / 0.207	Loss_lf 4.431 / 2.909	Loss_lf_tri 0.282 / 0.260	Prec 91.59% / 85.69%	
Extract Features: [50/302]	Time 0.167 (1.029)	Data 0.001 (0.853)	
Extract Features: [100/302]	Time 0.166 (0.600)	Data 0.000 (0.427)	
Extract Features: [150/302]	Time 0.167 (0.457)	Data 0.000 (0.285)	
Extract Features: [200/302]	Time 0.170 (0.386)	Data 0.001 (0.213)	
Extract Features: [250/302]	Time 0.168 (0.343)	Data 0.001 (0.171)	
Extract Features: [300/302]	Time 0.182 (0.314)	Data 0.000 (0.142)	
Mean AP: 63.5%

 * Finished phase   2 epoch   8  model no.1 mAP: 63.5%  best: 63.9%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.91581678390503
Clustering and labeling...

 Clustered into 131 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [9][20/200]	Time 1.296 (1.266)	Data 0.000 (0.000)	Loss_ce 1.343 / 1.928	Loss_ce_soft 1.225 / 2.731	Loss_tri_soft 0.228 / 0.202	Loss_lf 4.533 / 3.018	Loss_lf_tri 0.297 / 0.259	Prec 93.75% / 84.38%	
Epoch: [9][40/200]	Time 1.287 (2.336)	Data 0.000 (1.066)	Loss_ce 1.400 / 1.919	Loss_ce_soft 1.282 / 2.687	Loss_tri_soft 0.246 / 0.191	Loss_lf 4.513 / 2.858	Loss_lf_tri 0.307 / 0.245	Prec 91.09% / 84.53%	
Epoch: [9][60/200]	Time 1.278 (1.976)	Data 0.000 (0.711)	Loss_ce 1.387 / 1.930	Loss_ce_soft 1.285 / 2.694	Loss_tri_soft 0.230 / 0.196	Loss_lf 4.501 / 2.842	Loss_lf_tri 0.295 / 0.246	Prec 91.46% / 86.04%	
Epoch: [9][80/200]	Time 1.255 (2.344)	Data 0.001 (1.068)	Loss_ce 1.391 / 1.950	Loss_ce_soft 1.312 / 2.704	Loss_tri_soft 0.233 / 0.212	Loss_lf 4.489 / 2.854	Loss_lf_tri 0.302 / 0.262	Prec 91.09% / 85.39%	
Epoch: [9][100/200]	Time 1.286 (2.552)	Data 0.000 (1.279)	Loss_ce 1.392 / 1.932	Loss_ce_soft 1.308 / 2.671	Loss_tri_soft 0.238 / 0.207	Loss_lf 4.494 / 2.828	Loss_lf_tri 0.304 / 0.259	Prec 90.62% / 86.31%	
Epoch: [9][120/200]	Time 1.291 (2.337)	Data 0.000 (1.065)	Loss_ce 1.378 / 1.935	Loss_ce_soft 1.292 / 2.676	Loss_tri_soft 0.227 / 0.208	Loss_lf 4.473 / 2.838	Loss_lf_tri 0.296 / 0.260	Prec 90.89% / 86.67%	
Epoch: [9][140/200]	Time 44.535 (2.806)	Data 0.001 (1.220)	Loss_ce 1.372 / 1.951	Loss_ce_soft 1.294 / 2.700	Loss_tri_soft 0.228 / 0.210	Loss_lf 4.462 / 2.843	Loss_lf_tri 0.298 / 0.261	Prec 91.16% / 86.21%	
Epoch: [9][160/200]	Time 1.339 (2.617)	Data 0.001 (1.068)	Loss_ce 1.369 / 1.935	Loss_ce_soft 1.298 / 2.688	Loss_tri_soft 0.227 / 0.213	Loss_lf 4.462 / 2.829	Loss_lf_tri 0.298 / 0.265	Prec 91.25% / 86.60%	
Epoch: [9][180/200]	Time 1.228 (2.700)	Data 0.000 (1.185)	Loss_ce 1.360 / 1.921	Loss_ce_soft 1.298 / 2.673	Loss_tri_soft 0.228 / 0.216	Loss_lf 4.458 / 2.843	Loss_lf_tri 0.304 / 0.266	Prec 91.56% / 87.01%	
Epoch: [9][200/200]	Time 1.286 (2.771)	Data 0.001 (1.280)	Loss_ce 1.351 / 1.923	Loss_ce_soft 1.294 / 2.687	Loss_tri_soft 0.221 / 0.224	Loss_lf 4.453 / 2.853	Loss_lf_tri 0.298 / 0.273	Prec 91.94% / 87.00%	
Extract Features: [50/302]	Time 0.171 (1.033)	Data 0.001 (0.861)	
Extract Features: [100/302]	Time 0.168 (0.601)	Data 0.000 (0.430)	
Extract Features: [150/302]	Time 0.166 (0.464)	Data 0.001 (0.287)	
Extract Features: [200/302]	Time 0.168 (0.391)	Data 0.000 (0.215)	
Extract Features: [250/302]	Time 0.168 (0.347)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.159 (0.317)	Data 0.000 (0.144)	
Mean AP: 63.9%

 * Finished phase   2 epoch   9  model no.1 mAP: 63.9%  best: 63.9%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.77286171913147
Clustering and labeling...

 Clustered into 129 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [10][20/200]	Time 1.317 (1.269)	Data 0.000 (0.000)	Loss_ce 1.331 / 1.986	Loss_ce_soft 1.249 / 2.733	Loss_tri_soft 0.224 / 0.321	Loss_lf 4.484 / 2.933	Loss_lf_tri 0.306 / 0.330	Prec 90.00% / 83.75%	
Epoch: [10][40/200]	Time 1.314 (2.340)	Data 0.000 (1.070)	Loss_ce 1.362 / 1.965	Loss_ce_soft 1.265 / 2.706	Loss_tri_soft 0.212 / 0.245	Loss_lf 4.484 / 2.880	Loss_lf_tri 0.293 / 0.283	Prec 90.47% / 84.84%	
Epoch: [10][60/200]	Time 1.318 (2.000)	Data 0.000 (0.714)	Loss_ce 1.349 / 1.948	Loss_ce_soft 1.266 / 2.692	Loss_tri_soft 0.230 / 0.230	Loss_lf 4.488 / 2.866	Loss_lf_tri 0.302 / 0.276	Prec 91.04% / 85.94%	
Epoch: [10][80/200]	Time 1.263 (2.356)	Data 0.001 (1.069)	Loss_ce 1.350 / 1.944	Loss_ce_soft 1.287 / 2.679	Loss_tri_soft 0.247 / 0.212	Loss_lf 4.470 / 2.897	Loss_lf_tri 0.319 / 0.261	Prec 91.25% / 85.78%	
Epoch: [10][100/200]	Time 1.233 (2.576)	Data 0.000 (1.286)	Loss_ce 1.334 / 1.962	Loss_ce_soft 1.270 / 2.739	Loss_tri_soft 0.231 / 0.220	Loss_lf 4.448 / 2.915	Loss_lf_tri 0.305 / 0.268	Prec 91.81% / 86.25%	
Epoch: [10][120/200]	Time 1.320 (2.373)	Data 0.001 (1.072)	Loss_ce 1.326 / 1.971	Loss_ce_soft 1.265 / 2.738	Loss_tri_soft 0.223 / 0.223	Loss_lf 4.463 / 2.875	Loss_lf_tri 0.299 / 0.271	Prec 91.77% / 86.20%	
Epoch: [10][140/200]	Time 1.249 (2.823)	Data 0.000 (1.222)	Loss_ce 1.324 / 1.973	Loss_ce_soft 1.261 / 2.722	Loss_tri_soft 0.222 / 0.223	Loss_lf 4.451 / 2.878	Loss_lf_tri 0.299 / 0.273	Prec 91.96% / 85.89%	
Epoch: [10][160/200]	Time 1.333 (2.629)	Data 0.000 (1.069)	Loss_ce 1.323 / 1.958	Loss_ce_soft 1.254 / 2.721	Loss_tri_soft 0.212 / 0.226	Loss_lf 4.447 / 2.879	Loss_lf_tri 0.291 / 0.275	Prec 91.68% / 86.52%	
Epoch: [10][180/200]	Time 1.301 (2.712)	Data 0.000 (1.182)	Loss_ce 1.330 / 1.948	Loss_ce_soft 1.267 / 2.717	Loss_tri_soft 0.216 / 0.224	Loss_lf 4.448 / 2.872	Loss_lf_tri 0.295 / 0.273	Prec 91.49% / 86.81%	
Epoch: [10][200/200]	Time 1.320 (2.786)	Data 0.000 (1.277)	Loss_ce 1.328 / 1.937	Loss_ce_soft 1.275 / 2.707	Loss_tri_soft 0.221 / 0.223	Loss_lf 4.457 / 2.871	Loss_lf_tri 0.299 / 0.272	Prec 91.56% / 86.88%	
Extract Features: [50/302]	Time 0.168 (1.036)	Data 0.000 (0.861)	
Extract Features: [100/302]	Time 0.185 (0.603)	Data 0.000 (0.431)	
Extract Features: [150/302]	Time 0.165 (0.458)	Data 0.001 (0.287)	
Extract Features: [200/302]	Time 0.188 (0.386)	Data 0.000 (0.216)	
Extract Features: [250/302]	Time 0.172 (0.343)	Data 0.001 (0.172)	
Extract Features: [300/302]	Time 0.158 (0.314)	Data 0.000 (0.144)	
Mean AP: 63.6%

 * Finished phase   2 epoch  10  model no.1 mAP: 63.6%  best: 63.9%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.247708797454834
Clustering and labeling...

 Clustered into 130 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [11][20/200]	Time 1.334 (1.326)	Data 0.000 (0.000)	Loss_ce 1.416 / 1.908	Loss_ce_soft 1.352 / 2.660	Loss_tri_soft 0.259 / 0.206	Loss_lf 4.434 / 2.978	Loss_lf_tri 0.311 / 0.264	Prec 89.38% / 84.38%	
Epoch: [11][40/200]	Time 1.271 (2.326)	Data 0.000 (1.031)	Loss_ce 1.397 / 1.964	Loss_ce_soft 1.319 / 2.706	Loss_tri_soft 0.229 / 0.226	Loss_lf 4.430 / 2.928	Loss_lf_tri 0.292 / 0.283	Prec 90.47% / 84.22%	
Epoch: [11][60/200]	Time 1.297 (1.978)	Data 0.000 (0.687)	Loss_ce 1.377 / 1.992	Loss_ce_soft 1.297 / 2.750	Loss_tri_soft 0.220 / 0.221	Loss_lf 4.438 / 2.979	Loss_lf_tri 0.291 / 0.290	Prec 90.52% / 84.90%	
Epoch: [11][80/200]	Time 1.284 (2.321)	Data 0.001 (1.033)	Loss_ce 1.375 / 1.974	Loss_ce_soft 1.307 / 2.729	Loss_tri_soft 0.224 / 0.217	Loss_lf 4.468 / 2.915	Loss_lf_tri 0.299 / 0.281	Prec 90.94% / 85.23%	
Epoch: [11][100/200]	Time 1.271 (2.542)	Data 0.000 (1.250)	Loss_ce 1.383 / 1.990	Loss_ce_soft 1.322 / 2.740	Loss_tri_soft 0.229 / 0.241	Loss_lf 4.463 / 2.909	Loss_lf_tri 0.301 / 0.293	Prec 90.56% / 84.88%	
Epoch: [11][120/200]	Time 1.291 (2.336)	Data 0.000 (1.042)	Loss_ce 1.372 / 2.003	Loss_ce_soft 1.303 / 2.768	Loss_tri_soft 0.224 / 0.236	Loss_lf 4.450 / 2.946	Loss_lf_tri 0.293 / 0.287	Prec 90.78% / 85.26%	
Epoch: [11][140/200]	Time 1.263 (2.789)	Data 0.000 (1.195)	Loss_ce 1.370 / 2.016	Loss_ce_soft 1.305 / 2.779	Loss_tri_soft 0.232 / 0.246	Loss_lf 4.456 / 2.932	Loss_lf_tri 0.299 / 0.298	Prec 90.62% / 85.22%	
Epoch: [11][160/200]	Time 1.230 (2.598)	Data 0.000 (1.046)	Loss_ce 1.360 / 1.998	Loss_ce_soft 1.292 / 2.779	Loss_tri_soft 0.228 / 0.244	Loss_lf 4.453 / 2.941	Loss_lf_tri 0.296 / 0.295	Prec 90.82% / 85.62%	
Epoch: [11][180/200]	Time 1.334 (2.686)	Data 0.001 (1.165)	Loss_ce 1.348 / 1.987	Loss_ce_soft 1.275 / 2.775	Loss_tri_soft 0.224 / 0.244	Loss_lf 4.441 / 2.916	Loss_lf_tri 0.295 / 0.294	Prec 91.28% / 85.73%	
Epoch: [11][200/200]	Time 1.237 (2.760)	Data 0.000 (1.254)	Loss_ce 1.346 / 1.974	Loss_ce_soft 1.265 / 2.773	Loss_tri_soft 0.220 / 0.242	Loss_lf 4.436 / 2.910	Loss_lf_tri 0.294 / 0.293	Prec 91.28% / 85.97%	
Extract Features: [50/302]	Time 0.167 (1.023)	Data 0.001 (0.851)	
Extract Features: [100/302]	Time 0.165 (0.596)	Data 0.001 (0.426)	
Extract Features: [150/302]	Time 0.175 (0.454)	Data 0.000 (0.284)	
Extract Features: [200/302]	Time 0.164 (0.383)	Data 0.000 (0.213)	
Extract Features: [250/302]	Time 0.165 (0.340)	Data 0.000 (0.170)	
Extract Features: [300/302]	Time 0.181 (0.312)	Data 0.001 (0.142)	
Mean AP: 64.0%

 * Finished phase   2 epoch  11  model no.1 mAP: 64.0%  best: 64.0% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.691888570785522
Clustering and labeling...

 Clustered into 136 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [12][20/200]	Time 1.363 (1.277)	Data 0.000 (0.000)	Loss_ce 1.442 / 1.890	Loss_ce_soft 1.260 / 2.658	Loss_tri_soft 0.265 / 0.275	Loss_lf 4.523 / 2.855	Loss_lf_tri 0.328 / 0.306	Prec 89.38% / 85.94%	
Epoch: [12][40/200]	Time 1.237 (2.376)	Data 0.001 (1.081)	Loss_ce 1.411 / 1.947	Loss_ce_soft 1.241 / 2.682	Loss_tri_soft 0.253 / 0.266	Loss_lf 4.482 / 2.844	Loss_lf_tri 0.315 / 0.304	Prec 89.53% / 85.47%	
Epoch: [12][60/200]	Time 1.229 (2.034)	Data 0.000 (0.721)	Loss_ce 1.423 / 2.003	Loss_ce_soft 1.288 / 2.743	Loss_tri_soft 0.249 / 0.260	Loss_lf 4.472 / 2.903	Loss_lf_tri 0.313 / 0.303	Prec 88.54% / 84.48%	
Epoch: [12][80/200]	Time 1.271 (2.364)	Data 0.000 (1.061)	Loss_ce 1.412 / 1.986	Loss_ce_soft 1.276 / 2.737	Loss_tri_soft 0.248 / 0.254	Loss_lf 4.446 / 2.946	Loss_lf_tri 0.315 / 0.301	Prec 89.14% / 85.23%	
Epoch: [12][100/200]	Time 1.238 (2.142)	Data 0.000 (0.849)	Loss_ce 1.392 / 1.987	Loss_ce_soft 1.270 / 2.726	Loss_tri_soft 0.252 / 0.246	Loss_lf 4.447 / 2.917	Loss_lf_tri 0.316 / 0.297	Prec 89.94% / 85.31%	
Epoch: [12][120/200]	Time 1.242 (2.355)	Data 0.000 (1.068)	Loss_ce 1.384 / 2.008	Loss_ce_soft 1.277 / 2.753	Loss_tri_soft 0.243 / 0.247	Loss_lf 4.443 / 2.946	Loss_lf_tri 0.311 / 0.297	Prec 90.47% / 84.64%	
Epoch: [12][140/200]	Time 1.244 (2.506)	Data 0.000 (1.221)	Loss_ce 1.375 / 2.015	Loss_ce_soft 1.275 / 2.764	Loss_tri_soft 0.247 / 0.241	Loss_lf 4.446 / 2.954	Loss_lf_tri 0.316 / 0.293	Prec 90.67% / 84.51%	
Epoch: [12][160/200]	Time 1.288 (2.620)	Data 0.000 (1.068)	Loss_ce 1.372 / 1.998	Loss_ce_soft 1.283 / 2.754	Loss_tri_soft 0.247 / 0.232	Loss_lf 4.445 / 2.928	Loss_lf_tri 0.317 / 0.287	Prec 90.74% / 84.92%	
Epoch: [12][180/200]	Time 1.297 (2.702)	Data 0.000 (1.179)	Loss_ce 1.376 / 1.990	Loss_ce_soft 1.301 / 2.755	Loss_tri_soft 0.249 / 0.225	Loss_lf 4.453 / 2.917	Loss_lf_tri 0.318 / 0.284	Prec 90.45% / 84.76%	
Epoch: [12][200/200]	Time 1.264 (2.558)	Data 0.001 (1.061)	Loss_ce 1.368 / 1.968	Loss_ce_soft 1.300 / 2.749	Loss_tri_soft 0.244 / 0.222	Loss_lf 4.449 / 2.916	Loss_lf_tri 0.315 / 0.281	Prec 90.44% / 85.31%	
Extract Features: [50/302]	Time 0.168 (1.052)	Data 0.001 (0.859)	
Extract Features: [100/302]	Time 0.166 (0.611)	Data 0.001 (0.430)	
Extract Features: [150/302]	Time 0.169 (0.464)	Data 0.000 (0.287)	
Extract Features: [200/302]	Time 0.166 (0.391)	Data 0.000 (0.215)	
Extract Features: [250/302]	Time 0.167 (0.346)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.179 (0.317)	Data 0.000 (0.143)	
Mean AP: 63.8%

 * Finished phase   2 epoch  12  model no.1 mAP: 63.8%  best: 64.0%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.080764055252075
Clustering and labeling...

 Clustered into 133 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [13][20/200]	Time 1.358 (1.303)	Data 0.001 (0.000)	Loss_ce 1.347 / 1.982	Loss_ce_soft 1.232 / 2.742	Loss_tri_soft 0.179 / 0.200	Loss_lf 4.403 / 3.051	Loss_lf_tri 0.278 / 0.254	Prec 91.56% / 84.06%	
Epoch: [13][40/200]	Time 1.312 (2.347)	Data 0.001 (1.040)	Loss_ce 1.364 / 2.029	Loss_ce_soft 1.236 / 2.826	Loss_tri_soft 0.208 / 0.245	Loss_lf 4.415 / 3.032	Loss_lf_tri 0.296 / 0.291	Prec 90.94% / 83.44%	
Epoch: [13][60/200]	Time 1.227 (1.990)	Data 0.000 (0.694)	Loss_ce 1.352 / 2.014	Loss_ce_soft 1.221 / 2.767	Loss_tri_soft 0.215 / 0.242	Loss_lf 4.416 / 3.047	Loss_lf_tri 0.299 / 0.295	Prec 91.35% / 83.33%	
Epoch: [13][80/200]	Time 1.243 (2.345)	Data 0.001 (1.058)	Loss_ce 1.348 / 1.987	Loss_ce_soft 1.222 / 2.736	Loss_tri_soft 0.213 / 0.240	Loss_lf 4.426 / 3.079	Loss_lf_tri 0.301 / 0.289	Prec 91.56% / 84.77%	
Epoch: [13][100/200]	Time 43.805 (2.560)	Data 42.440 (1.271)	Loss_ce 1.341 / 1.982	Loss_ce_soft 1.212 / 2.736	Loss_tri_soft 0.217 / 0.242	Loss_lf 4.415 / 3.022	Loss_lf_tri 0.301 / 0.293	Prec 91.31% / 85.44%	
Epoch: [13][120/200]	Time 1.276 (2.356)	Data 0.001 (1.059)	Loss_ce 1.329 / 2.006	Loss_ce_soft 1.205 / 2.785	Loss_tri_soft 0.217 / 0.239	Loss_lf 4.417 / 3.004	Loss_lf_tri 0.303 / 0.291	Prec 91.77% / 84.79%	
Epoch: [13][140/200]	Time 44.111 (2.814)	Data 0.000 (1.214)	Loss_ce 1.328 / 2.004	Loss_ce_soft 1.204 / 2.779	Loss_tri_soft 0.216 / 0.225	Loss_lf 4.424 / 2.986	Loss_lf_tri 0.300 / 0.281	Prec 91.79% / 84.91%	
Epoch: [13][160/200]	Time 1.315 (2.627)	Data 0.000 (1.062)	Loss_ce 1.334 / 1.972	Loss_ce_soft 1.223 / 2.754	Loss_tri_soft 0.218 / 0.223	Loss_lf 4.425 / 2.952	Loss_lf_tri 0.301 / 0.279	Prec 91.95% / 85.86%	
Epoch: [13][180/200]	Time 1.300 (2.709)	Data 0.000 (1.174)	Loss_ce 1.331 / 1.968	Loss_ce_soft 1.225 / 2.752	Loss_tri_soft 0.218 / 0.226	Loss_lf 4.420 / 2.947	Loss_lf_tri 0.299 / 0.281	Prec 92.15% / 85.94%	
Epoch: [13][200/200]	Time 1.230 (2.776)	Data 0.000 (1.268)	Loss_ce 1.326 / 1.963	Loss_ce_soft 1.219 / 2.746	Loss_tri_soft 0.216 / 0.227	Loss_lf 4.414 / 2.943	Loss_lf_tri 0.299 / 0.282	Prec 92.25% / 85.97%	
Extract Features: [50/302]	Time 0.172 (1.035)	Data 0.000 (0.863)	
Extract Features: [100/302]	Time 0.167 (0.603)	Data 0.001 (0.432)	
Extract Features: [150/302]	Time 0.166 (0.459)	Data 0.000 (0.288)	
Extract Features: [200/302]	Time 0.166 (0.387)	Data 0.000 (0.216)	
Extract Features: [250/302]	Time 0.167 (0.344)	Data 0.001 (0.173)	
Extract Features: [300/302]	Time 0.182 (0.315)	Data 0.000 (0.144)	
Mean AP: 63.3%

 * Finished phase   2 epoch  13  model no.1 mAP: 63.3%  best: 64.0%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.930814266204834
Clustering and labeling...

 Clustered into 142 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [14][20/200]	Time 1.291 (1.282)	Data 0.000 (0.000)	Loss_ce 1.432 / 1.915	Loss_ce_soft 1.428 / 2.578	Loss_tri_soft 0.201 / 0.246	Loss_lf 4.449 / 2.706	Loss_lf_tri 0.288 / 0.319	Prec 90.62% / 85.62%	
Epoch: [14][40/200]	Time 1.317 (2.347)	Data 0.000 (1.072)	Loss_ce 1.421 / 1.962	Loss_ce_soft 1.391 / 2.679	Loss_tri_soft 0.216 / 0.231	Loss_lf 4.430 / 2.837	Loss_lf_tri 0.302 / 0.294	Prec 90.47% / 85.31%	
Epoch: [14][60/200]	Time 1.322 (2.027)	Data 0.001 (0.715)	Loss_ce 1.404 / 1.949	Loss_ce_soft 1.355 / 2.694	Loss_tri_soft 0.242 / 0.202	Loss_lf 4.446 / 2.852	Loss_lf_tri 0.324 / 0.273	Prec 90.52% / 86.04%	
Epoch: [14][80/200]	Time 1.353 (2.389)	Data 0.001 (1.077)	Loss_ce 1.407 / 1.977	Loss_ce_soft 1.338 / 2.701	Loss_tri_soft 0.228 / 0.190	Loss_lf 4.436 / 2.840	Loss_lf_tri 0.316 / 0.259	Prec 89.92% / 84.61%	
Epoch: [14][100/200]	Time 1.318 (2.178)	Data 0.000 (0.862)	Loss_ce 1.387 / 1.982	Loss_ce_soft 1.309 / 2.706	Loss_tri_soft 0.231 / 0.196	Loss_lf 4.430 / 2.899	Loss_lf_tri 0.315 / 0.265	Prec 90.50% / 84.00%	
Epoch: [14][120/200]	Time 1.233 (2.383)	Data 0.000 (1.076)	Loss_ce 1.381 / 1.979	Loss_ce_soft 1.308 / 2.698	Loss_tri_soft 0.239 / 0.188	Loss_lf 4.420 / 2.895	Loss_lf_tri 0.322 / 0.256	Prec 90.62% / 84.32%	
Epoch: [14][140/200]	Time 1.320 (2.231)	Data 0.000 (0.922)	Loss_ce 1.378 / 1.987	Loss_ce_soft 1.306 / 2.716	Loss_tri_soft 0.230 / 0.192	Loss_lf 4.420 / 2.930	Loss_lf_tri 0.316 / 0.258	Prec 90.49% / 84.64%	
Epoch: [14][160/200]	Time 1.235 (2.647)	Data 0.000 (1.076)	Loss_ce 1.372 / 1.971	Loss_ce_soft 1.304 / 2.716	Loss_tri_soft 0.231 / 0.189	Loss_lf 4.426 / 2.939	Loss_lf_tri 0.319 / 0.256	Prec 90.66% / 85.20%	
Epoch: [14][180/200]	Time 1.323 (2.734)	Data 0.000 (1.196)	Loss_ce 1.368 / 1.969	Loss_ce_soft 1.295 / 2.728	Loss_tri_soft 0.231 / 0.194	Loss_lf 4.416 / 2.926	Loss_lf_tri 0.318 / 0.260	Prec 90.73% / 85.38%	
Epoch: [14][200/200]	Time 1.315 (2.593)	Data 0.000 (1.076)	Loss_ce 1.365 / 1.961	Loss_ce_soft 1.295 / 2.725	Loss_tri_soft 0.231 / 0.197	Loss_lf 4.420 / 2.913	Loss_lf_tri 0.317 / 0.263	Prec 90.91% / 85.72%	
Extract Features: [50/302]	Time 0.186 (1.017)	Data 0.001 (0.844)	
Extract Features: [100/302]	Time 0.167 (0.606)	Data 0.000 (0.422)	
Extract Features: [150/302]	Time 0.170 (0.461)	Data 0.001 (0.282)	
Extract Features: [200/302]	Time 0.166 (0.388)	Data 0.000 (0.211)	
Extract Features: [250/302]	Time 0.166 (0.345)	Data 0.000 (0.169)	
Extract Features: [300/302]	Time 0.179 (0.316)	Data 0.000 (0.141)	
Mean AP: 64.1%

 * Finished phase   2 epoch  14  model no.1 mAP: 64.1%  best: 64.1% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.92281460762024
Clustering and labeling...

 Clustered into 138 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [15][20/200]	Time 1.322 (1.327)	Data 0.000 (0.000)	Loss_ce 1.399 / 1.972	Loss_ce_soft 1.160 / 2.730	Loss_tri_soft 0.196 / 0.200	Loss_lf 4.390 / 2.929	Loss_lf_tri 0.278 / 0.263	Prec 89.06% / 84.69%	
Epoch: [15][40/200]	Time 1.287 (2.396)	Data 0.000 (1.080)	Loss_ce 1.383 / 1.981	Loss_ce_soft 1.225 / 2.803	Loss_tri_soft 0.237 / 0.218	Loss_lf 4.434 / 2.929	Loss_lf_tri 0.308 / 0.275	Prec 90.78% / 84.69%	
Epoch: [15][60/200]	Time 1.292 (2.021)	Data 0.001 (0.720)	Loss_ce 1.374 / 1.993	Loss_ce_soft 1.229 / 2.800	Loss_tri_soft 0.215 / 0.214	Loss_lf 4.447 / 2.923	Loss_lf_tri 0.293 / 0.268	Prec 91.04% / 85.73%	
Epoch: [15][80/200]	Time 1.272 (2.378)	Data 0.000 (1.081)	Loss_ce 1.370 / 1.974	Loss_ce_soft 1.265 / 2.773	Loss_tri_soft 0.216 / 0.215	Loss_lf 4.449 / 2.922	Loss_lf_tri 0.298 / 0.273	Prec 91.25% / 86.17%	
Epoch: [15][100/200]	Time 1.252 (2.177)	Data 0.000 (0.865)	Loss_ce 1.361 / 1.962	Loss_ce_soft 1.253 / 2.731	Loss_tri_soft 0.205 / 0.205	Loss_lf 4.458 / 2.908	Loss_lf_tri 0.291 / 0.263	Prec 91.12% / 86.44%	
Epoch: [15][120/200]	Time 1.250 (2.372)	Data 0.000 (1.066)	Loss_ce 1.353 / 1.956	Loss_ce_soft 1.272 / 2.718	Loss_tri_soft 0.204 / 0.204	Loss_lf 4.458 / 2.927	Loss_lf_tri 0.289 / 0.265	Prec 91.35% / 86.41%	
Epoch: [15][140/200]	Time 1.231 (2.517)	Data 0.000 (1.218)	Loss_ce 1.345 / 1.950	Loss_ce_soft 1.265 / 2.715	Loss_tri_soft 0.204 / 0.201	Loss_lf 4.450 / 2.963	Loss_lf_tri 0.293 / 0.260	Prec 91.52% / 86.47%	
Epoch: [15][160/200]	Time 1.297 (2.629)	Data 0.000 (1.066)	Loss_ce 1.338 / 1.947	Loss_ce_soft 1.264 / 2.707	Loss_tri_soft 0.201 / 0.205	Loss_lf 4.445 / 2.937	Loss_lf_tri 0.290 / 0.263	Prec 91.72% / 86.29%	
Epoch: [15][180/200]	Time 1.266 (2.717)	Data 0.000 (1.183)	Loss_ce 1.340 / 1.950	Loss_ce_soft 1.268 / 2.715	Loss_tri_soft 0.203 / 0.206	Loss_lf 4.446 / 2.949	Loss_lf_tri 0.292 / 0.264	Prec 91.56% / 86.25%	
Epoch: [15][200/200]	Time 1.319 (2.576)	Data 0.001 (1.065)	Loss_ce 1.343 / 1.949	Loss_ce_soft 1.277 / 2.722	Loss_tri_soft 0.206 / 0.208	Loss_lf 4.449 / 2.957	Loss_lf_tri 0.296 / 0.266	Prec 91.44% / 86.28%	
Extract Features: [50/302]	Time 0.187 (1.042)	Data 0.000 (0.869)	
Extract Features: [100/302]	Time 0.168 (0.607)	Data 0.001 (0.434)	
Extract Features: [150/302]	Time 0.170 (0.462)	Data 0.000 (0.290)	
Extract Features: [200/302]	Time 0.169 (0.390)	Data 0.000 (0.217)	
Extract Features: [250/302]	Time 0.172 (0.346)	Data 0.000 (0.174)	
Extract Features: [300/302]	Time 0.162 (0.317)	Data 0.000 (0.145)	
Mean AP: 64.0%

 * Finished phase   2 epoch  15  model no.1 mAP: 64.0%  best: 64.1%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.011785507202148
Clustering and labeling...

 Clustered into 139 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [16][20/200]	Time 1.294 (1.310)	Data 0.000 (0.000)	Loss_ce 1.403 / 1.963	Loss_ce_soft 1.340 / 2.767	Loss_tri_soft 0.240 / 0.266	Loss_lf 4.470 / 2.751	Loss_lf_tri 0.327 / 0.322	Prec 90.94% / 86.56%	
Epoch: [16][40/200]	Time 1.241 (2.349)	Data 0.000 (1.049)	Loss_ce 1.397 / 1.956	Loss_ce_soft 1.330 / 2.741	Loss_tri_soft 0.228 / 0.245	Loss_lf 4.464 / 2.796	Loss_lf_tri 0.326 / 0.300	Prec 90.78% / 85.94%	
Epoch: [16][60/200]	Time 1.231 (2.013)	Data 0.000 (0.700)	Loss_ce 1.394 / 1.963	Loss_ce_soft 1.311 / 2.742	Loss_tri_soft 0.231 / 0.233	Loss_lf 4.453 / 2.870	Loss_lf_tri 0.327 / 0.290	Prec 90.42% / 85.62%	
Epoch: [16][80/200]	Time 1.229 (2.353)	Data 0.000 (1.053)	Loss_ce 1.383 / 1.947	Loss_ce_soft 1.296 / 2.686	Loss_tri_soft 0.229 / 0.232	Loss_lf 4.440 / 2.841	Loss_lf_tri 0.328 / 0.290	Prec 91.17% / 85.55%	
Epoch: [16][100/200]	Time 1.226 (2.134)	Data 0.000 (0.843)	Loss_ce 1.370 / 1.943	Loss_ce_soft 1.280 / 2.694	Loss_tri_soft 0.217 / 0.223	Loss_lf 4.423 / 2.897	Loss_lf_tri 0.316 / 0.282	Prec 91.62% / 85.94%	
Epoch: [16][120/200]	Time 1.270 (2.340)	Data 0.000 (1.051)	Loss_ce 1.362 / 1.950	Loss_ce_soft 1.277 / 2.676	Loss_tri_soft 0.219 / 0.212	Loss_lf 4.419 / 2.902	Loss_lf_tri 0.315 / 0.276	Prec 92.14% / 85.68%	
Epoch: [16][140/200]	Time 1.300 (2.493)	Data 0.000 (1.206)	Loss_ce 1.352 / 1.957	Loss_ce_soft 1.274 / 2.677	Loss_tri_soft 0.218 / 0.205	Loss_lf 4.424 / 2.909	Loss_lf_tri 0.313 / 0.270	Prec 92.28% / 85.71%	
Epoch: [16][160/200]	Time 1.249 (2.605)	Data 0.001 (1.055)	Loss_ce 1.345 / 1.943	Loss_ce_soft 1.266 / 2.692	Loss_tri_soft 0.213 / 0.210	Loss_lf 4.420 / 2.917	Loss_lf_tri 0.309 / 0.276	Prec 92.23% / 86.33%	
Epoch: [16][180/200]	Time 1.231 (2.692)	Data 0.000 (1.174)	Loss_ce 1.335 / 1.931	Loss_ce_soft 1.258 / 2.672	Loss_tri_soft 0.215 / 0.216	Loss_lf 4.422 / 2.909	Loss_lf_tri 0.313 / 0.282	Prec 92.57% / 86.49%	
Epoch: [16][200/200]	Time 1.270 (2.551)	Data 0.000 (1.057)	Loss_ce 1.331 / 1.918	Loss_ce_soft 1.252 / 2.667	Loss_tri_soft 0.217 / 0.210	Loss_lf 4.414 / 2.902	Loss_lf_tri 0.315 / 0.277	Prec 92.56% / 86.81%	
Extract Features: [50/302]	Time 0.167 (1.000)	Data 0.001 (0.830)	
Extract Features: [100/302]	Time 0.169 (0.586)	Data 0.001 (0.415)	
Extract Features: [150/302]	Time 0.166 (0.447)	Data 0.001 (0.277)	
Extract Features: [200/302]	Time 0.172 (0.383)	Data 0.000 (0.208)	
Extract Features: [250/302]	Time 0.186 (0.341)	Data 0.000 (0.166)	
Extract Features: [300/302]	Time 0.180 (0.313)	Data 0.000 (0.139)	
Mean AP: 64.1%

 * Finished phase   2 epoch  16  model no.1 mAP: 64.1%  best: 64.1% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.987794160842896
Clustering and labeling...

 Clustered into 143 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [17][20/200]	Time 1.291 (1.258)	Data 0.000 (0.000)	Loss_ce 1.422 / 1.817	Loss_ce_soft 1.258 / 2.578	Loss_tri_soft 0.208 / 0.219	Loss_lf 4.509 / 2.820	Loss_lf_tri 0.280 / 0.246	Prec 90.94% / 88.44%	
Epoch: [17][40/200]	Time 1.245 (2.323)	Data 0.000 (1.065)	Loss_ce 1.406 / 1.842	Loss_ce_soft 1.270 / 2.572	Loss_tri_soft 0.193 / 0.189	Loss_lf 4.401 / 2.836	Loss_lf_tri 0.286 / 0.232	Prec 91.56% / 86.88%	
Epoch: [17][60/200]	Time 1.317 (1.981)	Data 0.000 (0.710)	Loss_ce 1.398 / 1.870	Loss_ce_soft 1.278 / 2.593	Loss_tri_soft 0.185 / 0.183	Loss_lf 4.407 / 2.826	Loss_lf_tri 0.282 / 0.249	Prec 91.77% / 86.56%	
Epoch: [17][80/200]	Time 1.313 (2.334)	Data 0.000 (1.055)	Loss_ce 1.379 / 1.910	Loss_ce_soft 1.261 / 2.640	Loss_tri_soft 0.178 / 0.191	Loss_lf 4.409 / 2.848	Loss_lf_tri 0.281 / 0.256	Prec 92.11% / 85.70%	
Epoch: [17][100/200]	Time 1.284 (2.119)	Data 0.000 (0.844)	Loss_ce 1.371 / 1.901	Loss_ce_soft 1.252 / 2.612	Loss_tri_soft 0.190 / 0.199	Loss_lf 4.413 / 2.878	Loss_lf_tri 0.291 / 0.259	Prec 92.06% / 86.19%	
Epoch: [17][120/200]	Time 1.321 (2.334)	Data 0.000 (1.058)	Loss_ce 1.356 / 1.926	Loss_ce_soft 1.252 / 2.640	Loss_tri_soft 0.191 / 0.204	Loss_lf 4.420 / 2.907	Loss_lf_tri 0.288 / 0.265	Prec 92.50% / 86.04%	
Epoch: [17][140/200]	Time 1.284 (2.180)	Data 0.000 (0.907)	Loss_ce 1.347 / 1.923	Loss_ce_soft 1.251 / 2.629	Loss_tri_soft 0.192 / 0.201	Loss_lf 4.426 / 2.898	Loss_lf_tri 0.289 / 0.260	Prec 92.72% / 86.25%	
Epoch: [17][160/200]	Time 1.312 (2.604)	Data 0.000 (1.058)	Loss_ce 1.353 / 1.916	Loss_ce_soft 1.254 / 2.638	Loss_tri_soft 0.196 / 0.193	Loss_lf 4.425 / 2.887	Loss_lf_tri 0.293 / 0.254	Prec 92.30% / 86.64%	
Epoch: [17][180/200]	Time 1.285 (2.708)	Data 0.000 (1.179)	Loss_ce 1.353 / 1.910	Loss_ce_soft 1.258 / 2.642	Loss_tri_soft 0.197 / 0.201	Loss_lf 4.419 / 2.900	Loss_lf_tri 0.294 / 0.262	Prec 92.29% / 86.77%	
Epoch: [17][200/200]	Time 1.332 (2.566)	Data 0.000 (1.062)	Loss_ce 1.349 / 1.901	Loss_ce_soft 1.254 / 2.638	Loss_tri_soft 0.198 / 0.199	Loss_lf 4.415 / 2.927	Loss_lf_tri 0.294 / 0.262	Prec 92.28% / 87.06%	
Extract Features: [50/302]	Time 0.182 (1.025)	Data 0.000 (0.853)	
Extract Features: [100/302]	Time 0.168 (0.598)	Data 0.001 (0.427)	
Extract Features: [150/302]	Time 0.169 (0.455)	Data 0.000 (0.285)	
Extract Features: [200/302]	Time 0.172 (0.384)	Data 0.000 (0.214)	
Extract Features: [250/302]	Time 0.169 (0.341)	Data 0.001 (0.171)	
Extract Features: [300/302]	Time 0.181 (0.312)	Data 0.001 (0.142)	
Mean AP: 64.0%

 * Finished phase   2 epoch  17  model no.1 mAP: 64.0%  best: 64.1%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.02778172492981
Clustering and labeling...

 Clustered into 146 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [18][20/200]	Time 1.248 (1.271)	Data 0.001 (0.000)	Loss_ce 1.389 / 1.964	Loss_ce_soft 1.293 / 2.755	Loss_tri_soft 0.227 / 0.205	Loss_lf 4.491 / 3.144	Loss_lf_tri 0.315 / 0.273	Prec 89.69% / 84.38%	
Epoch: [18][40/200]	Time 1.255 (2.311)	Data 0.000 (1.043)	Loss_ce 1.383 / 1.935	Loss_ce_soft 1.230 / 2.671	Loss_tri_soft 0.200 / 0.202	Loss_lf 4.446 / 3.100	Loss_lf_tri 0.295 / 0.272	Prec 90.47% / 84.84%	
Epoch: [18][60/200]	Time 1.234 (1.958)	Data 0.001 (0.695)	Loss_ce 1.384 / 1.934	Loss_ce_soft 1.227 / 2.673	Loss_tri_soft 0.199 / 0.217	Loss_lf 4.408 / 3.018	Loss_lf_tri 0.292 / 0.283	Prec 90.21% / 85.73%	
Epoch: [18][80/200]	Time 1.250 (2.333)	Data 0.000 (1.043)	Loss_ce 1.381 / 1.954	Loss_ce_soft 1.242 / 2.702	Loss_tri_soft 0.185 / 0.220	Loss_lf 4.418 / 3.053	Loss_lf_tri 0.275 / 0.286	Prec 90.62% / 85.08%	
Epoch: [18][100/200]	Time 1.308 (2.130)	Data 0.000 (0.834)	Loss_ce 1.358 / 1.971	Loss_ce_soft 1.219 / 2.710	Loss_tri_soft 0.184 / 0.216	Loss_lf 4.396 / 2.998	Loss_lf_tri 0.273 / 0.283	Prec 91.19% / 85.19%	
Epoch: [18][120/200]	Time 1.263 (2.343)	Data 0.001 (1.047)	Loss_ce 1.354 / 1.962	Loss_ce_soft 1.210 / 2.700	Loss_tri_soft 0.186 / 0.216	Loss_lf 4.387 / 2.978	Loss_lf_tri 0.273 / 0.282	Prec 91.41% / 85.68%	
Epoch: [18][140/200]	Time 1.274 (2.189)	Data 0.000 (0.898)	Loss_ce 1.357 / 1.984	Loss_ce_soft 1.227 / 2.707	Loss_tri_soft 0.195 / 0.217	Loss_lf 4.386 / 2.990	Loss_lf_tri 0.281 / 0.285	Prec 91.34% / 85.27%	
Epoch: [18][160/200]	Time 1.304 (2.616)	Data 0.001 (1.055)	Loss_ce 1.350 / 1.965	Loss_ce_soft 1.220 / 2.690	Loss_tri_soft 0.191 / 0.219	Loss_lf 4.374 / 2.990	Loss_lf_tri 0.281 / 0.284	Prec 91.52% / 85.55%	
Epoch: [18][180/200]	Time 1.292 (2.473)	Data 0.000 (0.938)	Loss_ce 1.345 / 1.966	Loss_ce_soft 1.213 / 2.688	Loss_tri_soft 0.188 / 0.225	Loss_lf 4.381 / 2.992	Loss_lf_tri 0.279 / 0.289	Prec 91.70% / 85.49%	
Epoch: [18][200/200]	Time 1.257 (2.564)	Data 0.000 (1.056)	Loss_ce 1.337 / 1.951	Loss_ce_soft 1.212 / 2.673	Loss_tri_soft 0.190 / 0.222	Loss_lf 4.384 / 2.977	Loss_lf_tri 0.283 / 0.288	Prec 91.78% / 85.88%	
Extract Features: [50/302]	Time 0.166 (1.024)	Data 0.000 (0.850)	
Extract Features: [100/302]	Time 0.171 (0.596)	Data 0.001 (0.425)	
Extract Features: [150/302]	Time 0.167 (0.455)	Data 0.001 (0.283)	
Extract Features: [200/302]	Time 0.167 (0.384)	Data 0.000 (0.213)	
Extract Features: [250/302]	Time 0.166 (0.341)	Data 0.000 (0.170)	
Extract Features: [300/302]	Time 0.180 (0.318)	Data 0.000 (0.142)	
Mean AP: 64.4%

 * Finished phase   2 epoch  18  model no.1 mAP: 64.4%  best: 64.4% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.060770988464355
Clustering and labeling...

 Clustered into 141 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [19][20/200]	Time 1.349 (1.315)	Data 0.000 (0.000)	Loss_ce 1.413 / 1.970	Loss_ce_soft 1.345 / 2.725	Loss_tri_soft 0.190 / 0.186	Loss_lf 4.458 / 2.940	Loss_lf_tri 0.270 / 0.246	Prec 89.69% / 83.12%	
Epoch: [19][40/200]	Time 1.291 (2.390)	Data 0.000 (1.081)	Loss_ce 1.378 / 1.924	Loss_ce_soft 1.295 / 2.643	Loss_tri_soft 0.211 / 0.180	Loss_lf 4.415 / 2.902	Loss_lf_tri 0.296 / 0.247	Prec 90.62% / 84.69%	
Epoch: [19][60/200]	Time 1.231 (2.013)	Data 0.000 (0.721)	Loss_ce 1.349 / 1.929	Loss_ce_soft 1.281 / 2.672	Loss_tri_soft 0.214 / 0.176	Loss_lf 4.440 / 2.929	Loss_lf_tri 0.290 / 0.249	Prec 91.56% / 85.52%	
Epoch: [19][80/200]	Time 1.234 (2.360)	Data 0.000 (1.072)	Loss_ce 1.330 / 1.942	Loss_ce_soft 1.237 / 2.689	Loss_tri_soft 0.207 / 0.172	Loss_lf 4.430 / 2.914	Loss_lf_tri 0.294 / 0.248	Prec 91.80% / 85.31%	
Epoch: [19][100/200]	Time 1.238 (2.142)	Data 0.000 (0.858)	Loss_ce 1.330 / 1.929	Loss_ce_soft 1.242 / 2.676	Loss_tri_soft 0.206 / 0.171	Loss_lf 4.441 / 2.887	Loss_lf_tri 0.295 / 0.245	Prec 92.06% / 86.00%	
Epoch: [19][120/200]	Time 1.432 (2.375)	Data 0.000 (1.078)	Loss_ce 1.321 / 1.929	Loss_ce_soft 1.227 / 2.668	Loss_tri_soft 0.198 / 0.174	Loss_lf 4.437 / 2.892	Loss_lf_tri 0.292 / 0.246	Prec 92.45% / 86.15%	
Epoch: [19][140/200]	Time 1.384 (2.234)	Data 0.000 (0.924)	Loss_ce 1.320 / 1.933	Loss_ce_soft 1.227 / 2.648	Loss_tri_soft 0.204 / 0.174	Loss_lf 4.439 / 2.906	Loss_lf_tri 0.296 / 0.247	Prec 92.54% / 86.12%	
Epoch: [19][160/200]	Time 1.314 (2.668)	Data 0.000 (1.083)	Loss_ce 1.314 / 1.933	Loss_ce_soft 1.220 / 2.649	Loss_tri_soft 0.203 / 0.183	Loss_lf 4.441 / 2.918	Loss_lf_tri 0.295 / 0.253	Prec 92.73% / 86.05%	
Epoch: [19][180/200]	Time 1.348 (2.758)	Data 0.000 (1.202)	Loss_ce 1.315 / 1.929	Loss_ce_soft 1.223 / 2.656	Loss_tri_soft 0.207 / 0.187	Loss_lf 4.436 / 2.924	Loss_lf_tri 0.298 / 0.256	Prec 92.57% / 86.32%	
Epoch: [19][200/200]	Time 1.265 (2.616)	Data 0.000 (1.082)	Loss_ce 1.314 / 1.915	Loss_ce_soft 1.217 / 2.641	Loss_tri_soft 0.210 / 0.184	Loss_lf 4.436 / 2.934	Loss_lf_tri 0.301 / 0.252	Prec 92.78% / 86.84%	
Extract Features: [50/302]	Time 0.168 (1.009)	Data 0.000 (0.834)	
Extract Features: [100/302]	Time 0.169 (0.590)	Data 0.001 (0.417)	
Extract Features: [150/302]	Time 0.171 (0.451)	Data 0.001 (0.278)	
Extract Features: [200/302]	Time 0.168 (0.381)	Data 0.001 (0.209)	
Extract Features: [250/302]	Time 0.173 (0.340)	Data 0.000 (0.167)	
Extract Features: [300/302]	Time 0.180 (0.312)	Data 0.000 (0.139)	
Mean AP: 63.7%

 * Finished phase   2 epoch  19  model no.1 mAP: 63.7%  best: 64.4%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.629586219787598
Clustering and labeling...

 Clustered into 143 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [20][20/200]	Time 1.319 (1.284)	Data 0.001 (0.000)	Loss_ce 1.373 / 1.857	Loss_ce_soft 1.220 / 2.562	Loss_tri_soft 0.201 / 0.138	Loss_lf 4.419 / 2.923	Loss_lf_tri 0.304 / 0.211	Prec 89.69% / 88.12%	
Epoch: [20][40/200]	Time 1.267 (2.369)	Data 0.000 (1.080)	Loss_ce 1.362 / 1.893	Loss_ce_soft 1.221 / 2.618	Loss_tri_soft 0.190 / 0.138	Loss_lf 4.423 / 2.927	Loss_lf_tri 0.289 / 0.215	Prec 90.62% / 87.81%	
Epoch: [20][60/200]	Time 1.305 (2.013)	Data 0.000 (0.720)	Loss_ce 1.338 / 1.922	Loss_ce_soft 1.184 / 2.645	Loss_tri_soft 0.179 / 0.158	Loss_lf 4.409 / 3.017	Loss_lf_tri 0.278 / 0.237	Prec 91.25% / 86.46%	
Epoch: [20][80/200]	Time 1.325 (2.373)	Data 0.000 (1.082)	Loss_ce 1.339 / 1.935	Loss_ce_soft 1.175 / 2.646	Loss_tri_soft 0.182 / 0.175	Loss_lf 4.402 / 2.954	Loss_lf_tri 0.282 / 0.248	Prec 91.25% / 86.56%	
Epoch: [20][100/200]	Time 1.294 (2.170)	Data 0.000 (0.866)	Loss_ce 1.341 / 1.943	Loss_ce_soft 1.193 / 2.655	Loss_tri_soft 0.182 / 0.174	Loss_lf 4.420 / 2.965	Loss_lf_tri 0.283 / 0.246	Prec 91.06% / 86.62%	
Epoch: [20][120/200]	Time 1.266 (2.386)	Data 0.001 (1.086)	Loss_ce 1.342 / 1.950	Loss_ce_soft 1.212 / 2.659	Loss_tri_soft 0.187 / 0.174	Loss_lf 4.424 / 2.948	Loss_lf_tri 0.288 / 0.245	Prec 91.09% / 86.51%	
Epoch: [20][140/200]	Time 1.310 (2.230)	Data 0.000 (0.931)	Loss_ce 1.350 / 1.964	Loss_ce_soft 1.222 / 2.666	Loss_tri_soft 0.192 / 0.181	Loss_lf 4.416 / 2.925	Loss_lf_tri 0.290 / 0.249	Prec 90.76% / 86.12%	
Epoch: [20][160/200]	Time 1.353 (2.665)	Data 0.000 (1.089)	Loss_ce 1.351 / 1.947	Loss_ce_soft 1.223 / 2.651	Loss_tri_soft 0.189 / 0.189	Loss_lf 4.411 / 2.929	Loss_lf_tri 0.287 / 0.254	Prec 90.74% / 86.48%	
Epoch: [20][180/200]	Time 1.346 (2.761)	Data 0.001 (1.211)	Loss_ce 1.345 / 1.944	Loss_ce_soft 1.223 / 2.653	Loss_tri_soft 0.192 / 0.192	Loss_lf 4.415 / 2.922	Loss_lf_tri 0.289 / 0.257	Prec 90.80% / 86.63%	
Epoch: [20][200/200]	Time 1.341 (2.620)	Data 0.000 (1.090)	Loss_ce 1.337 / 1.940	Loss_ce_soft 1.214 / 2.652	Loss_tri_soft 0.193 / 0.197	Loss_lf 4.413 / 2.928	Loss_lf_tri 0.291 / 0.261	Prec 91.12% / 86.56%	
Extract Features: [50/302]	Time 0.170 (1.049)	Data 0.000 (0.874)	
Extract Features: [100/302]	Time 0.165 (0.609)	Data 0.000 (0.437)	
Extract Features: [150/302]	Time 0.186 (0.462)	Data 0.001 (0.291)	
Extract Features: [200/302]	Time 0.168 (0.389)	Data 0.001 (0.219)	
Extract Features: [250/302]	Time 0.166 (0.345)	Data 0.001 (0.175)	
Extract Features: [300/302]	Time 0.213 (0.317)	Data 0.000 (0.146)	
Mean AP: 64.3%

 * Finished phase   2 epoch  20  model no.1 mAP: 64.3%  best: 64.4%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.092760801315308
Clustering and labeling...

 Clustered into 135 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [21][20/200]	Time 1.322 (1.272)	Data 0.000 (0.000)	Loss_ce 1.291 / 1.872	Loss_ce_soft 1.115 / 2.657	Loss_tri_soft 0.159 / 0.176	Loss_lf 4.345 / 2.834	Loss_lf_tri 0.269 / 0.242	Prec 92.81% / 84.38%	
Epoch: [21][40/200]	Time 1.246 (2.345)	Data 0.000 (1.058)	Loss_ce 1.297 / 1.965	Loss_ce_soft 1.109 / 2.713	Loss_tri_soft 0.196 / 0.191	Loss_lf 4.411 / 2.875	Loss_lf_tri 0.308 / 0.262	Prec 93.28% / 83.28%	
Epoch: [21][60/200]	Time 1.306 (1.985)	Data 0.000 (0.705)	Loss_ce 1.322 / 1.952	Loss_ce_soft 1.150 / 2.637	Loss_tri_soft 0.202 / 0.208	Loss_lf 4.408 / 2.797	Loss_lf_tri 0.308 / 0.273	Prec 92.81% / 83.96%	
Epoch: [21][80/200]	Time 1.329 (2.339)	Data 0.000 (1.057)	Loss_ce 1.319 / 1.951	Loss_ce_soft 1.181 / 2.634	Loss_tri_soft 0.195 / 0.219	Loss_lf 4.415 / 2.863	Loss_lf_tri 0.302 / 0.280	Prec 92.73% / 84.61%	
Epoch: [21][100/200]	Time 42.915 (2.552)	Data 41.573 (1.261)	Loss_ce 1.322 / 1.952	Loss_ce_soft 1.182 / 2.651	Loss_tri_soft 0.203 / 0.208	Loss_lf 4.413 / 2.857	Loss_lf_tri 0.305 / 0.272	Prec 92.62% / 85.25%	
Epoch: [21][120/200]	Time 1.268 (2.338)	Data 0.000 (1.051)	Loss_ce 1.318 / 1.962	Loss_ce_soft 1.180 / 2.679	Loss_tri_soft 0.200 / 0.204	Loss_lf 4.394 / 2.900	Loss_lf_tri 0.301 / 0.268	Prec 92.92% / 85.57%	
Epoch: [21][140/200]	Time 1.287 (2.491)	Data 0.000 (1.206)	Loss_ce 1.328 / 1.960	Loss_ce_soft 1.202 / 2.668	Loss_tri_soft 0.210 / 0.201	Loss_lf 4.418 / 2.909	Loss_lf_tri 0.311 / 0.269	Prec 92.68% / 85.71%	
Epoch: [21][160/200]	Time 1.265 (2.606)	Data 0.000 (1.056)	Loss_ce 1.328 / 1.944	Loss_ce_soft 1.208 / 2.649	Loss_tri_soft 0.208 / 0.198	Loss_lf 4.420 / 2.913	Loss_lf_tri 0.308 / 0.267	Prec 92.89% / 85.94%	
Epoch: [21][180/200]	Time 1.311 (2.687)	Data 0.000 (1.167)	Loss_ce 1.320 / 1.930	Loss_ce_soft 1.194 / 2.637	Loss_tri_soft 0.202 / 0.194	Loss_lf 4.407 / 2.902	Loss_lf_tri 0.304 / 0.265	Prec 92.99% / 86.35%	
Epoch: [21][200/200]	Time 1.264 (2.758)	Data 0.000 (1.260)	Loss_ce 1.314 / 1.926	Loss_ce_soft 1.192 / 2.643	Loss_tri_soft 0.211 / 0.197	Loss_lf 4.406 / 2.901	Loss_lf_tri 0.312 / 0.268	Prec 93.19% / 86.69%	
Extract Features: [50/302]	Time 0.172 (1.037)	Data 0.000 (0.864)	
Extract Features: [100/302]	Time 0.169 (0.603)	Data 0.001 (0.432)	
Extract Features: [150/302]	Time 0.167 (0.467)	Data 0.000 (0.288)	
Extract Features: [200/302]	Time 0.186 (0.393)	Data 0.000 (0.216)	
Extract Features: [250/302]	Time 0.184 (0.350)	Data 0.000 (0.173)	
Extract Features: [300/302]	Time 0.181 (0.323)	Data 0.000 (0.144)	
Mean AP: 63.7%

 * Finished phase   2 epoch  21  model no.1 mAP: 63.7%  best: 64.4%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.009786367416382
Clustering and labeling...

 Clustered into 137 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [22][20/200]	Time 1.324 (1.269)	Data 0.001 (0.000)	Loss_ce 1.464 / 1.920	Loss_ce_soft 1.428 / 2.664	Loss_tri_soft 0.204 / 0.230	Loss_lf 4.433 / 2.937	Loss_lf_tri 0.328 / 0.302	Prec 92.50% / 85.62%	
Epoch: [22][40/200]	Time 1.253 (2.352)	Data 0.000 (1.068)	Loss_ce 1.410 / 1.931	Loss_ce_soft 1.319 / 2.657	Loss_tri_soft 0.221 / 0.226	Loss_lf 4.390 / 2.888	Loss_lf_tri 0.328 / 0.285	Prec 91.41% / 86.41%	
Epoch: [22][60/200]	Time 1.286 (1.990)	Data 0.000 (0.712)	Loss_ce 1.378 / 1.932	Loss_ce_soft 1.269 / 2.667	Loss_tri_soft 0.211 / 0.215	Loss_lf 4.385 / 2.968	Loss_lf_tri 0.311 / 0.279	Prec 92.29% / 86.77%	
Epoch: [22][80/200]	Time 1.305 (2.339)	Data 0.000 (1.060)	Loss_ce 1.355 / 1.920	Loss_ce_soft 1.238 / 2.631	Loss_tri_soft 0.202 / 0.201	Loss_lf 4.396 / 2.961	Loss_lf_tri 0.305 / 0.268	Prec 92.34% / 87.42%	
Epoch: [22][100/200]	Time 1.318 (2.133)	Data 0.000 (0.848)	Loss_ce 1.341 / 1.948	Loss_ce_soft 1.233 / 2.639	Loss_tri_soft 0.203 / 0.211	Loss_lf 4.401 / 2.894	Loss_lf_tri 0.305 / 0.277	Prec 92.44% / 86.06%	
Epoch: [22][120/200]	Time 1.300 (2.341)	Data 0.000 (1.056)	Loss_ce 1.331 / 1.941	Loss_ce_soft 1.225 / 2.616	Loss_tri_soft 0.200 / 0.209	Loss_lf 4.393 / 2.902	Loss_lf_tri 0.302 / 0.270	Prec 92.66% / 86.51%	
Epoch: [22][140/200]	Time 1.237 (2.491)	Data 0.001 (1.210)	Loss_ce 1.320 / 1.944	Loss_ce_soft 1.202 / 2.606	Loss_tri_soft 0.203 / 0.209	Loss_lf 4.410 / 2.903	Loss_lf_tri 0.305 / 0.272	Prec 92.63% / 85.98%	
Epoch: [22][160/200]	Time 1.262 (2.610)	Data 0.000 (1.058)	Loss_ce 1.311 / 1.937	Loss_ce_soft 1.188 / 2.627	Loss_tri_soft 0.195 / 0.214	Loss_lf 4.402 / 2.914	Loss_lf_tri 0.298 / 0.279	Prec 92.85% / 86.25%	
Epoch: [22][180/200]	Time 1.241 (2.698)	Data 0.000 (1.177)	Loss_ce 1.321 / 1.932	Loss_ce_soft 1.204 / 2.622	Loss_tri_soft 0.203 / 0.218	Loss_lf 4.413 / 2.895	Loss_lf_tri 0.304 / 0.283	Prec 92.53% / 86.39%	
Epoch: [22][200/200]	Time 1.276 (2.553)	Data 0.000 (1.060)	Loss_ce 1.314 / 1.921	Loss_ce_soft 1.199 / 2.610	Loss_tri_soft 0.203 / 0.220	Loss_lf 4.407 / 2.886	Loss_lf_tri 0.305 / 0.284	Prec 92.69% / 86.75%	
Extract Features: [50/302]	Time 0.168 (1.025)	Data 0.000 (0.853)	
Extract Features: [100/302]	Time 0.167 (0.598)	Data 0.000 (0.427)	
Extract Features: [150/302]	Time 0.167 (0.455)	Data 0.000 (0.284)	
Extract Features: [200/302]	Time 0.170 (0.384)	Data 0.000 (0.213)	
Extract Features: [250/302]	Time 0.166 (0.341)	Data 0.001 (0.171)	
Extract Features: [300/302]	Time 0.180 (0.313)	Data 0.000 (0.142)	
Mean AP: 64.0%

 * Finished phase   2 epoch  22  model no.1 mAP: 64.0%  best: 64.4%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.99579119682312
Clustering and labeling...

 Clustered into 140 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [23][20/200]	Time 1.264 (1.281)	Data 0.001 (0.000)	Loss_ce 1.358 / 2.014	Loss_ce_soft 1.146 / 2.733	Loss_tri_soft 0.195 / 0.210	Loss_lf 4.401 / 2.961	Loss_lf_tri 0.288 / 0.281	Prec 91.25% / 83.44%	
Epoch: [23][40/200]	Time 1.273 (2.353)	Data 0.000 (1.064)	Loss_ce 1.395 / 1.919	Loss_ce_soft 1.259 / 2.632	Loss_tri_soft 0.187 / 0.189	Loss_lf 4.432 / 2.937	Loss_lf_tri 0.292 / 0.266	Prec 90.62% / 86.09%	
Epoch: [23][60/200]	Time 1.385 (1.998)	Data 0.000 (0.709)	Loss_ce 1.360 / 1.891	Loss_ce_soft 1.255 / 2.571	Loss_tri_soft 0.194 / 0.184	Loss_lf 4.450 / 2.874	Loss_lf_tri 0.297 / 0.256	Prec 91.56% / 86.77%	
Epoch: [23][80/200]	Time 1.383 (2.359)	Data 0.000 (1.063)	Loss_ce 1.364 / 1.906	Loss_ce_soft 1.255 / 2.565	Loss_tri_soft 0.198 / 0.180	Loss_lf 4.443 / 2.867	Loss_lf_tri 0.300 / 0.249	Prec 91.48% / 86.41%	
Epoch: [23][100/200]	Time 1.331 (2.152)	Data 0.000 (0.851)	Loss_ce 1.355 / 1.933	Loss_ce_soft 1.260 / 2.597	Loss_tri_soft 0.197 / 0.193	Loss_lf 4.443 / 2.881	Loss_lf_tri 0.299 / 0.258	Prec 91.94% / 86.00%	
Epoch: [23][120/200]	Time 1.322 (2.365)	Data 0.000 (1.065)	Loss_ce 1.345 / 1.928	Loss_ce_soft 1.257 / 2.584	Loss_tri_soft 0.189 / 0.205	Loss_lf 4.439 / 2.893	Loss_lf_tri 0.293 / 0.267	Prec 92.40% / 86.72%	
Epoch: [23][140/200]	Time 1.262 (2.226)	Data 0.000 (0.913)	Loss_ce 1.352 / 1.936	Loss_ce_soft 1.257 / 2.585	Loss_tri_soft 0.193 / 0.202	Loss_lf 4.433 / 2.903	Loss_lf_tri 0.296 / 0.265	Prec 91.92% / 86.12%	
Epoch: [23][160/200]	Time 1.351 (2.641)	Data 0.000 (1.061)	Loss_ce 1.352 / 1.918	Loss_ce_soft 1.268 / 2.582	Loss_tri_soft 0.196 / 0.195	Loss_lf 4.436 / 2.894	Loss_lf_tri 0.302 / 0.258	Prec 91.91% / 86.45%	
Epoch: [23][180/200]	Time 1.233 (2.728)	Data 0.000 (1.178)	Loss_ce 1.346 / 1.907	Loss_ce_soft 1.252 / 2.592	Loss_tri_soft 0.192 / 0.195	Loss_lf 4.433 / 2.897	Loss_lf_tri 0.298 / 0.257	Prec 91.98% / 86.88%	
Epoch: [23][200/200]	Time 1.379 (2.583)	Data 0.001 (1.060)	Loss_ce 1.336 / 1.898	Loss_ce_soft 1.240 / 2.596	Loss_tri_soft 0.193 / 0.193	Loss_lf 4.427 / 2.903	Loss_lf_tri 0.299 / 0.257	Prec 92.19% / 87.03%	
Extract Features: [50/302]	Time 0.169 (1.032)	Data 0.000 (0.857)	
Extract Features: [100/302]	Time 0.170 (0.603)	Data 0.001 (0.429)	
Extract Features: [150/302]	Time 0.167 (0.459)	Data 0.000 (0.286)	
Extract Features: [200/302]	Time 0.186 (0.387)	Data 0.001 (0.215)	
Extract Features: [250/302]	Time 0.167 (0.344)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.179 (0.316)	Data 0.000 (0.143)	
Mean AP: 63.5%

 * Finished phase   2 epoch  23  model no.1 mAP: 63.5%  best: 64.4%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.975797653198242
Clustering and labeling...

 Clustered into 140 classes 

###############################
Lamda for less forget is set to  20.615528128088304
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [24][20/200]	Time 1.231 (1.273)	Data 0.000 (0.000)	Loss_ce 1.372 / 1.847	Loss_ce_soft 1.265 / 2.707	Loss_tri_soft 0.197 / 0.193	Loss_lf 4.365 / 2.984	Loss_lf_tri 0.291 / 0.261	Prec 90.62% / 89.38%	
Epoch: [24][40/200]	Time 1.364 (2.432)	Data 0.001 (1.126)	Loss_ce 1.354 / 1.891	Loss_ce_soft 1.222 / 2.700	Loss_tri_soft 0.213 / 0.179	Loss_lf 4.385 / 3.013	Loss_lf_tri 0.308 / 0.246	Prec 91.56% / 88.75%	
Epoch: [24][60/200]	Time 1.313 (2.071)	Data 0.001 (0.751)	Loss_ce 1.333 / 1.902	Loss_ce_soft 1.224 / 2.666	Loss_tri_soft 0.199 / 0.193	Loss_lf 4.391 / 2.951	Loss_lf_tri 0.298 / 0.260	Prec 92.08% / 88.33%	
Epoch: [24][80/200]	Time 1.339 (2.405)	Data 0.000 (1.087)	Loss_ce 1.329 / 1.919	Loss_ce_soft 1.221 / 2.667	Loss_tri_soft 0.198 / 0.198	Loss_lf 4.394 / 2.972	Loss_lf_tri 0.297 / 0.266	Prec 92.19% / 87.50%	
Epoch: [24][100/200]	Time 1.304 (2.203)	Data 0.000 (0.869)	Loss_ce 1.319 / 1.921	Loss_ce_soft 1.218 / 2.653	Loss_tri_soft 0.188 / 0.202	Loss_lf 4.380 / 2.942	Loss_lf_tri 0.290 / 0.273	Prec 92.75% / 86.81%	
Epoch: [24][120/200]	Time 1.345 (2.406)	Data 0.001 (1.076)	Loss_ce 1.307 / 1.926	Loss_ce_soft 1.188 / 2.651	Loss_tri_soft 0.190 / 0.203	Loss_lf 4.380 / 2.941	Loss_lf_tri 0.289 / 0.270	Prec 93.07% / 86.88%	
Epoch: [24][140/200]	Time 1.379 (2.253)	Data 0.000 (0.922)	Loss_ce 1.297 / 1.920	Loss_ce_soft 1.180 / 2.625	Loss_tri_soft 0.192 / 0.196	Loss_lf 4.388 / 2.909	Loss_lf_tri 0.291 / 0.266	Prec 93.44% / 86.79%	
Epoch: [24][160/200]	Time 1.270 (2.666)	Data 0.000 (1.077)	Loss_ce 1.297 / 1.909	Loss_ce_soft 1.187 / 2.634	Loss_tri_soft 0.187 / 0.191	Loss_lf 4.396 / 2.943	Loss_lf_tri 0.288 / 0.266	Prec 93.36% / 86.95%	
Epoch: [24][180/200]	Time 1.232 (2.748)	Data 0.001 (1.195)	Loss_ce 1.291 / 1.896	Loss_ce_soft 1.182 / 2.632	Loss_tri_soft 0.182 / 0.185	Loss_lf 4.391 / 2.935	Loss_lf_tri 0.287 / 0.261	Prec 93.61% / 87.19%	
Epoch: [24][200/200]	Time 1.310 (2.606)	Data 0.000 (1.076)	Loss_ce 1.290 / 1.891	Loss_ce_soft 1.185 / 2.617	Loss_tri_soft 0.181 / 0.190	Loss_lf 4.396 / 2.932	Loss_lf_tri 0.286 / 0.267	Prec 93.72% / 87.16%	
Extract Features: [50/302]	Time 0.166 (1.039)	Data 0.000 (0.868)	
Extract Features: [100/302]	Time 0.167 (0.604)	Data 0.000 (0.434)	
Extract Features: [150/302]	Time 0.168 (0.460)	Data 0.001 (0.289)	
Extract Features: [200/302]	Time 0.167 (0.388)	Data 0.000 (0.217)	
Extract Features: [250/302]	Time 0.166 (0.344)	Data 0.000 (0.174)	
Extract Features: [300/302]	Time 0.179 (0.315)	Data 0.000 (0.145)	
Mean AP: 64.0%

 * Finished phase   2 epoch  24  model no.1 mAP: 64.0%  best: 64.4%

update proto_dataset
=> Loaded checkpoint 'logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase3_model_best.pth.tar'
mismatch: module.classifier.fc2.weight torch.Size([146, 2048]) torch.Size([140, 2048])
mismatch: module.classifier_max.fc2.weight torch.Size([146, 2048]) torch.Size([140, 2048])
missing keys in state_dict: {'module.classifier_max.fc2.weight', 'module.classifier.fc2.weight'}
Computing original distance...
Computing Jaccard distance...
Time cost: 9.8498375415802
Clustering and labeling...
phase 2 Clustered into 141 example classes 
delete by camera is 38
delete by cluster distance is 5,total num is 103
NMI of phase2 is 0.9598871426184494 
pickle into logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase2_proto.pkl


 phase 3 have 523 old classes
=> Loaded checkpoint 'logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase3_model_best.pth.tar'
phase:3 input id:100,input image:1845
Computing original distance...
Computing Jaccard distance...
Time cost: 11.027459383010864
eps for cluster: 0.062
Clustering and labeling...

 Clustered into 131 classes 

in_features: 2048 out_features1: 425 out_features2: 98
###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [0][20/200]	Time 1.328 (1.267)	Data 0.000 (0.000)	Loss_ce 1.485 / 1.919	Loss_ce_soft 1.515 / 2.943	Loss_tri_soft 0.315 / 0.205	Loss_lf 4.662 / 2.718	Loss_lf_tri 0.291 / 0.201	Prec 87.50% / 87.19%	
Epoch: [0][40/200]	Time 1.255 (2.328)	Data 0.001 (1.058)	Loss_ce 1.467 / 1.916	Loss_ce_soft 1.473 / 3.008	Loss_tri_soft 0.283 / 0.202	Loss_lf 4.615 / 2.714	Loss_lf_tri 0.267 / 0.197	Prec 87.97% / 87.50%	
Epoch: [0][60/200]	Time 1.317 (1.985)	Data 0.000 (0.706)	Loss_ce 1.451 / 1.949	Loss_ce_soft 1.474 / 2.996	Loss_tri_soft 0.270 / 0.213	Loss_lf 4.630 / 2.738	Loss_lf_tri 0.261 / 0.200	Prec 88.33% / 86.67%	
Epoch: [0][80/200]	Time 1.274 (2.343)	Data 0.000 (1.046)	Loss_ce 1.438 / 1.967	Loss_ce_soft 1.444 / 3.025	Loss_tri_soft 0.258 / 0.216	Loss_lf 4.620 / 2.768	Loss_lf_tri 0.252 / 0.202	Prec 88.67% / 86.41%	
Epoch: [0][100/200]	Time 1.323 (2.542)	Data 0.000 (1.244)	Loss_ce 1.437 / 1.983	Loss_ce_soft 1.445 / 3.048	Loss_tri_soft 0.246 / 0.219	Loss_lf 4.626 / 2.790	Loss_lf_tri 0.246 / 0.205	Prec 89.25% / 85.81%	
Epoch: [0][120/200]	Time 1.313 (2.333)	Data 0.000 (1.037)	Loss_ce 1.430 / 2.021	Loss_ce_soft 1.441 / 3.084	Loss_tri_soft 0.246 / 0.231	Loss_lf 4.611 / 2.803	Loss_lf_tri 0.246 / 0.216	Prec 89.58% / 85.00%	
Epoch: [0][140/200]	Time 1.238 (2.480)	Data 0.000 (1.184)	Loss_ce 1.434 / 2.022	Loss_ce_soft 1.464 / 3.073	Loss_tri_soft 0.248 / 0.220	Loss_lf 4.624 / 2.803	Loss_lf_tri 0.248 / 0.206	Prec 89.64% / 85.27%	
Epoch: [0][160/200]	Time 1.221 (2.328)	Data 0.001 (1.036)	Loss_ce 1.439 / 2.025	Loss_ce_soft 1.486 / 3.087	Loss_tri_soft 0.248 / 0.218	Loss_lf 4.632 / 2.804	Loss_lf_tri 0.250 / 0.205	Prec 88.98% / 85.23%	
Epoch: [0][180/200]	Time 1.230 (2.682)	Data 0.000 (1.156)	Loss_ce 1.433 / 2.018	Loss_ce_soft 1.480 / 3.090	Loss_tri_soft 0.252 / 0.225	Loss_lf 4.632 / 2.832	Loss_lf_tri 0.253 / 0.211	Prec 89.13% / 85.59%	
Epoch: [0][200/200]	Time 1.237 (2.753)	Data 0.000 (1.253)	Loss_ce 1.423 / 2.025	Loss_ce_soft 1.460 / 3.105	Loss_tri_soft 0.247 / 0.225	Loss_lf 4.630 / 2.854	Loss_lf_tri 0.249 / 0.212	Prec 89.50% / 85.50%	
Extract Features: [50/302]	Time 0.188 (1.026)	Data 0.001 (0.853)	
Extract Features: [100/302]	Time 0.178 (0.598)	Data 0.000 (0.427)	
Extract Features: [150/302]	Time 0.166 (0.456)	Data 0.000 (0.284)	
Extract Features: [200/302]	Time 0.167 (0.384)	Data 0.001 (0.213)	
Extract Features: [250/302]	Time 0.167 (0.342)	Data 0.000 (0.171)	
Extract Features: [300/302]	Time 0.163 (0.313)	Data 0.000 (0.142)	
Mean AP: 63.0%

 * Finished phase   3 epoch   0  model no.1 mAP: 63.0%  best: 63.0% *

Computing original distance...
Computing Jaccard distance...
Time cost: 11.023460626602173
Clustering and labeling...

 Clustered into 122 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [1][20/200]	Time 1.338 (1.355)	Data 0.000 (0.000)	Loss_ce 1.424 / 2.000	Loss_ce_soft 1.426 / 3.130	Loss_tri_soft 0.272 / 0.217	Loss_lf 4.807 / 2.797	Loss_lf_tri 0.265 / 0.210	Prec 87.50% / 87.19%	
Epoch: [1][40/200]	Time 1.304 (2.495)	Data 0.000 (1.107)	Loss_ce 1.427 / 2.057	Loss_ce_soft 1.372 / 3.139	Loss_tri_soft 0.246 / 0.220	Loss_lf 4.744 / 2.799	Loss_lf_tri 0.252 / 0.216	Prec 88.28% / 85.00%	
Epoch: [1][60/200]	Time 1.352 (2.114)	Data 0.000 (0.738)	Loss_ce 1.400 / 2.036	Loss_ce_soft 1.325 / 3.044	Loss_tri_soft 0.248 / 0.214	Loss_lf 4.723 / 2.827	Loss_lf_tri 0.253 / 0.208	Prec 88.65% / 85.10%	
Epoch: [1][80/200]	Time 1.315 (2.477)	Data 0.000 (1.110)	Loss_ce 1.389 / 2.055	Loss_ce_soft 1.316 / 3.048	Loss_tri_soft 0.237 / 0.232	Loss_lf 4.708 / 2.838	Loss_lf_tri 0.244 / 0.222	Prec 89.14% / 84.84%	
Epoch: [1][100/200]	Time 1.329 (2.695)	Data 0.000 (1.334)	Loss_ce 1.380 / 2.069	Loss_ce_soft 1.289 / 3.046	Loss_tri_soft 0.230 / 0.232	Loss_lf 4.705 / 2.840	Loss_lf_tri 0.239 / 0.222	Prec 89.50% / 84.25%	
Epoch: [1][120/200]	Time 1.314 (2.470)	Data 0.000 (1.111)	Loss_ce 1.378 / 2.074	Loss_ce_soft 1.294 / 3.049	Loss_tri_soft 0.234 / 0.233	Loss_lf 4.701 / 2.880	Loss_lf_tri 0.244 / 0.223	Prec 89.74% / 84.01%	
Epoch: [1][140/200]	Time 1.354 (2.633)	Data 0.000 (1.276)	Loss_ce 1.365 / 2.069	Loss_ce_soft 1.276 / 3.041	Loss_tri_soft 0.231 / 0.227	Loss_lf 4.701 / 2.880	Loss_lf_tri 0.241 / 0.218	Prec 90.27% / 84.33%	
Epoch: [1][160/200]	Time 1.375 (2.754)	Data 0.000 (1.400)	Loss_ce 1.373 / 2.084	Loss_ce_soft 1.292 / 3.054	Loss_tri_soft 0.234 / 0.220	Loss_lf 4.699 / 2.891	Loss_lf_tri 0.245 / 0.214	Prec 89.88% / 84.26%	
Epoch: [1][180/200]	Time 1.351 (2.848)	Data 0.000 (1.244)	Loss_ce 1.371 / 2.067	Loss_ce_soft 1.286 / 3.048	Loss_tri_soft 0.230 / 0.213	Loss_lf 4.697 / 2.888	Loss_lf_tri 0.244 / 0.209	Prec 89.79% / 84.51%	
Epoch: [1][200/200]	Time 1.356 (2.909)	Data 0.000 (1.333)	Loss_ce 1.361 / 2.064	Loss_ce_soft 1.273 / 3.056	Loss_tri_soft 0.227 / 0.212	Loss_lf 4.698 / 2.907	Loss_lf_tri 0.242 / 0.208	Prec 90.06% / 84.56%	
Extract Features: [50/302]	Time 0.184 (1.053)	Data 0.000 (0.882)	
Extract Features: [100/302]	Time 0.166 (0.612)	Data 0.001 (0.441)	
Extract Features: [150/302]	Time 0.187 (0.465)	Data 0.000 (0.294)	
Extract Features: [200/302]	Time 0.173 (0.392)	Data 0.000 (0.221)	
Extract Features: [250/302]	Time 0.165 (0.347)	Data 0.000 (0.177)	
Extract Features: [300/302]	Time 0.168 (0.323)	Data 0.000 (0.147)	
Mean AP: 63.6%

 * Finished phase   3 epoch   1  model no.1 mAP: 63.6%  best: 63.6% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.950485944747925
Clustering and labeling...

 Clustered into 132 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [2][20/200]	Time 1.357 (1.335)	Data 0.000 (0.000)	Loss_ce 1.414 / 2.088	Loss_ce_soft 1.250 / 3.193	Loss_tri_soft 0.136 / 0.195	Loss_lf 4.666 / 2.861	Loss_lf_tri 0.171 / 0.216	Prec 90.31% / 83.75%	
Epoch: [2][40/200]	Time 1.308 (2.405)	Data 0.001 (1.075)	Loss_ce 1.419 / 2.105	Loss_ce_soft 1.280 / 3.160	Loss_tri_soft 0.162 / 0.235	Loss_lf 4.662 / 2.922	Loss_lf_tri 0.194 / 0.240	Prec 89.38% / 83.12%	
Epoch: [2][60/200]	Time 1.386 (2.050)	Data 0.001 (0.717)	Loss_ce 1.393 / 2.095	Loss_ce_soft 1.268 / 3.104	Loss_tri_soft 0.177 / 0.223	Loss_lf 4.683 / 2.909	Loss_lf_tri 0.211 / 0.228	Prec 90.10% / 82.19%	
Epoch: [2][80/200]	Time 1.297 (2.406)	Data 0.000 (1.076)	Loss_ce 1.394 / 2.095	Loss_ce_soft 1.273 / 3.057	Loss_tri_soft 0.183 / 0.211	Loss_lf 4.671 / 2.932	Loss_lf_tri 0.215 / 0.217	Prec 90.23% / 82.42%	
Epoch: [2][100/200]	Time 45.705 (2.639)	Data 44.418 (1.305)	Loss_ce 1.395 / 2.095	Loss_ce_soft 1.284 / 3.055	Loss_tri_soft 0.184 / 0.204	Loss_lf 4.661 / 2.978	Loss_lf_tri 0.215 / 0.212	Prec 90.19% / 82.44%	
Epoch: [2][120/200]	Time 1.346 (2.421)	Data 0.000 (1.087)	Loss_ce 1.399 / 2.109	Loss_ce_soft 1.290 / 3.079	Loss_tri_soft 0.191 / 0.207	Loss_lf 4.661 / 2.961	Loss_lf_tri 0.221 / 0.214	Prec 89.95% / 83.07%	
Epoch: [2][140/200]	Time 1.341 (2.591)	Data 0.000 (1.256)	Loss_ce 1.403 / 2.109	Loss_ce_soft 1.303 / 3.068	Loss_tri_soft 0.193 / 0.203	Loss_lf 4.661 / 2.931	Loss_lf_tri 0.225 / 0.210	Prec 90.00% / 83.48%	
Epoch: [2][160/200]	Time 1.369 (2.434)	Data 0.000 (1.099)	Loss_ce 1.397 / 2.118	Loss_ce_soft 1.294 / 3.082	Loss_tri_soft 0.189 / 0.201	Loss_lf 4.662 / 2.960	Loss_lf_tri 0.220 / 0.208	Prec 90.16% / 83.83%	
Epoch: [2][180/200]	Time 1.387 (2.787)	Data 0.000 (1.215)	Loss_ce 1.393 / 2.089	Loss_ce_soft 1.290 / 3.060	Loss_tri_soft 0.193 / 0.204	Loss_lf 4.662 / 2.946	Loss_lf_tri 0.225 / 0.209	Prec 90.17% / 84.38%	
Epoch: [2][200/200]	Time 1.325 (2.857)	Data 0.001 (1.308)	Loss_ce 1.390 / 2.070	Loss_ce_soft 1.290 / 3.042	Loss_tri_soft 0.197 / 0.206	Loss_lf 4.664 / 2.941	Loss_lf_tri 0.228 / 0.210	Prec 90.22% / 84.88%	
Extract Features: [50/302]	Time 0.164 (1.069)	Data 0.000 (0.891)	
Extract Features: [100/302]	Time 0.165 (0.635)	Data 0.000 (0.446)	
Extract Features: [150/302]	Time 0.166 (0.480)	Data 0.000 (0.297)	
Extract Features: [200/302]	Time 0.166 (0.402)	Data 0.001 (0.223)	
Extract Features: [250/302]	Time 0.165 (0.355)	Data 0.000 (0.178)	
Extract Features: [300/302]	Time 0.158 (0.324)	Data 0.000 (0.149)	
Mean AP: 64.3%

 * Finished phase   3 epoch   2  model no.1 mAP: 64.3%  best: 64.3% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.78153944015503
Clustering and labeling...

 Clustered into 130 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [3][20/200]	Time 1.282 (1.286)	Data 0.000 (0.000)	Loss_ce 1.410 / 2.003	Loss_ce_soft 1.317 / 2.953	Loss_tri_soft 0.206 / 0.248	Loss_lf 4.710 / 2.951	Loss_lf_tri 0.243 / 0.246	Prec 89.38% / 85.62%	
Epoch: [3][40/200]	Time 1.346 (2.419)	Data 0.000 (1.111)	Loss_ce 1.394 / 2.045	Loss_ce_soft 1.305 / 2.989	Loss_tri_soft 0.213 / 0.206	Loss_lf 4.708 / 3.021	Loss_lf_tri 0.240 / 0.220	Prec 89.84% / 83.75%	
Epoch: [3][60/200]	Time 1.345 (2.059)	Data 0.000 (0.740)	Loss_ce 1.431 / 2.063	Loss_ce_soft 1.325 / 2.998	Loss_tri_soft 0.202 / 0.209	Loss_lf 4.685 / 2.972	Loss_lf_tri 0.230 / 0.221	Prec 88.12% / 82.50%	
Epoch: [3][80/200]	Time 1.357 (2.434)	Data 0.001 (1.105)	Loss_ce 1.421 / 2.049	Loss_ce_soft 1.310 / 2.994	Loss_tri_soft 0.193 / 0.201	Loss_lf 4.675 / 2.977	Loss_lf_tri 0.228 / 0.218	Prec 88.59% / 83.91%	
Epoch: [3][100/200]	Time 1.369 (2.664)	Data 0.000 (1.327)	Loss_ce 1.409 / 2.079	Loss_ce_soft 1.320 / 3.042	Loss_tri_soft 0.199 / 0.207	Loss_lf 4.696 / 2.984	Loss_lf_tri 0.231 / 0.222	Prec 89.31% / 83.75%	
Epoch: [3][120/200]	Time 1.292 (2.443)	Data 0.000 (1.105)	Loss_ce 1.401 / 2.091	Loss_ce_soft 1.304 / 3.034	Loss_tri_soft 0.202 / 0.198	Loss_lf 4.688 / 2.973	Loss_lf_tri 0.237 / 0.213	Prec 89.17% / 83.49%	
Epoch: [3][140/200]	Time 1.290 (2.599)	Data 0.001 (1.262)	Loss_ce 1.388 / 2.106	Loss_ce_soft 1.295 / 3.055	Loss_tri_soft 0.195 / 0.209	Loss_lf 4.683 / 2.947	Loss_lf_tri 0.231 / 0.221	Prec 89.55% / 83.44%	
Epoch: [3][160/200]	Time 1.301 (2.441)	Data 0.000 (1.104)	Loss_ce 1.395 / 2.109	Loss_ce_soft 1.294 / 3.043	Loss_tri_soft 0.195 / 0.208	Loss_lf 4.681 / 2.921	Loss_lf_tri 0.232 / 0.218	Prec 89.34% / 83.63%	
Epoch: [3][180/200]	Time 1.314 (2.795)	Data 0.000 (1.219)	Loss_ce 1.393 / 2.088	Loss_ce_soft 1.295 / 3.033	Loss_tri_soft 0.197 / 0.206	Loss_lf 4.683 / 2.954	Loss_lf_tri 0.234 / 0.217	Prec 89.58% / 84.41%	
Epoch: [3][200/200]	Time 1.282 (2.872)	Data 0.000 (1.313)	Loss_ce 1.381 / 2.073	Loss_ce_soft 1.284 / 3.020	Loss_tri_soft 0.197 / 0.202	Loss_lf 4.682 / 2.932	Loss_lf_tri 0.233 / 0.215	Prec 90.03% / 84.72%	
Extract Features: [50/302]	Time 0.168 (1.023)	Data 0.000 (0.852)	
Extract Features: [100/302]	Time 0.182 (0.596)	Data 0.000 (0.426)	
Extract Features: [150/302]	Time 0.171 (0.453)	Data 0.001 (0.284)	
Extract Features: [200/302]	Time 0.179 (0.383)	Data 0.000 (0.213)	
Extract Features: [250/302]	Time 0.184 (0.343)	Data 0.000 (0.171)	
Extract Features: [300/302]	Time 0.165 (0.317)	Data 0.000 (0.142)	
Mean AP: 63.9%

 * Finished phase   3 epoch   3  model no.1 mAP: 63.9%  best: 64.3%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.818527460098267
Clustering and labeling...

 Clustered into 125 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [4][20/200]	Time 1.344 (1.324)	Data 0.000 (0.000)	Loss_ce 1.441 / 2.040	Loss_ce_soft 1.274 / 2.978	Loss_tri_soft 0.182 / 0.190	Loss_lf 4.665 / 2.943	Loss_lf_tri 0.219 / 0.208	Prec 89.69% / 84.69%	
Epoch: [4][40/200]	Time 1.352 (2.408)	Data 0.000 (1.077)	Loss_ce 1.431 / 2.039	Loss_ce_soft 1.281 / 2.998	Loss_tri_soft 0.217 / 0.192	Loss_lf 4.663 / 2.938	Loss_lf_tri 0.246 / 0.202	Prec 90.31% / 84.38%	
Epoch: [4][60/200]	Time 1.340 (2.053)	Data 0.000 (0.718)	Loss_ce 1.406 / 2.088	Loss_ce_soft 1.271 / 3.026	Loss_tri_soft 0.211 / 0.204	Loss_lf 4.646 / 2.979	Loss_lf_tri 0.242 / 0.215	Prec 90.94% / 83.12%	
Epoch: [4][80/200]	Time 1.344 (2.405)	Data 0.000 (1.074)	Loss_ce 1.385 / 2.099	Loss_ce_soft 1.256 / 3.000	Loss_tri_soft 0.201 / 0.189	Loss_lf 4.652 / 2.910	Loss_lf_tri 0.232 / 0.202	Prec 91.41% / 82.97%	
Epoch: [4][100/200]	Time 1.323 (2.618)	Data 0.000 (1.289)	Loss_ce 1.389 / 2.118	Loss_ce_soft 1.261 / 3.015	Loss_tri_soft 0.197 / 0.195	Loss_lf 4.650 / 2.882	Loss_lf_tri 0.231 / 0.204	Prec 90.94% / 83.31%	
Epoch: [4][120/200]	Time 1.330 (2.398)	Data 0.000 (1.075)	Loss_ce 1.387 / 2.119	Loss_ce_soft 1.267 / 3.026	Loss_tri_soft 0.209 / 0.198	Loss_lf 4.666 / 2.896	Loss_lf_tri 0.241 / 0.206	Prec 90.73% / 83.91%	
Epoch: [4][140/200]	Time 1.313 (2.565)	Data 0.000 (1.239)	Loss_ce 1.370 / 2.133	Loss_ce_soft 1.243 / 3.049	Loss_tri_soft 0.196 / 0.202	Loss_lf 4.664 / 2.910	Loss_lf_tri 0.229 / 0.212	Prec 91.03% / 83.53%	
Epoch: [4][160/200]	Time 1.325 (2.695)	Data 0.000 (1.354)	Loss_ce 1.375 / 2.150	Loss_ce_soft 1.267 / 3.060	Loss_tri_soft 0.200 / 0.210	Loss_lf 4.662 / 2.939	Loss_lf_tri 0.234 / 0.220	Prec 90.59% / 83.24%	
Epoch: [4][180/200]	Time 1.428 (2.772)	Data 0.001 (1.203)	Loss_ce 1.368 / 2.135	Loss_ce_soft 1.256 / 3.055	Loss_tri_soft 0.199 / 0.209	Loss_lf 4.655 / 2.942	Loss_lf_tri 0.233 / 0.221	Prec 90.66% / 83.40%	
Epoch: [4][200/200]	Time 1.258 (2.837)	Data 0.000 (1.297)	Loss_ce 1.362 / 2.120	Loss_ce_soft 1.260 / 3.048	Loss_tri_soft 0.199 / 0.201	Loss_lf 4.657 / 2.929	Loss_lf_tri 0.235 / 0.212	Prec 90.94% / 83.94%	
Extract Features: [50/302]	Time 0.167 (1.021)	Data 0.001 (0.850)	
Extract Features: [100/302]	Time 0.167 (0.595)	Data 0.000 (0.425)	
Extract Features: [150/302]	Time 0.187 (0.452)	Data 0.001 (0.283)	
Extract Features: [200/302]	Time 0.183 (0.381)	Data 0.001 (0.213)	
Extract Features: [250/302]	Time 0.165 (0.339)	Data 0.000 (0.170)	
Extract Features: [300/302]	Time 0.182 (0.310)	Data 0.000 (0.142)	
Mean AP: 64.8%

 * Finished phase   3 epoch   4  model no.1 mAP: 64.8%  best: 64.8% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.763544797897339
Clustering and labeling...

 Clustered into 132 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [5][20/200]	Time 1.338 (1.335)	Data 0.000 (0.000)	Loss_ce 1.416 / 2.015	Loss_ce_soft 1.321 / 3.022	Loss_tri_soft 0.218 / 0.180	Loss_lf 4.689 / 2.679	Loss_lf_tri 0.250 / 0.194	Prec 89.06% / 85.31%	
Epoch: [5][40/200]	Time 1.359 (2.437)	Data 0.001 (1.101)	Loss_ce 1.490 / 2.040	Loss_ce_soft 1.459 / 3.009	Loss_tri_soft 0.232 / 0.181	Loss_lf 4.734 / 2.797	Loss_lf_tri 0.263 / 0.198	Prec 88.12% / 84.22%	
Epoch: [5][60/200]	Time 1.380 (2.070)	Data 0.000 (0.734)	Loss_ce 1.494 / 2.039	Loss_ce_soft 1.457 / 2.979	Loss_tri_soft 0.238 / 0.169	Loss_lf 4.720 / 2.803	Loss_lf_tri 0.271 / 0.188	Prec 87.60% / 85.21%	
Epoch: [5][80/200]	Time 1.379 (2.438)	Data 0.001 (1.103)	Loss_ce 1.481 / 2.067	Loss_ce_soft 1.431 / 3.017	Loss_tri_soft 0.215 / 0.163	Loss_lf 4.721 / 2.871	Loss_lf_tri 0.254 / 0.189	Prec 88.12% / 84.69%	
Epoch: [5][100/200]	Time 46.522 (2.668)	Data 45.118 (1.333)	Loss_ce 1.467 / 2.071	Loss_ce_soft 1.420 / 3.029	Loss_tri_soft 0.208 / 0.163	Loss_lf 4.715 / 2.895	Loss_lf_tri 0.247 / 0.189	Prec 88.56% / 84.94%	
Epoch: [5][120/200]	Time 1.327 (2.462)	Data 0.000 (1.111)	Loss_ce 1.446 / 2.067	Loss_ce_soft 1.386 / 3.010	Loss_tri_soft 0.200 / 0.162	Loss_lf 4.715 / 2.863	Loss_lf_tri 0.241 / 0.189	Prec 88.80% / 85.05%	
Epoch: [5][140/200]	Time 1.383 (2.610)	Data 0.000 (1.262)	Loss_ce 1.437 / 2.066	Loss_ce_soft 1.386 / 3.007	Loss_tri_soft 0.196 / 0.161	Loss_lf 4.717 / 2.905	Loss_lf_tri 0.240 / 0.188	Prec 88.84% / 85.13%	
Epoch: [5][160/200]	Time 1.313 (2.450)	Data 0.000 (1.104)	Loss_ce 1.427 / 2.080	Loss_ce_soft 1.372 / 3.018	Loss_tri_soft 0.198 / 0.162	Loss_lf 4.717 / 2.919	Loss_lf_tri 0.242 / 0.189	Prec 89.06% / 84.69%	
Epoch: [5][180/200]	Time 1.322 (2.803)	Data 0.001 (1.221)	Loss_ce 1.419 / 2.072	Loss_ce_soft 1.366 / 3.025	Loss_tri_soft 0.194 / 0.158	Loss_lf 4.718 / 2.902	Loss_lf_tri 0.240 / 0.185	Prec 89.20% / 85.00%	
Epoch: [5][200/200]	Time 1.352 (2.881)	Data 0.001 (1.322)	Loss_ce 1.414 / 2.057	Loss_ce_soft 1.357 / 3.005	Loss_tri_soft 0.194 / 0.160	Loss_lf 4.714 / 2.889	Loss_lf_tri 0.239 / 0.186	Prec 89.25% / 85.38%	
Extract Features: [50/302]	Time 0.170 (1.059)	Data 0.001 (0.886)	
Extract Features: [100/302]	Time 0.168 (0.612)	Data 0.000 (0.443)	
Extract Features: [150/302]	Time 0.166 (0.464)	Data 0.000 (0.295)	
Extract Features: [200/302]	Time 0.166 (0.390)	Data 0.000 (0.222)	
Extract Features: [250/302]	Time 0.164 (0.346)	Data 0.000 (0.177)	
Extract Features: [300/302]	Time 0.158 (0.315)	Data 0.000 (0.148)	
Mean AP: 64.6%

 * Finished phase   3 epoch   5  model no.1 mAP: 64.6%  best: 64.8%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.768543481826782
Clustering and labeling...

 Clustered into 137 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [6][20/200]	Time 1.297 (1.324)	Data 0.000 (0.000)	Loss_ce 1.375 / 2.006	Loss_ce_soft 1.299 / 2.938	Loss_tri_soft 0.193 / 0.164	Loss_lf 4.645 / 2.801	Loss_lf_tri 0.239 / 0.191	Prec 90.00% / 87.81%	
Epoch: [6][40/200]	Time 1.328 (2.431)	Data 0.001 (1.099)	Loss_ce 1.430 / 2.110	Loss_ce_soft 1.394 / 3.052	Loss_tri_soft 0.205 / 0.173	Loss_lf 4.689 / 2.972	Loss_lf_tri 0.243 / 0.195	Prec 88.75% / 85.00%	
Epoch: [6][60/200]	Time 1.333 (2.069)	Data 0.001 (0.733)	Loss_ce 1.423 / 2.107	Loss_ce_soft 1.392 / 3.018	Loss_tri_soft 0.219 / 0.177	Loss_lf 4.693 / 2.973	Loss_lf_tri 0.260 / 0.198	Prec 89.06% / 84.17%	
Epoch: [6][80/200]	Time 1.332 (2.442)	Data 0.000 (1.104)	Loss_ce 1.413 / 2.114	Loss_ce_soft 1.376 / 3.020	Loss_tri_soft 0.205 / 0.170	Loss_lf 4.701 / 2.989	Loss_lf_tri 0.248 / 0.198	Prec 89.84% / 83.91%	
Epoch: [6][100/200]	Time 1.350 (2.222)	Data 0.000 (0.883)	Loss_ce 1.400 / 2.108	Loss_ce_soft 1.356 / 3.033	Loss_tri_soft 0.204 / 0.164	Loss_lf 4.703 / 2.982	Loss_lf_tri 0.245 / 0.191	Prec 90.25% / 84.44%	
Epoch: [6][120/200]	Time 1.347 (2.462)	Data 0.001 (1.110)	Loss_ce 1.420 / 2.129	Loss_ce_soft 1.384 / 3.044	Loss_tri_soft 0.213 / 0.178	Loss_lf 4.702 / 2.982	Loss_lf_tri 0.254 / 0.199	Prec 89.64% / 83.96%	
Epoch: [6][140/200]	Time 1.323 (2.627)	Data 0.000 (1.276)	Loss_ce 1.407 / 2.135	Loss_ce_soft 1.370 / 3.044	Loss_tri_soft 0.205 / 0.181	Loss_lf 4.700 / 2.952	Loss_lf_tri 0.248 / 0.202	Prec 89.78% / 83.88%	
Epoch: [6][160/200]	Time 1.385 (2.466)	Data 0.001 (1.117)	Loss_ce 1.397 / 2.137	Loss_ce_soft 1.363 / 3.035	Loss_tri_soft 0.200 / 0.182	Loss_lf 4.692 / 2.973	Loss_lf_tri 0.246 / 0.204	Prec 90.00% / 83.79%	
Epoch: [6][180/200]	Time 1.305 (2.826)	Data 0.000 (1.241)	Loss_ce 1.387 / 2.131	Loss_ce_soft 1.355 / 3.031	Loss_tri_soft 0.200 / 0.191	Loss_lf 4.699 / 2.984	Loss_lf_tri 0.244 / 0.212	Prec 90.21% / 83.65%	
Epoch: [6][200/200]	Time 1.309 (2.679)	Data 0.000 (1.117)	Loss_ce 1.381 / 2.106	Loss_ce_soft 1.355 / 3.006	Loss_tri_soft 0.204 / 0.192	Loss_lf 4.693 / 2.968	Loss_lf_tri 0.250 / 0.214	Prec 90.19% / 84.25%	
Extract Features: [50/302]	Time 0.166 (1.060)	Data 0.000 (0.890)	
Extract Features: [100/302]	Time 0.167 (0.615)	Data 0.000 (0.445)	
Extract Features: [150/302]	Time 0.165 (0.466)	Data 0.000 (0.297)	
Extract Features: [200/302]	Time 0.166 (0.392)	Data 0.000 (0.223)	
Extract Features: [250/302]	Time 0.167 (0.347)	Data 0.001 (0.178)	
Extract Features: [300/302]	Time 0.157 (0.317)	Data 0.000 (0.149)	
Mean AP: 65.1%

 * Finished phase   3 epoch   6  model no.1 mAP: 65.1%  best: 65.1% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.83252239227295
Clustering and labeling...

 Clustered into 135 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [7][20/200]	Time 1.227 (1.273)	Data 0.001 (0.000)	Loss_ce 1.454 / 2.112	Loss_ce_soft 1.355 / 3.046	Loss_tri_soft 0.196 / 0.171	Loss_lf 4.643 / 3.268	Loss_lf_tri 0.265 / 0.199	Prec 88.44% / 81.25%	
Epoch: [7][40/200]	Time 1.290 (2.316)	Data 0.000 (1.042)	Loss_ce 1.388 / 2.130	Loss_ce_soft 1.324 / 3.086	Loss_tri_soft 0.166 / 0.203	Loss_lf 4.672 / 3.174	Loss_lf_tri 0.238 / 0.220	Prec 90.94% / 81.56%	
Epoch: [7][60/200]	Time 1.292 (1.980)	Data 0.000 (0.695)	Loss_ce 1.371 / 2.136	Loss_ce_soft 1.275 / 3.098	Loss_tri_soft 0.175 / 0.229	Loss_lf 4.656 / 3.108	Loss_lf_tri 0.237 / 0.241	Prec 92.08% / 82.71%	
Epoch: [7][80/200]	Time 1.237 (2.336)	Data 0.000 (1.051)	Loss_ce 1.391 / 2.114	Loss_ce_soft 1.284 / 3.054	Loss_tri_soft 0.168 / 0.209	Loss_lf 4.653 / 3.007	Loss_lf_tri 0.224 / 0.225	Prec 91.56% / 83.36%	
Epoch: [7][100/200]	Time 42.776 (2.537)	Data 41.518 (1.256)	Loss_ce 1.387 / 2.120	Loss_ce_soft 1.283 / 3.049	Loss_tri_soft 0.166 / 0.203	Loss_lf 4.651 / 2.969	Loss_lf_tri 0.226 / 0.222	Prec 91.38% / 82.94%	
Epoch: [7][120/200]	Time 1.339 (2.339)	Data 0.000 (1.047)	Loss_ce 1.378 / 2.107	Loss_ce_soft 1.287 / 3.016	Loss_tri_soft 0.165 / 0.190	Loss_lf 4.670 / 2.935	Loss_lf_tri 0.228 / 0.211	Prec 91.41% / 83.33%	
Epoch: [7][140/200]	Time 1.280 (2.492)	Data 0.000 (1.199)	Loss_ce 1.388 / 2.131	Loss_ce_soft 1.302 / 3.031	Loss_tri_soft 0.169 / 0.182	Loss_lf 4.665 / 2.950	Loss_lf_tri 0.232 / 0.206	Prec 91.21% / 82.72%	
Epoch: [7][160/200]	Time 1.352 (2.342)	Data 0.000 (1.049)	Loss_ce 1.377 / 2.135	Loss_ce_soft 1.295 / 3.025	Loss_tri_soft 0.164 / 0.172	Loss_lf 4.668 / 2.956	Loss_lf_tri 0.226 / 0.198	Prec 91.37% / 82.70%	
Epoch: [7][180/200]	Time 1.307 (2.696)	Data 0.001 (1.168)	Loss_ce 1.368 / 2.114	Loss_ce_soft 1.281 / 3.011	Loss_tri_soft 0.163 / 0.172	Loss_lf 4.670 / 2.950	Loss_lf_tri 0.226 / 0.199	Prec 91.53% / 83.12%	
Epoch: [7][200/200]	Time 1.301 (2.766)	Data 0.000 (1.260)	Loss_ce 1.377 / 2.102	Loss_ce_soft 1.296 / 3.003	Loss_tri_soft 0.168 / 0.174	Loss_lf 4.672 / 2.938	Loss_lf_tri 0.230 / 0.202	Prec 91.34% / 83.41%	
Extract Features: [50/302]	Time 0.169 (1.034)	Data 0.001 (0.859)	
Extract Features: [100/302]	Time 0.188 (0.602)	Data 0.001 (0.429)	
Extract Features: [150/302]	Time 0.175 (0.459)	Data 0.000 (0.286)	
Extract Features: [200/302]	Time 0.171 (0.387)	Data 0.000 (0.215)	
Extract Features: [250/302]	Time 0.165 (0.344)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.161 (0.314)	Data 0.000 (0.143)	
Mean AP: 64.7%

 * Finished phase   3 epoch   7  model no.1 mAP: 64.7%  best: 65.1%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.675572633743286
Clustering and labeling...

 Clustered into 138 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [8][20/200]	Time 1.232 (1.261)	Data 0.000 (0.000)	Loss_ce 1.398 / 2.129	Loss_ce_soft 1.232 / 3.044	Loss_tri_soft 0.130 / 0.158	Loss_lf 4.653 / 3.129	Loss_lf_tri 0.191 / 0.202	Prec 91.56% / 80.31%	
Epoch: [8][40/200]	Time 1.325 (2.349)	Data 0.000 (1.079)	Loss_ce 1.435 / 2.041	Loss_ce_soft 1.378 / 2.906	Loss_tri_soft 0.175 / 0.169	Loss_lf 4.707 / 2.995	Loss_lf_tri 0.237 / 0.203	Prec 90.47% / 84.38%	
Epoch: [8][60/200]	Time 1.352 (2.011)	Data 0.000 (0.719)	Loss_ce 1.420 / 2.055	Loss_ce_soft 1.370 / 2.936	Loss_tri_soft 0.186 / 0.166	Loss_lf 4.738 / 2.977	Loss_lf_tri 0.240 / 0.192	Prec 90.73% / 84.48%	
Epoch: [8][80/200]	Time 1.349 (2.379)	Data 0.000 (1.071)	Loss_ce 1.397 / 2.079	Loss_ce_soft 1.348 / 2.957	Loss_tri_soft 0.193 / 0.170	Loss_lf 4.741 / 2.981	Loss_lf_tri 0.241 / 0.194	Prec 91.17% / 83.83%	
Epoch: [8][100/200]	Time 1.290 (2.182)	Data 0.000 (0.857)	Loss_ce 1.391 / 2.082	Loss_ce_soft 1.350 / 2.960	Loss_tri_soft 0.194 / 0.177	Loss_lf 4.741 / 2.977	Loss_lf_tri 0.247 / 0.201	Prec 91.00% / 83.56%	
Epoch: [8][120/200]	Time 1.299 (2.377)	Data 0.000 (1.058)	Loss_ce 1.385 / 2.086	Loss_ce_soft 1.350 / 2.955	Loss_tri_soft 0.189 / 0.177	Loss_lf 4.737 / 2.933	Loss_lf_tri 0.245 / 0.200	Prec 91.15% / 83.54%	
Epoch: [8][140/200]	Time 1.263 (2.519)	Data 0.000 (1.208)	Loss_ce 1.387 / 2.097	Loss_ce_soft 1.349 / 2.970	Loss_tri_soft 0.179 / 0.177	Loss_lf 4.736 / 2.934	Loss_lf_tri 0.234 / 0.201	Prec 90.58% / 83.04%	
Epoch: [8][160/200]	Time 1.346 (2.362)	Data 0.000 (1.057)	Loss_ce 1.386 / 2.098	Loss_ce_soft 1.346 / 2.955	Loss_tri_soft 0.176 / 0.173	Loss_lf 4.724 / 2.929	Loss_lf_tri 0.231 / 0.198	Prec 90.74% / 83.20%	
Epoch: [8][180/200]	Time 1.314 (2.711)	Data 0.000 (1.175)	Loss_ce 1.382 / 2.087	Loss_ce_soft 1.339 / 2.956	Loss_tri_soft 0.176 / 0.175	Loss_lf 4.727 / 2.929	Loss_lf_tri 0.228 / 0.201	Prec 90.90% / 83.78%	
Epoch: [8][200/200]	Time 1.320 (2.573)	Data 0.001 (1.057)	Loss_ce 1.382 / 2.071	Loss_ce_soft 1.335 / 2.953	Loss_tri_soft 0.179 / 0.172	Loss_lf 4.725 / 2.920	Loss_lf_tri 0.231 / 0.201	Prec 90.88% / 84.41%	
Extract Features: [50/302]	Time 0.166 (1.011)	Data 0.001 (0.839)	
Extract Features: [100/302]	Time 0.169 (0.590)	Data 0.000 (0.420)	
Extract Features: [150/302]	Time 0.184 (0.450)	Data 0.001 (0.280)	
Extract Features: [200/302]	Time 0.167 (0.380)	Data 0.000 (0.210)	
Extract Features: [250/302]	Time 0.166 (0.338)	Data 0.000 (0.168)	
Extract Features: [300/302]	Time 0.182 (0.310)	Data 0.001 (0.140)	
Mean AP: 64.5%

 * Finished phase   3 epoch   8  model no.1 mAP: 64.5%  best: 65.1%

Computing original distance...
Computing Jaccard distance...
Time cost: 13.048282384872437
Clustering and labeling...

 Clustered into 132 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [9][20/200]	Time 1.299 (1.310)	Data 0.000 (0.000)	Loss_ce 1.350 / 1.992	Loss_ce_soft 1.259 / 2.925	Loss_tri_soft 0.274 / 0.191	Loss_lf 4.750 / 2.909	Loss_lf_tri 0.298 / 0.228	Prec 92.81% / 85.62%	
Epoch: [9][40/200]	Time 1.272 (2.381)	Data 0.000 (1.073)	Loss_ce 1.375 / 1.995	Loss_ce_soft 1.230 / 2.857	Loss_tri_soft 0.230 / 0.200	Loss_lf 4.709 / 2.915	Loss_lf_tri 0.271 / 0.229	Prec 92.97% / 85.47%	
Epoch: [9][60/200]	Time 1.232 (2.040)	Data 0.000 (0.716)	Loss_ce 1.361 / 2.007	Loss_ce_soft 1.205 / 2.864	Loss_tri_soft 0.192 / 0.186	Loss_lf 4.691 / 2.864	Loss_lf_tri 0.239 / 0.213	Prec 92.81% / 85.42%	
Epoch: [9][80/200]	Time 1.255 (2.383)	Data 0.000 (1.070)	Loss_ce 1.359 / 2.033	Loss_ce_soft 1.214 / 2.867	Loss_tri_soft 0.186 / 0.187	Loss_lf 4.690 / 2.877	Loss_lf_tri 0.236 / 0.211	Prec 92.34% / 85.16%	
Epoch: [9][100/200]	Time 43.717 (2.587)	Data 42.368 (1.280)	Loss_ce 1.349 / 2.050	Loss_ce_soft 1.199 / 2.860	Loss_tri_soft 0.190 / 0.180	Loss_lf 4.684 / 2.877	Loss_lf_tri 0.241 / 0.206	Prec 92.00% / 84.62%	
Epoch: [9][120/200]	Time 1.323 (2.369)	Data 0.000 (1.066)	Loss_ce 1.345 / 2.068	Loss_ce_soft 1.205 / 2.868	Loss_tri_soft 0.190 / 0.182	Loss_lf 4.697 / 2.887	Loss_lf_tri 0.244 / 0.210	Prec 92.24% / 84.38%	
Epoch: [9][140/200]	Time 1.277 (2.515)	Data 0.000 (1.215)	Loss_ce 1.344 / 2.093	Loss_ce_soft 1.205 / 2.884	Loss_tri_soft 0.189 / 0.180	Loss_lf 4.692 / 2.900	Loss_lf_tri 0.244 / 0.209	Prec 91.83% / 84.15%	
Epoch: [9][160/200]	Time 1.334 (2.362)	Data 0.000 (1.063)	Loss_ce 1.335 / 2.103	Loss_ce_soft 1.202 / 2.893	Loss_tri_soft 0.188 / 0.185	Loss_lf 4.687 / 2.901	Loss_lf_tri 0.245 / 0.209	Prec 92.15% / 83.87%	
Epoch: [9][180/200]	Time 1.223 (2.714)	Data 0.001 (1.181)	Loss_ce 1.328 / 2.093	Loss_ce_soft 1.198 / 2.902	Loss_tri_soft 0.185 / 0.188	Loss_lf 4.692 / 2.897	Loss_lf_tri 0.244 / 0.212	Prec 92.29% / 84.13%	
Epoch: [9][200/200]	Time 1.258 (2.778)	Data 0.001 (1.268)	Loss_ce 1.328 / 2.085	Loss_ce_soft 1.197 / 2.919	Loss_tri_soft 0.184 / 0.184	Loss_lf 4.691 / 2.905	Loss_lf_tri 0.243 / 0.209	Prec 92.25% / 84.53%	
Extract Features: [50/302]	Time 0.166 (1.017)	Data 0.000 (0.843)	
Extract Features: [100/302]	Time 0.187 (0.593)	Data 0.001 (0.422)	
Extract Features: [150/302]	Time 0.165 (0.452)	Data 0.000 (0.281)	
Extract Features: [200/302]	Time 0.169 (0.381)	Data 0.001 (0.211)	
Extract Features: [250/302]	Time 0.166 (0.339)	Data 0.001 (0.169)	
Extract Features: [300/302]	Time 0.179 (0.311)	Data 0.000 (0.141)	
Mean AP: 65.0%

 * Finished phase   3 epoch   9  model no.1 mAP: 65.0%  best: 65.1%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.715561389923096
Clustering and labeling...

 Clustered into 141 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [10][20/200]	Time 3.059 (1.405)	Data 0.001 (0.000)	Loss_ce 1.387 / 2.179	Loss_ce_soft 1.269 / 3.197	Loss_tri_soft 0.209 / 0.171	Loss_lf 4.732 / 3.032	Loss_lf_tri 0.256 / 0.202	Prec 92.81% / 81.88%	
Epoch: [10][40/200]	Time 1.297 (2.383)	Data 0.000 (1.042)	Loss_ce 1.391 / 2.071	Loss_ce_soft 1.290 / 2.965	Loss_tri_soft 0.200 / 0.187	Loss_lf 4.717 / 3.078	Loss_lf_tri 0.261 / 0.211	Prec 92.50% / 85.00%	
Epoch: [10][60/200]	Time 1.237 (2.011)	Data 0.001 (0.695)	Loss_ce 1.381 / 2.093	Loss_ce_soft 1.278 / 3.006	Loss_tri_soft 0.171 / 0.203	Loss_lf 4.714 / 3.010	Loss_lf_tri 0.239 / 0.226	Prec 91.98% / 85.00%	
Epoch: [10][80/200]	Time 1.354 (2.361)	Data 0.000 (1.047)	Loss_ce 1.377 / 2.093	Loss_ce_soft 1.272 / 2.989	Loss_tri_soft 0.171 / 0.177	Loss_lf 4.700 / 3.020	Loss_lf_tri 0.233 / 0.204	Prec 91.80% / 84.61%	
Epoch: [10][100/200]	Time 1.370 (2.163)	Data 0.000 (0.838)	Loss_ce 1.375 / 2.101	Loss_ce_soft 1.254 / 2.985	Loss_tri_soft 0.175 / 0.167	Loss_lf 4.696 / 3.003	Loss_lf_tri 0.237 / 0.198	Prec 91.62% / 84.56%	
Epoch: [10][120/200]	Time 1.252 (2.377)	Data 0.001 (1.054)	Loss_ce 1.368 / 2.103	Loss_ce_soft 1.255 / 2.964	Loss_tri_soft 0.175 / 0.168	Loss_lf 4.703 / 3.000	Loss_lf_tri 0.238 / 0.197	Prec 92.08% / 84.27%	
Epoch: [10][140/200]	Time 1.321 (2.227)	Data 0.000 (0.904)	Loss_ce 1.369 / 2.107	Loss_ce_soft 1.267 / 2.951	Loss_tri_soft 0.174 / 0.172	Loss_lf 4.706 / 2.996	Loss_lf_tri 0.237 / 0.202	Prec 91.83% / 84.15%	
Epoch: [10][160/200]	Time 1.309 (2.371)	Data 0.001 (1.051)	Loss_ce 1.362 / 2.109	Loss_ce_soft 1.262 / 2.939	Loss_tri_soft 0.173 / 0.170	Loss_lf 4.704 / 2.960	Loss_lf_tri 0.238 / 0.200	Prec 91.95% / 83.75%	
Epoch: [10][180/200]	Time 1.256 (2.722)	Data 0.001 (1.168)	Loss_ce 1.357 / 2.109	Loss_ce_soft 1.266 / 2.934	Loss_tri_soft 0.172 / 0.175	Loss_lf 4.710 / 2.952	Loss_lf_tri 0.236 / 0.204	Prec 92.19% / 83.78%	
Epoch: [10][200/200]	Time 1.339 (2.577)	Data 0.000 (1.051)	Loss_ce 1.352 / 2.088	Loss_ce_soft 1.270 / 2.918	Loss_tri_soft 0.169 / 0.176	Loss_lf 4.708 / 2.947	Loss_lf_tri 0.233 / 0.205	Prec 92.09% / 84.41%	
Extract Features: [50/302]	Time 0.171 (1.015)	Data 0.001 (0.841)	
Extract Features: [100/302]	Time 0.188 (0.593)	Data 0.000 (0.421)	
Extract Features: [150/302]	Time 0.166 (0.451)	Data 0.001 (0.280)	
Extract Features: [200/302]	Time 0.168 (0.381)	Data 0.000 (0.210)	
Extract Features: [250/302]	Time 0.168 (0.339)	Data 0.000 (0.168)	
Extract Features: [300/302]	Time 0.180 (0.311)	Data 0.000 (0.140)	
Mean AP: 64.6%

 * Finished phase   3 epoch  10  model no.1 mAP: 64.6%  best: 65.1%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.79953384399414
Clustering and labeling...

 Clustered into 141 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [11][20/200]	Time 1.310 (1.346)	Data 0.000 (0.000)	Loss_ce 1.504 / 2.031	Loss_ce_soft 1.454 / 2.810	Loss_tri_soft 0.188 / 0.131	Loss_lf 4.718 / 3.028	Loss_lf_tri 0.269 / 0.172	Prec 85.31% / 82.81%	
Epoch: [11][40/200]	Time 1.267 (2.373)	Data 0.001 (1.059)	Loss_ce 1.488 / 2.045	Loss_ce_soft 1.392 / 2.877	Loss_tri_soft 0.167 / 0.168	Loss_lf 4.746 / 3.018	Loss_lf_tri 0.238 / 0.193	Prec 86.72% / 84.22%	
Epoch: [11][60/200]	Time 1.261 (2.006)	Data 0.001 (0.706)	Loss_ce 1.447 / 2.027	Loss_ce_soft 1.364 / 2.883	Loss_tri_soft 0.163 / 0.157	Loss_lf 4.716 / 2.968	Loss_lf_tri 0.240 / 0.185	Prec 88.23% / 85.62%	
Epoch: [11][80/200]	Time 1.311 (2.353)	Data 0.001 (1.054)	Loss_ce 1.416 / 2.028	Loss_ce_soft 1.315 / 2.903	Loss_tri_soft 0.163 / 0.160	Loss_lf 4.701 / 2.959	Loss_lf_tri 0.235 / 0.190	Prec 89.45% / 86.56%	
Epoch: [11][100/200]	Time 1.248 (2.135)	Data 0.000 (0.843)	Loss_ce 1.415 / 2.031	Loss_ce_soft 1.338 / 2.901	Loss_tri_soft 0.159 / 0.151	Loss_lf 4.708 / 2.942	Loss_lf_tri 0.235 / 0.180	Prec 89.31% / 86.44%	
Epoch: [11][120/200]	Time 1.351 (2.344)	Data 0.000 (1.052)	Loss_ce 1.411 / 2.059	Loss_ce_soft 1.336 / 2.909	Loss_tri_soft 0.168 / 0.149	Loss_lf 4.707 / 2.963	Loss_lf_tri 0.242 / 0.183	Prec 89.32% / 85.31%	
Epoch: [11][140/200]	Time 1.307 (2.200)	Data 0.000 (0.902)	Loss_ce 1.414 / 2.078	Loss_ce_soft 1.346 / 2.945	Loss_tri_soft 0.174 / 0.152	Loss_lf 4.709 / 2.997	Loss_lf_tri 0.248 / 0.188	Prec 89.20% / 85.31%	
Epoch: [11][160/200]	Time 1.292 (2.347)	Data 0.001 (1.051)	Loss_ce 1.405 / 2.084	Loss_ce_soft 1.350 / 2.958	Loss_tri_soft 0.172 / 0.149	Loss_lf 4.708 / 3.006	Loss_lf_tri 0.246 / 0.184	Prec 89.38% / 85.31%	
Epoch: [11][180/200]	Time 1.275 (2.697)	Data 0.000 (1.169)	Loss_ce 1.398 / 2.074	Loss_ce_soft 1.335 / 2.941	Loss_tri_soft 0.171 / 0.147	Loss_lf 4.699 / 2.992	Loss_lf_tri 0.245 / 0.183	Prec 89.48% / 85.45%	
Epoch: [11][200/200]	Time 1.312 (2.555)	Data 0.000 (1.052)	Loss_ce 1.396 / 2.064	Loss_ce_soft 1.339 / 2.936	Loss_tri_soft 0.174 / 0.153	Loss_lf 4.700 / 2.965	Loss_lf_tri 0.246 / 0.189	Prec 89.44% / 85.62%	
Extract Features: [50/302]	Time 0.172 (1.016)	Data 0.000 (0.840)	
Extract Features: [100/302]	Time 0.168 (0.593)	Data 0.000 (0.420)	
Extract Features: [150/302]	Time 0.167 (0.452)	Data 0.001 (0.280)	
Extract Features: [200/302]	Time 0.166 (0.382)	Data 0.001 (0.210)	
Extract Features: [250/302]	Time 0.165 (0.339)	Data 0.000 (0.168)	
Extract Features: [300/302]	Time 0.185 (0.311)	Data 0.000 (0.140)	
Mean AP: 64.9%

 * Finished phase   3 epoch  11  model no.1 mAP: 64.9%  best: 65.1%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.631587266921997
Clustering and labeling...

 Clustered into 141 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [12][20/200]	Time 1.271 (1.354)	Data 0.000 (0.000)	Loss_ce 1.412 / 1.982	Loss_ce_soft 1.360 / 2.851	Loss_tri_soft 0.203 / 0.163	Loss_lf 4.756 / 2.879	Loss_lf_tri 0.263 / 0.185	Prec 91.88% / 85.31%	
Epoch: [12][40/200]	Time 1.239 (2.367)	Data 0.000 (1.055)	Loss_ce 1.398 / 2.010	Loss_ce_soft 1.324 / 2.858	Loss_tri_soft 0.199 / 0.160	Loss_lf 4.752 / 3.012	Loss_lf_tri 0.257 / 0.190	Prec 91.88% / 85.78%	
Epoch: [12][60/200]	Time 1.333 (2.004)	Data 0.000 (0.703)	Loss_ce 1.396 / 2.038	Loss_ce_soft 1.316 / 2.845	Loss_tri_soft 0.223 / 0.164	Loss_lf 4.753 / 2.943	Loss_lf_tri 0.276 / 0.199	Prec 91.04% / 85.00%	
Epoch: [12][80/200]	Time 1.278 (2.352)	Data 0.000 (1.054)	Loss_ce 1.387 / 2.043	Loss_ce_soft 1.317 / 2.840	Loss_tri_soft 0.226 / 0.156	Loss_lf 4.727 / 2.925	Loss_lf_tri 0.277 / 0.189	Prec 91.64% / 85.08%	
Epoch: [12][100/200]	Time 1.317 (2.140)	Data 0.000 (0.843)	Loss_ce 1.379 / 2.037	Loss_ce_soft 1.299 / 2.844	Loss_tri_soft 0.213 / 0.144	Loss_lf 4.733 / 2.950	Loss_lf_tri 0.270 / 0.179	Prec 91.81% / 85.62%	
Epoch: [12][120/200]	Time 1.287 (2.355)	Data 0.000 (1.060)	Loss_ce 1.381 / 2.055	Loss_ce_soft 1.298 / 2.865	Loss_tri_soft 0.213 / 0.148	Loss_lf 4.731 / 2.957	Loss_lf_tri 0.270 / 0.185	Prec 91.67% / 85.31%	
Epoch: [12][140/200]	Time 1.230 (2.201)	Data 0.000 (0.908)	Loss_ce 1.373 / 2.069	Loss_ce_soft 1.279 / 2.873	Loss_tri_soft 0.210 / 0.145	Loss_lf 4.720 / 2.962	Loss_lf_tri 0.268 / 0.182	Prec 91.88% / 85.09%	
Epoch: [12][160/200]	Time 1.308 (2.347)	Data 0.000 (1.050)	Loss_ce 1.373 / 2.070	Loss_ce_soft 1.289 / 2.870	Loss_tri_soft 0.211 / 0.145	Loss_lf 4.724 / 2.943	Loss_lf_tri 0.272 / 0.184	Prec 91.99% / 85.55%	
Epoch: [12][180/200]	Time 1.370 (2.701)	Data 0.000 (1.172)	Loss_ce 1.373 / 2.050	Loss_ce_soft 1.284 / 2.858	Loss_tri_soft 0.202 / 0.141	Loss_lf 4.715 / 2.921	Loss_lf_tri 0.263 / 0.180	Prec 91.77% / 85.97%	
Epoch: [12][200/200]	Time 1.294 (2.564)	Data 0.000 (1.055)	Loss_ce 1.367 / 2.057	Loss_ce_soft 1.281 / 2.879	Loss_tri_soft 0.198 / 0.144	Loss_lf 4.716 / 2.946	Loss_lf_tri 0.259 / 0.185	Prec 91.72% / 85.94%	
Extract Features: [50/302]	Time 0.173 (1.029)	Data 0.000 (0.857)	
Extract Features: [100/302]	Time 0.165 (0.600)	Data 0.000 (0.429)	
Extract Features: [150/302]	Time 0.189 (0.457)	Data 0.000 (0.286)	
Extract Features: [200/302]	Time 0.167 (0.386)	Data 0.001 (0.215)	
Extract Features: [250/302]	Time 0.169 (0.344)	Data 0.001 (0.172)	
Extract Features: [300/302]	Time 0.159 (0.315)	Data 0.000 (0.143)	
Mean AP: 65.1%

 * Finished phase   3 epoch  12  model no.1 mAP: 65.1%  best: 65.1% *

Computing original distance...
Computing Jaccard distance...
Time cost: 13.347716808319092
Clustering and labeling...

 Clustered into 137 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [13][20/200]	Time 1.302 (1.382)	Data 0.000 (0.000)	Loss_ce 1.390 / 2.079	Loss_ce_soft 1.294 / 2.935	Loss_tri_soft 0.172 / 0.134	Loss_lf 4.754 / 2.853	Loss_lf_tri 0.236 / 0.182	Prec 91.88% / 84.06%	
Epoch: [13][40/200]	Time 1.322 (2.407)	Data 0.001 (1.054)	Loss_ce 1.375 / 2.077	Loss_ce_soft 1.286 / 2.916	Loss_tri_soft 0.159 / 0.164	Loss_lf 4.725 / 2.920	Loss_lf_tri 0.233 / 0.208	Prec 92.03% / 83.28%	
Epoch: [13][60/200]	Time 1.341 (2.048)	Data 0.000 (0.703)	Loss_ce 1.369 / 2.077	Loss_ce_soft 1.286 / 2.903	Loss_tri_soft 0.143 / 0.146	Loss_lf 4.726 / 2.914	Loss_lf_tri 0.217 / 0.192	Prec 92.29% / 84.17%	
Epoch: [13][80/200]	Time 1.236 (2.396)	Data 0.000 (1.062)	Loss_ce 1.353 / 2.074	Loss_ce_soft 1.273 / 2.897	Loss_tri_soft 0.145 / 0.142	Loss_lf 4.730 / 2.916	Loss_lf_tri 0.220 / 0.192	Prec 92.73% / 84.06%	
Epoch: [13][100/200]	Time 1.306 (2.175)	Data 0.000 (0.850)	Loss_ce 1.361 / 2.067	Loss_ce_soft 1.270 / 2.878	Loss_tri_soft 0.151 / 0.134	Loss_lf 4.735 / 2.945	Loss_lf_tri 0.227 / 0.184	Prec 92.06% / 84.19%	
Epoch: [13][120/200]	Time 1.308 (2.402)	Data 0.000 (1.073)	Loss_ce 1.367 / 2.090	Loss_ce_soft 1.278 / 2.894	Loss_tri_soft 0.160 / 0.136	Loss_lf 4.730 / 2.936	Loss_lf_tri 0.237 / 0.185	Prec 91.82% / 83.91%	
Epoch: [13][140/200]	Time 1.244 (2.560)	Data 0.001 (1.231)	Loss_ce 1.365 / 2.093	Loss_ce_soft 1.270 / 2.888	Loss_tri_soft 0.160 / 0.140	Loss_lf 4.715 / 2.944	Loss_lf_tri 0.234 / 0.187	Prec 91.96% / 84.15%	
Epoch: [13][160/200]	Time 1.346 (2.403)	Data 0.000 (1.077)	Loss_ce 1.363 / 2.122	Loss_ce_soft 1.273 / 2.933	Loss_tri_soft 0.162 / 0.150	Loss_lf 4.715 / 2.973	Loss_lf_tri 0.235 / 0.193	Prec 91.72% / 83.40%	
Epoch: [13][180/200]	Time 1.318 (2.760)	Data 0.000 (1.192)	Loss_ce 1.356 / 2.104	Loss_ce_soft 1.269 / 2.912	Loss_tri_soft 0.162 / 0.156	Loss_lf 4.717 / 2.974	Loss_lf_tri 0.235 / 0.198	Prec 91.84% / 83.92%	
Epoch: [13][200/200]	Time 1.238 (2.610)	Data 0.000 (1.073)	Loss_ce 1.350 / 2.093	Loss_ce_soft 1.267 / 2.910	Loss_tri_soft 0.161 / 0.156	Loss_lf 4.711 / 2.974	Loss_lf_tri 0.232 / 0.195	Prec 91.97% / 84.16%	
Extract Features: [50/302]	Time 0.170 (1.033)	Data 0.000 (0.860)	
Extract Features: [100/302]	Time 0.165 (0.601)	Data 0.000 (0.430)	
Extract Features: [150/302]	Time 0.169 (0.456)	Data 0.001 (0.287)	
Extract Features: [200/302]	Time 0.169 (0.384)	Data 0.001 (0.215)	
Extract Features: [250/302]	Time 0.165 (0.342)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.179 (0.313)	Data 0.000 (0.144)	
Mean AP: 64.9%

 * Finished phase   3 epoch  13  model no.1 mAP: 64.9%  best: 65.1%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.777540445327759
Clustering and labeling...

 Clustered into 146 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [14][20/200]	Time 1.353 (1.358)	Data 0.001 (0.000)	Loss_ce 1.471 / 2.109	Loss_ce_soft 1.335 / 3.060	Loss_tri_soft 0.202 / 0.090	Loss_lf 4.671 / 3.155	Loss_lf_tri 0.287 / 0.140	Prec 89.06% / 81.56%	
Epoch: [14][40/200]	Time 1.367 (2.479)	Data 0.000 (1.126)	Loss_ce 1.444 / 2.071	Loss_ce_soft 1.337 / 2.996	Loss_tri_soft 0.199 / 0.118	Loss_lf 4.699 / 3.056	Loss_lf_tri 0.276 / 0.157	Prec 89.69% / 83.75%	
Epoch: [14][60/200]	Time 1.338 (2.126)	Data 0.000 (0.751)	Loss_ce 1.429 / 2.078	Loss_ce_soft 1.336 / 2.979	Loss_tri_soft 0.188 / 0.117	Loss_lf 4.709 / 3.038	Loss_lf_tri 0.264 / 0.159	Prec 89.79% / 84.06%	
Epoch: [14][80/200]	Time 1.235 (2.451)	Data 0.000 (1.093)	Loss_ce 1.432 / 2.091	Loss_ce_soft 1.374 / 2.958	Loss_tri_soft 0.190 / 0.129	Loss_lf 4.724 / 2.959	Loss_lf_tri 0.259 / 0.169	Prec 90.39% / 83.67%	
Epoch: [14][100/200]	Time 1.297 (2.224)	Data 0.000 (0.874)	Loss_ce 1.413 / 2.080	Loss_ce_soft 1.346 / 2.924	Loss_tri_soft 0.181 / 0.130	Loss_lf 4.719 / 2.945	Loss_lf_tri 0.250 / 0.172	Prec 90.62% / 84.25%	
Epoch: [14][120/200]	Time 1.257 (2.426)	Data 0.000 (1.085)	Loss_ce 1.410 / 2.093	Loss_ce_soft 1.347 / 2.932	Loss_tri_soft 0.181 / 0.133	Loss_lf 4.723 / 2.982	Loss_lf_tri 0.249 / 0.177	Prec 90.57% / 84.11%	
Epoch: [14][140/200]	Time 1.296 (2.269)	Data 0.000 (0.930)	Loss_ce 1.407 / 2.116	Loss_ce_soft 1.339 / 2.953	Loss_tri_soft 0.175 / 0.138	Loss_lf 4.721 / 2.999	Loss_lf_tri 0.246 / 0.182	Prec 90.40% / 83.75%	
Epoch: [14][160/200]	Time 1.349 (2.428)	Data 0.000 (1.089)	Loss_ce 1.408 / 2.116	Loss_ce_soft 1.334 / 2.946	Loss_tri_soft 0.175 / 0.138	Loss_lf 4.719 / 3.002	Loss_lf_tri 0.246 / 0.182	Prec 90.23% / 83.83%	
Epoch: [14][180/200]	Time 1.341 (2.556)	Data 0.001 (0.968)	Loss_ce 1.406 / 2.103	Loss_ce_soft 1.338 / 2.939	Loss_tri_soft 0.174 / 0.135	Loss_lf 4.719 / 2.986	Loss_lf_tri 0.244 / 0.179	Prec 90.52% / 84.27%	
Epoch: [14][200/200]	Time 1.278 (2.643)	Data 0.000 (1.086)	Loss_ce 1.398 / 2.080	Loss_ce_soft 1.333 / 2.914	Loss_tri_soft 0.169 / 0.135	Loss_lf 4.722 / 2.989	Loss_lf_tri 0.239 / 0.181	Prec 90.75% / 84.88%	
Extract Features: [50/302]	Time 0.164 (1.014)	Data 0.000 (0.844)	
Extract Features: [100/302]	Time 0.164 (0.591)	Data 0.001 (0.422)	
Extract Features: [150/302]	Time 0.168 (0.451)	Data 0.000 (0.282)	
Extract Features: [200/302]	Time 0.169 (0.381)	Data 0.001 (0.211)	
Extract Features: [250/302]	Time 0.164 (0.338)	Data 0.000 (0.169)	
Extract Features: [300/302]	Time 0.160 (0.310)	Data 0.001 (0.141)	
Mean AP: 64.6%

 * Finished phase   3 epoch  14  model no.1 mAP: 64.6%  best: 65.1%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.952484846115112
Clustering and labeling...

 Clustered into 152 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [15][20/200]	Time 1.341 (1.337)	Data 0.000 (0.000)	Loss_ce 1.412 / 2.132	Loss_ce_soft 1.353 / 3.071	Loss_tri_soft 0.147 / 0.183	Loss_lf 4.786 / 3.311	Loss_lf_tri 0.221 / 0.241	Prec 90.62% / 78.44%	
Epoch: [15][40/200]	Time 1.331 (2.439)	Data 0.000 (1.102)	Loss_ce 1.433 / 2.097	Loss_ce_soft 1.363 / 2.991	Loss_tri_soft 0.145 / 0.160	Loss_lf 4.746 / 3.082	Loss_lf_tri 0.221 / 0.203	Prec 89.38% / 80.78%	
Epoch: [15][60/200]	Time 1.337 (2.071)	Data 0.001 (0.735)	Loss_ce 1.426 / 2.075	Loss_ce_soft 1.363 / 2.968	Loss_tri_soft 0.150 / 0.169	Loss_lf 4.718 / 3.112	Loss_lf_tri 0.224 / 0.210	Prec 89.79% / 81.98%	
Epoch: [15][80/200]	Time 1.342 (2.451)	Data 0.000 (1.116)	Loss_ce 1.442 / 2.060	Loss_ce_soft 1.387 / 2.931	Loss_tri_soft 0.165 / 0.153	Loss_lf 4.741 / 3.051	Loss_lf_tri 0.233 / 0.192	Prec 89.38% / 82.89%	
Epoch: [15][100/200]	Time 1.340 (2.229)	Data 0.000 (0.892)	Loss_ce 1.425 / 2.068	Loss_ce_soft 1.379 / 2.928	Loss_tri_soft 0.163 / 0.148	Loss_lf 4.744 / 3.014	Loss_lf_tri 0.231 / 0.189	Prec 90.06% / 83.12%	
Epoch: [15][120/200]	Time 1.346 (2.474)	Data 0.000 (1.121)	Loss_ce 1.422 / 2.073	Loss_ce_soft 1.376 / 2.922	Loss_tri_soft 0.157 / 0.144	Loss_lf 4.736 / 3.004	Loss_lf_tri 0.230 / 0.187	Prec 90.21% / 83.02%	
Epoch: [15][140/200]	Time 1.323 (2.315)	Data 0.000 (0.961)	Loss_ce 1.428 / 2.070	Loss_ce_soft 1.386 / 2.908	Loss_tri_soft 0.154 / 0.145	Loss_lf 4.728 / 2.999	Loss_lf_tri 0.229 / 0.188	Prec 89.82% / 83.30%	
Epoch: [15][160/200]	Time 1.373 (2.468)	Data 0.000 (1.116)	Loss_ce 1.425 / 2.071	Loss_ce_soft 1.388 / 2.907	Loss_tri_soft 0.154 / 0.142	Loss_lf 4.730 / 2.997	Loss_lf_tri 0.230 / 0.186	Prec 89.69% / 83.87%	
Epoch: [15][180/200]	Time 1.313 (2.592)	Data 0.000 (0.992)	Loss_ce 1.414 / 2.067	Loss_ce_soft 1.372 / 2.896	Loss_tri_soft 0.159 / 0.144	Loss_lf 4.731 / 2.989	Loss_lf_tri 0.233 / 0.187	Prec 89.93% / 84.17%	
Epoch: [15][200/200]	Time 1.324 (2.692)	Data 0.000 (1.117)	Loss_ce 1.415 / 2.056	Loss_ce_soft 1.384 / 2.896	Loss_tri_soft 0.164 / 0.146	Loss_lf 4.733 / 2.984	Loss_lf_tri 0.239 / 0.188	Prec 89.75% / 84.31%	
Extract Features: [50/302]	Time 0.166 (1.048)	Data 0.001 (0.878)	
Extract Features: [100/302]	Time 0.173 (0.608)	Data 0.001 (0.439)	
Extract Features: [150/302]	Time 0.184 (0.462)	Data 0.000 (0.293)	
Extract Features: [200/302]	Time 0.164 (0.388)	Data 0.001 (0.220)	
Extract Features: [250/302]	Time 0.166 (0.344)	Data 0.000 (0.176)	
Extract Features: [300/302]	Time 0.158 (0.314)	Data 0.000 (0.146)	
Mean AP: 65.2%

 * Finished phase   3 epoch  15  model no.1 mAP: 65.2%  best: 65.2% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.618591547012329
Clustering and labeling...

 Clustered into 145 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [16][20/200]	Time 1.335 (1.341)	Data 0.001 (0.000)	Loss_ce 1.432 / 2.056	Loss_ce_soft 1.270 / 2.863	Loss_tri_soft 0.175 / 0.201	Loss_lf 4.699 / 2.971	Loss_lf_tri 0.243 / 0.231	Prec 89.69% / 85.00%	
Epoch: [16][40/200]	Time 1.297 (2.446)	Data 0.000 (1.103)	Loss_ce 1.416 / 2.008	Loss_ce_soft 1.243 / 2.790	Loss_tri_soft 0.180 / 0.175	Loss_lf 4.720 / 2.968	Loss_lf_tri 0.237 / 0.215	Prec 89.06% / 85.78%	
Epoch: [16][60/200]	Time 1.310 (2.075)	Data 0.000 (0.736)	Loss_ce 1.390 / 2.023	Loss_ce_soft 1.247 / 2.822	Loss_tri_soft 0.174 / 0.142	Loss_lf 4.693 / 2.946	Loss_lf_tri 0.237 / 0.184	Prec 90.42% / 85.21%	
Epoch: [16][80/200]	Time 1.262 (2.429)	Data 0.001 (1.096)	Loss_ce 1.380 / 2.024	Loss_ce_soft 1.254 / 2.835	Loss_tri_soft 0.179 / 0.131	Loss_lf 4.719 / 2.980	Loss_lf_tri 0.244 / 0.174	Prec 90.55% / 85.31%	
Epoch: [16][100/200]	Time 1.332 (2.210)	Data 0.000 (0.877)	Loss_ce 1.378 / 2.023	Loss_ce_soft 1.259 / 2.844	Loss_tri_soft 0.174 / 0.126	Loss_lf 4.719 / 2.967	Loss_lf_tri 0.242 / 0.171	Prec 90.44% / 84.88%	
Epoch: [16][120/200]	Time 1.380 (2.457)	Data 0.001 (1.103)	Loss_ce 1.391 / 2.047	Loss_ce_soft 1.295 / 2.863	Loss_tri_soft 0.181 / 0.135	Loss_lf 4.698 / 2.975	Loss_lf_tri 0.251 / 0.182	Prec 90.62% / 84.58%	
Epoch: [16][140/200]	Time 1.359 (2.297)	Data 0.000 (0.946)	Loss_ce 1.382 / 2.056	Loss_ce_soft 1.295 / 2.867	Loss_tri_soft 0.174 / 0.139	Loss_lf 4.707 / 2.943	Loss_lf_tri 0.242 / 0.186	Prec 90.89% / 84.96%	
Epoch: [16][160/200]	Time 1.294 (2.446)	Data 0.000 (1.097)	Loss_ce 1.375 / 2.074	Loss_ce_soft 1.301 / 2.901	Loss_tri_soft 0.171 / 0.140	Loss_lf 4.710 / 2.953	Loss_lf_tri 0.240 / 0.187	Prec 91.05% / 84.61%	
Epoch: [16][180/200]	Time 1.250 (2.559)	Data 0.000 (0.975)	Loss_ce 1.377 / 2.072	Loss_ce_soft 1.307 / 2.900	Loss_tri_soft 0.169 / 0.143	Loss_lf 4.710 / 2.961	Loss_lf_tri 0.241 / 0.190	Prec 91.01% / 84.69%	
Epoch: [16][200/200]	Time 1.307 (2.657)	Data 0.001 (1.099)	Loss_ce 1.379 / 2.064	Loss_ce_soft 1.308 / 2.893	Loss_tri_soft 0.167 / 0.145	Loss_lf 4.709 / 2.960	Loss_lf_tri 0.242 / 0.192	Prec 90.91% / 84.88%	
Extract Features: [50/302]	Time 0.169 (1.061)	Data 0.000 (0.890)	
Extract Features: [100/302]	Time 0.183 (0.615)	Data 0.000 (0.445)	
Extract Features: [150/302]	Time 0.166 (0.467)	Data 0.000 (0.297)	
Extract Features: [200/302]	Time 0.164 (0.392)	Data 0.000 (0.223)	
Extract Features: [250/302]	Time 0.166 (0.347)	Data 0.000 (0.178)	
Extract Features: [300/302]	Time 0.160 (0.317)	Data 0.000 (0.149)	
Mean AP: 65.3%

 * Finished phase   3 epoch  16  model no.1 mAP: 65.3%  best: 65.3% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.76054573059082
Clustering and labeling...

 Clustered into 139 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [17][20/200]	Time 1.323 (1.346)	Data 0.000 (0.000)	Loss_ce 1.392 / 1.939	Loss_ce_soft 1.209 / 2.829	Loss_tri_soft 0.169 / 0.148	Loss_lf 4.663 / 2.765	Loss_lf_tri 0.235 / 0.184	Prec 90.62% / 87.50%	
Epoch: [17][40/200]	Time 1.319 (2.456)	Data 0.000 (1.114)	Loss_ce 1.384 / 1.993	Loss_ce_soft 1.225 / 2.863	Loss_tri_soft 0.163 / 0.148	Loss_lf 4.669 / 2.938	Loss_lf_tri 0.239 / 0.189	Prec 91.25% / 86.25%	
Epoch: [17][60/200]	Time 1.334 (2.083)	Data 0.000 (0.743)	Loss_ce 1.376 / 2.018	Loss_ce_soft 1.241 / 2.848	Loss_tri_soft 0.160 / 0.147	Loss_lf 4.689 / 2.918	Loss_lf_tri 0.237 / 0.191	Prec 91.67% / 85.83%	
Epoch: [17][80/200]	Time 1.345 (2.431)	Data 0.000 (1.091)	Loss_ce 1.355 / 2.033	Loss_ce_soft 1.223 / 2.867	Loss_tri_soft 0.160 / 0.161	Loss_lf 4.700 / 2.888	Loss_lf_tri 0.231 / 0.198	Prec 92.19% / 85.47%	
Epoch: [17][100/200]	Time 1.367 (2.212)	Data 0.001 (0.873)	Loss_ce 1.371 / 2.057	Loss_ce_soft 1.232 / 2.873	Loss_tri_soft 0.170 / 0.155	Loss_lf 4.700 / 2.885	Loss_lf_tri 0.242 / 0.194	Prec 91.75% / 85.25%	
Epoch: [17][120/200]	Time 1.381 (2.435)	Data 0.001 (1.081)	Loss_ce 1.375 / 2.058	Loss_ce_soft 1.249 / 2.879	Loss_tri_soft 0.174 / 0.149	Loss_lf 4.698 / 2.890	Loss_lf_tri 0.247 / 0.191	Prec 91.30% / 85.36%	
Epoch: [17][140/200]	Time 1.346 (2.584)	Data 0.001 (1.231)	Loss_ce 1.374 / 2.066	Loss_ce_soft 1.244 / 2.875	Loss_tri_soft 0.172 / 0.150	Loss_lf 4.702 / 2.909	Loss_lf_tri 0.247 / 0.194	Prec 91.38% / 85.00%	
Epoch: [17][160/200]	Time 1.358 (2.428)	Data 0.000 (1.077)	Loss_ce 1.363 / 2.073	Loss_ce_soft 1.240 / 2.871	Loss_tri_soft 0.167 / 0.143	Loss_lf 4.690 / 2.900	Loss_lf_tri 0.244 / 0.187	Prec 91.52% / 84.92%	
Epoch: [17][180/200]	Time 1.340 (2.785)	Data 0.000 (1.197)	Loss_ce 1.352 / 2.064	Loss_ce_soft 1.232 / 2.867	Loss_tri_soft 0.164 / 0.143	Loss_lf 4.682 / 2.908	Loss_lf_tri 0.243 / 0.187	Prec 91.98% / 85.03%	
Epoch: [17][200/200]	Time 1.365 (2.642)	Data 0.000 (1.077)	Loss_ce 1.348 / 2.049	Loss_ce_soft 1.235 / 2.867	Loss_tri_soft 0.162 / 0.145	Loss_lf 4.685 / 2.907	Loss_lf_tri 0.243 / 0.188	Prec 92.03% / 85.50%	
Extract Features: [50/302]	Time 0.169 (1.068)	Data 0.001 (0.898)	
Extract Features: [100/302]	Time 0.182 (0.618)	Data 0.000 (0.449)	
Extract Features: [150/302]	Time 0.183 (0.468)	Data 0.000 (0.299)	
Extract Features: [200/302]	Time 0.172 (0.393)	Data 0.000 (0.225)	
Extract Features: [250/302]	Time 0.167 (0.349)	Data 0.000 (0.180)	
Extract Features: [300/302]	Time 0.159 (0.318)	Data 0.000 (0.150)	
Mean AP: 65.0%

 * Finished phase   3 epoch  17  model no.1 mAP: 65.0%  best: 65.3%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.939489364624023
Clustering and labeling...

 Clustered into 146 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [18][20/200]	Time 1.322 (1.310)	Data 0.000 (0.000)	Loss_ce 1.502 / 2.036	Loss_ce_soft 1.465 / 2.978	Loss_tri_soft 0.165 / 0.140	Loss_lf 4.773 / 2.700	Loss_lf_tri 0.251 / 0.186	Prec 89.38% / 84.06%	
Epoch: [18][40/200]	Time 1.312 (2.435)	Data 0.000 (1.111)	Loss_ce 1.445 / 2.054	Loss_ce_soft 1.364 / 3.011	Loss_tri_soft 0.164 / 0.153	Loss_lf 4.735 / 2.851	Loss_lf_tri 0.254 / 0.203	Prec 90.62% / 84.69%	
Epoch: [18][60/200]	Time 1.325 (2.063)	Data 0.000 (0.741)	Loss_ce 1.401 / 2.043	Loss_ce_soft 1.317 / 2.957	Loss_tri_soft 0.160 / 0.147	Loss_lf 4.726 / 2.886	Loss_lf_tri 0.240 / 0.195	Prec 91.56% / 85.00%	
Epoch: [18][80/200]	Time 1.267 (2.429)	Data 0.000 (1.113)	Loss_ce 1.395 / 2.048	Loss_ce_soft 1.331 / 2.924	Loss_tri_soft 0.152 / 0.156	Loss_lf 4.713 / 2.930	Loss_lf_tri 0.236 / 0.204	Prec 91.80% / 85.00%	
Epoch: [18][100/200]	Time 1.324 (2.276)	Data 0.000 (0.890)	Loss_ce 1.400 / 2.051	Loss_ce_soft 1.350 / 2.892	Loss_tri_soft 0.158 / 0.154	Loss_lf 4.729 / 2.926	Loss_lf_tri 0.243 / 0.202	Prec 90.88% / 85.00%	
Epoch: [18][120/200]	Time 1.319 (2.496)	Data 0.001 (1.101)	Loss_ce 1.384 / 2.053	Loss_ce_soft 1.322 / 2.872	Loss_tri_soft 0.154 / 0.153	Loss_lf 4.721 / 2.912	Loss_lf_tri 0.241 / 0.203	Prec 91.35% / 85.26%	
Epoch: [18][140/200]	Time 1.323 (2.331)	Data 0.000 (0.943)	Loss_ce 1.378 / 2.049	Loss_ce_soft 1.312 / 2.859	Loss_tri_soft 0.154 / 0.146	Loss_lf 4.713 / 2.915	Loss_lf_tri 0.240 / 0.196	Prec 91.52% / 85.40%	
Epoch: [18][160/200]	Time 1.350 (2.484)	Data 0.000 (1.104)	Loss_ce 1.377 / 2.046	Loss_ce_soft 1.311 / 2.841	Loss_tri_soft 0.150 / 0.142	Loss_lf 4.721 / 2.938	Loss_lf_tri 0.236 / 0.192	Prec 91.56% / 85.51%	
Epoch: [18][180/200]	Time 1.325 (2.609)	Data 0.000 (0.982)	Loss_ce 1.373 / 2.042	Loss_ce_soft 1.300 / 2.850	Loss_tri_soft 0.149 / 0.148	Loss_lf 4.716 / 2.928	Loss_lf_tri 0.237 / 0.197	Prec 91.42% / 85.59%	
Epoch: [18][200/200]	Time 1.283 (2.703)	Data 0.000 (1.105)	Loss_ce 1.364 / 2.030	Loss_ce_soft 1.293 / 2.845	Loss_tri_soft 0.151 / 0.148	Loss_lf 4.717 / 2.926	Loss_lf_tri 0.236 / 0.199	Prec 91.66% / 85.72%	
Extract Features: [50/302]	Time 0.166 (1.067)	Data 0.000 (0.896)	
Extract Features: [100/302]	Time 0.164 (0.618)	Data 0.000 (0.448)	
Extract Features: [150/302]	Time 0.167 (0.468)	Data 0.001 (0.299)	
Extract Features: [200/302]	Time 0.163 (0.393)	Data 0.000 (0.224)	
Extract Features: [250/302]	Time 0.165 (0.348)	Data 0.001 (0.179)	
Extract Features: [300/302]	Time 0.157 (0.318)	Data 0.000 (0.150)	
Mean AP: 65.3%

 * Finished phase   3 epoch  18  model no.1 mAP: 65.3%  best: 65.3% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.965480089187622
Clustering and labeling...

 Clustered into 140 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [19][20/200]	Time 1.353 (1.326)	Data 0.000 (0.000)	Loss_ce 1.377 / 1.989	Loss_ce_soft 1.337 / 2.766	Loss_tri_soft 0.179 / 0.140	Loss_lf 4.772 / 2.953	Loss_lf_tri 0.273 / 0.192	Prec 91.25% / 86.56%	
Epoch: [19][40/200]	Time 1.321 (2.458)	Data 0.000 (1.127)	Loss_ce 1.402 / 2.031	Loss_ce_soft 1.306 / 2.892	Loss_tri_soft 0.188 / 0.156	Loss_lf 4.744 / 3.031	Loss_lf_tri 0.277 / 0.200	Prec 91.25% / 85.16%	
Epoch: [19][60/200]	Time 1.293 (2.087)	Data 0.000 (0.751)	Loss_ce 1.361 / 2.006	Loss_ce_soft 1.244 / 2.853	Loss_tri_soft 0.160 / 0.177	Loss_lf 4.722 / 3.018	Loss_lf_tri 0.244 / 0.216	Prec 91.98% / 86.15%	
Epoch: [19][80/200]	Time 1.323 (2.433)	Data 0.000 (1.097)	Loss_ce 1.370 / 2.009	Loss_ce_soft 1.253 / 2.823	Loss_tri_soft 0.177 / 0.162	Loss_lf 4.736 / 3.006	Loss_lf_tri 0.263 / 0.203	Prec 91.88% / 86.09%	
Epoch: [19][100/200]	Time 1.349 (2.214)	Data 0.000 (0.878)	Loss_ce 1.374 / 2.027	Loss_ce_soft 1.259 / 2.823	Loss_tri_soft 0.185 / 0.160	Loss_lf 4.731 / 2.969	Loss_lf_tri 0.271 / 0.201	Prec 91.69% / 85.88%	
Epoch: [19][120/200]	Time 1.373 (2.433)	Data 0.000 (1.087)	Loss_ce 1.368 / 2.049	Loss_ce_soft 1.263 / 2.853	Loss_tri_soft 0.176 / 0.159	Loss_lf 4.737 / 2.979	Loss_lf_tri 0.261 / 0.201	Prec 91.51% / 85.62%	
Epoch: [19][140/200]	Time 1.335 (2.277)	Data 0.000 (0.932)	Loss_ce 1.363 / 2.070	Loss_ce_soft 1.261 / 2.872	Loss_tri_soft 0.168 / 0.152	Loss_lf 4.738 / 2.998	Loss_lf_tri 0.254 / 0.195	Prec 91.56% / 85.13%	
Epoch: [19][160/200]	Time 1.337 (2.438)	Data 0.001 (1.094)	Loss_ce 1.355 / 2.078	Loss_ce_soft 1.252 / 2.882	Loss_tri_soft 0.162 / 0.153	Loss_lf 4.743 / 2.996	Loss_lf_tri 0.250 / 0.197	Prec 91.76% / 84.88%	
Epoch: [19][180/200]	Time 1.392 (2.815)	Data 0.000 (1.221)	Loss_ce 1.353 / 2.070	Loss_ce_soft 1.251 / 2.882	Loss_tri_soft 0.163 / 0.147	Loss_lf 4.737 / 2.980	Loss_lf_tri 0.251 / 0.194	Prec 91.84% / 84.76%	
Epoch: [19][200/200]	Time 1.334 (2.668)	Data 0.000 (1.099)	Loss_ce 1.341 / 2.052	Loss_ce_soft 1.238 / 2.863	Loss_tri_soft 0.157 / 0.141	Loss_lf 4.735 / 2.957	Loss_lf_tri 0.245 / 0.189	Prec 92.22% / 85.16%	
Extract Features: [50/302]	Time 0.172 (1.070)	Data 0.000 (0.900)	
Extract Features: [100/302]	Time 0.184 (0.619)	Data 0.000 (0.450)	
Extract Features: [150/302]	Time 0.164 (0.469)	Data 0.001 (0.300)	
Extract Features: [200/302]	Time 0.163 (0.394)	Data 0.000 (0.225)	
Extract Features: [250/302]	Time 0.165 (0.349)	Data 0.000 (0.180)	
Extract Features: [300/302]	Time 0.157 (0.318)	Data 0.000 (0.150)	
Mean AP: 65.6%

 * Finished phase   3 epoch  19  model no.1 mAP: 65.6%  best: 65.6% *

Computing original distance...
Computing Jaccard distance...
Time cost: 10.55561089515686
Clustering and labeling...

 Clustered into 149 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [20][20/200]	Time 1.397 (1.347)	Data 0.000 (0.000)	Loss_ce 1.477 / 1.958	Loss_ce_soft 1.384 / 2.669	Loss_tri_soft 0.201 / 0.123	Loss_lf 4.765 / 2.806	Loss_lf_tri 0.289 / 0.162	Prec 86.25% / 86.25%	
Epoch: [20][40/200]	Time 1.357 (2.424)	Data 0.000 (1.076)	Loss_ce 1.456 / 1.996	Loss_ce_soft 1.345 / 2.796	Loss_tri_soft 0.190 / 0.144	Loss_lf 4.753 / 2.896	Loss_lf_tri 0.267 / 0.186	Prec 87.03% / 86.56%	
Epoch: [20][60/200]	Time 1.350 (2.066)	Data 0.000 (0.717)	Loss_ce 1.438 / 2.008	Loss_ce_soft 1.322 / 2.836	Loss_tri_soft 0.185 / 0.136	Loss_lf 4.739 / 2.901	Loss_lf_tri 0.267 / 0.182	Prec 88.23% / 86.25%	
Epoch: [20][80/200]	Time 1.251 (2.422)	Data 0.001 (1.078)	Loss_ce 1.443 / 2.017	Loss_ce_soft 1.323 / 2.839	Loss_tri_soft 0.174 / 0.124	Loss_lf 4.744 / 2.950	Loss_lf_tri 0.263 / 0.170	Prec 87.81% / 85.55%	
Epoch: [20][100/200]	Time 1.358 (2.205)	Data 0.001 (0.862)	Loss_ce 1.433 / 2.020	Loss_ce_soft 1.310 / 2.829	Loss_tri_soft 0.181 / 0.123	Loss_lf 4.731 / 2.943	Loss_lf_tri 0.265 / 0.169	Prec 88.50% / 85.75%	
Epoch: [20][120/200]	Time 1.376 (2.412)	Data 0.000 (1.073)	Loss_ce 1.410 / 2.027	Loss_ce_soft 1.274 / 2.846	Loss_tri_soft 0.172 / 0.119	Loss_lf 4.716 / 2.942	Loss_lf_tri 0.255 / 0.165	Prec 89.22% / 85.78%	
Epoch: [20][140/200]	Time 1.333 (2.259)	Data 0.000 (0.920)	Loss_ce 1.404 / 2.031	Loss_ce_soft 1.267 / 2.842	Loss_tri_soft 0.163 / 0.124	Loss_lf 4.712 / 2.956	Loss_lf_tri 0.246 / 0.169	Prec 89.55% / 85.76%	
Epoch: [20][160/200]	Time 1.329 (2.433)	Data 0.000 (1.079)	Loss_ce 1.401 / 2.035	Loss_ce_soft 1.272 / 2.829	Loss_tri_soft 0.161 / 0.129	Loss_lf 4.722 / 2.943	Loss_lf_tri 0.246 / 0.176	Prec 89.77% / 86.05%	
Epoch: [20][180/200]	Time 1.357 (2.553)	Data 0.000 (0.960)	Loss_ce 1.398 / 2.041	Loss_ce_soft 1.277 / 2.838	Loss_tri_soft 0.163 / 0.132	Loss_lf 4.711 / 2.936	Loss_lf_tri 0.247 / 0.178	Prec 90.07% / 85.69%	
Epoch: [20][200/200]	Time 1.329 (2.655)	Data 0.000 (1.087)	Loss_ce 1.393 / 2.012	Loss_ce_soft 1.279 / 2.819	Loss_tri_soft 0.168 / 0.131	Loss_lf 4.715 / 2.919	Loss_lf_tri 0.250 / 0.177	Prec 90.09% / 86.41%	
Extract Features: [50/302]	Time 0.165 (1.074)	Data 0.000 (0.903)	
Extract Features: [100/302]	Time 0.168 (0.621)	Data 0.001 (0.451)	
Extract Features: [150/302]	Time 0.165 (0.470)	Data 0.001 (0.301)	
Extract Features: [200/302]	Time 0.183 (0.394)	Data 0.000 (0.226)	
Extract Features: [250/302]	Time 0.163 (0.349)	Data 0.000 (0.181)	
Extract Features: [300/302]	Time 0.160 (0.318)	Data 0.000 (0.151)	
Mean AP: 65.3%

 * Finished phase   3 epoch  20  model no.1 mAP: 65.3%  best: 65.6%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.695565700531006
Clustering and labeling...

 Clustered into 142 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [21][20/200]	Time 1.328 (1.348)	Data 0.000 (0.000)	Loss_ce 1.423 / 1.956	Loss_ce_soft 1.332 / 2.830	Loss_tri_soft 0.136 / 0.153	Loss_lf 4.743 / 2.692	Loss_lf_tri 0.219 / 0.189	Prec 89.69% / 85.94%	
Epoch: [21][40/200]	Time 1.304 (2.406)	Data 0.000 (1.064)	Loss_ce 1.455 / 1.957	Loss_ce_soft 1.385 / 2.792	Loss_tri_soft 0.176 / 0.134	Loss_lf 4.746 / 2.812	Loss_lf_tri 0.255 / 0.175	Prec 89.69% / 86.88%	
Epoch: [21][60/200]	Time 1.324 (2.052)	Data 0.000 (0.709)	Loss_ce 1.445 / 1.990	Loss_ce_soft 1.347 / 2.789	Loss_tri_soft 0.180 / 0.149	Loss_lf 4.729 / 2.941	Loss_lf_tri 0.261 / 0.195	Prec 89.17% / 86.25%	
Epoch: [21][80/200]	Time 1.348 (2.414)	Data 0.000 (1.070)	Loss_ce 1.420 / 2.001	Loss_ce_soft 1.325 / 2.817	Loss_tri_soft 0.165 / 0.161	Loss_lf 4.734 / 2.946	Loss_lf_tri 0.247 / 0.207	Prec 90.31% / 86.17%	
Epoch: [21][100/200]	Time 1.321 (2.199)	Data 0.001 (0.856)	Loss_ce 1.410 / 2.017	Loss_ce_soft 1.315 / 2.813	Loss_tri_soft 0.168 / 0.154	Loss_lf 4.734 / 2.979	Loss_lf_tri 0.253 / 0.201	Prec 90.62% / 85.38%	
Epoch: [21][120/200]	Time 1.302 (2.434)	Data 0.000 (1.089)	Loss_ce 1.378 / 2.032	Loss_ce_soft 1.265 / 2.832	Loss_tri_soft 0.165 / 0.155	Loss_lf 4.730 / 2.964	Loss_lf_tri 0.245 / 0.204	Prec 91.35% / 84.84%	
Epoch: [21][140/200]	Time 1.331 (2.277)	Data 0.000 (0.933)	Loss_ce 1.370 / 2.048	Loss_ce_soft 1.251 / 2.857	Loss_tri_soft 0.165 / 0.154	Loss_lf 4.728 / 2.947	Loss_lf_tri 0.247 / 0.202	Prec 91.65% / 84.73%	
Epoch: [21][160/200]	Time 1.300 (2.424)	Data 0.000 (1.084)	Loss_ce 1.367 / 2.043	Loss_ce_soft 1.243 / 2.848	Loss_tri_soft 0.163 / 0.159	Loss_lf 4.710 / 2.963	Loss_lf_tri 0.244 / 0.205	Prec 91.52% / 84.84%	
Epoch: [21][180/200]	Time 1.390 (2.776)	Data 0.000 (1.203)	Loss_ce 1.357 / 2.052	Loss_ce_soft 1.231 / 2.873	Loss_tri_soft 0.163 / 0.163	Loss_lf 4.715 / 2.970	Loss_lf_tri 0.246 / 0.211	Prec 91.60% / 84.83%	
Epoch: [21][200/200]	Time 1.301 (2.634)	Data 0.000 (1.082)	Loss_ce 1.356 / 2.042	Loss_ce_soft 1.226 / 2.876	Loss_tri_soft 0.162 / 0.161	Loss_lf 4.712 / 2.976	Loss_lf_tri 0.242 / 0.210	Prec 91.38% / 85.16%	
Extract Features: [50/302]	Time 0.167 (1.050)	Data 0.001 (0.879)	
Extract Features: [100/302]	Time 0.171 (0.628)	Data 0.001 (0.440)	
Extract Features: [150/302]	Time 0.186 (0.475)	Data 0.001 (0.293)	
Extract Features: [200/302]	Time 0.170 (0.399)	Data 0.001 (0.220)	
Extract Features: [250/302]	Time 0.164 (0.353)	Data 0.000 (0.176)	
Extract Features: [300/302]	Time 0.158 (0.322)	Data 0.000 (0.147)	
Mean AP: 65.0%

 * Finished phase   3 epoch  21  model no.1 mAP: 65.0%  best: 65.6%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.692567348480225
Clustering and labeling...

 Clustered into 147 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [22][20/200]	Time 1.426 (1.357)	Data 0.000 (0.000)	Loss_ce 1.357 / 1.953	Loss_ce_soft 1.199 / 2.769	Loss_tri_soft 0.186 / 0.237	Loss_lf 4.750 / 3.113	Loss_lf_tri 0.246 / 0.262	Prec 92.19% / 88.12%	
Epoch: [22][40/200]	Time 1.224 (2.435)	Data 0.001 (1.094)	Loss_ce 1.397 / 2.043	Loss_ce_soft 1.285 / 2.883	Loss_tri_soft 0.184 / 0.200	Loss_lf 4.737 / 3.090	Loss_lf_tri 0.250 / 0.242	Prec 90.78% / 85.62%	
Epoch: [22][60/200]	Time 1.305 (2.060)	Data 0.000 (0.729)	Loss_ce 1.396 / 2.036	Loss_ce_soft 1.311 / 2.857	Loss_tri_soft 0.190 / 0.182	Loss_lf 4.724 / 2.990	Loss_lf_tri 0.258 / 0.230	Prec 90.83% / 85.62%	
Epoch: [22][80/200]	Time 1.329 (2.418)	Data 0.000 (1.083)	Loss_ce 1.390 / 2.041	Loss_ce_soft 1.311 / 2.888	Loss_tri_soft 0.188 / 0.183	Loss_lf 4.732 / 3.019	Loss_lf_tri 0.257 / 0.228	Prec 91.02% / 86.25%	
Epoch: [22][100/200]	Time 1.338 (2.201)	Data 0.000 (0.866)	Loss_ce 1.390 / 2.046	Loss_ce_soft 1.324 / 2.885	Loss_tri_soft 0.187 / 0.178	Loss_lf 4.718 / 3.007	Loss_lf_tri 0.258 / 0.222	Prec 90.81% / 86.19%	
Epoch: [22][120/200]	Time 1.326 (2.423)	Data 0.000 (1.088)	Loss_ce 1.383 / 2.049	Loss_ce_soft 1.306 / 2.879	Loss_tri_soft 0.180 / 0.177	Loss_lf 4.717 / 2.992	Loss_lf_tri 0.254 / 0.222	Prec 90.99% / 86.30%	
Epoch: [22][140/200]	Time 1.351 (2.269)	Data 0.000 (0.933)	Loss_ce 1.383 / 2.040	Loss_ce_soft 1.310 / 2.859	Loss_tri_soft 0.187 / 0.170	Loss_lf 4.720 / 2.968	Loss_lf_tri 0.261 / 0.214	Prec 91.03% / 86.38%	
Epoch: [22][160/200]	Time 1.340 (2.419)	Data 0.000 (1.083)	Loss_ce 1.379 / 2.044	Loss_ce_soft 1.304 / 2.874	Loss_tri_soft 0.184 / 0.164	Loss_lf 4.705 / 2.971	Loss_lf_tri 0.258 / 0.208	Prec 91.02% / 86.56%	
Epoch: [22][180/200]	Time 1.255 (2.532)	Data 0.000 (0.963)	Loss_ce 1.375 / 2.041	Loss_ce_soft 1.306 / 2.861	Loss_tri_soft 0.187 / 0.162	Loss_lf 4.711 / 2.967	Loss_lf_tri 0.262 / 0.206	Prec 91.08% / 86.35%	
Epoch: [22][200/200]	Time 1.333 (2.622)	Data 0.001 (1.081)	Loss_ce 1.369 / 2.023	Loss_ce_soft 1.306 / 2.845	Loss_tri_soft 0.183 / 0.160	Loss_lf 4.708 / 2.948	Loss_lf_tri 0.262 / 0.205	Prec 91.28% / 86.53%	
Extract Features: [50/302]	Time 0.163 (1.031)	Data 0.001 (0.862)	
Extract Features: [100/302]	Time 0.183 (0.598)	Data 0.001 (0.431)	
Extract Features: [150/302]	Time 0.185 (0.455)	Data 0.000 (0.288)	
Extract Features: [200/302]	Time 0.165 (0.383)	Data 0.001 (0.216)	
Extract Features: [250/302]	Time 0.165 (0.340)	Data 0.001 (0.173)	
Extract Features: [300/302]	Time 0.156 (0.318)	Data 0.000 (0.144)	
Mean AP: 65.2%

 * Finished phase   3 epoch  22  model no.1 mAP: 65.2%  best: 65.6%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.688568592071533
Clustering and labeling...

 Clustered into 148 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [23][20/200]	Time 1.318 (1.336)	Data 0.000 (0.000)	Loss_ce 1.328 / 1.958	Loss_ce_soft 1.190 / 2.741	Loss_tri_soft 0.187 / 0.180	Loss_lf 4.745 / 2.802	Loss_lf_tri 0.275 / 0.218	Prec 92.19% / 85.00%	
Epoch: [23][40/200]	Time 1.329 (2.454)	Data 0.000 (1.110)	Loss_ce 1.365 / 1.971	Loss_ce_soft 1.231 / 2.782	Loss_tri_soft 0.156 / 0.146	Loss_lf 4.688 / 2.857	Loss_lf_tri 0.248 / 0.191	Prec 91.72% / 85.78%	
Epoch: [23][60/200]	Time 1.430 (2.094)	Data 0.001 (0.740)	Loss_ce 1.374 / 2.014	Loss_ce_soft 1.264 / 2.822	Loss_tri_soft 0.154 / 0.142	Loss_lf 4.725 / 2.864	Loss_lf_tri 0.246 / 0.191	Prec 91.04% / 85.21%	
Epoch: [23][80/200]	Time 1.347 (2.472)	Data 0.000 (1.113)	Loss_ce 1.369 / 2.040	Loss_ce_soft 1.271 / 2.820	Loss_tri_soft 0.151 / 0.152	Loss_lf 4.736 / 2.871	Loss_lf_tri 0.249 / 0.201	Prec 90.86% / 84.84%	
Epoch: [23][100/200]	Time 1.451 (2.257)	Data 0.000 (0.891)	Loss_ce 1.361 / 2.043	Loss_ce_soft 1.270 / 2.818	Loss_tri_soft 0.168 / 0.148	Loss_lf 4.734 / 2.858	Loss_lf_tri 0.255 / 0.198	Prec 91.38% / 84.75%	
Epoch: [23][120/200]	Time 1.407 (2.485)	Data 0.000 (1.121)	Loss_ce 1.367 / 2.023	Loss_ce_soft 1.271 / 2.794	Loss_tri_soft 0.169 / 0.148	Loss_lf 4.727 / 2.872	Loss_lf_tri 0.256 / 0.201	Prec 91.25% / 85.21%	
Epoch: [23][140/200]	Time 1.325 (2.324)	Data 0.001 (0.961)	Loss_ce 1.360 / 2.033	Loss_ce_soft 1.262 / 2.812	Loss_tri_soft 0.162 / 0.147	Loss_lf 4.717 / 2.890	Loss_lf_tri 0.247 / 0.201	Prec 91.52% / 85.45%	
Epoch: [23][160/200]	Time 1.313 (2.479)	Data 0.001 (1.116)	Loss_ce 1.360 / 2.066	Loss_ce_soft 1.264 / 2.849	Loss_tri_soft 0.163 / 0.153	Loss_lf 4.717 / 2.963	Loss_lf_tri 0.250 / 0.207	Prec 91.68% / 85.08%	
Epoch: [23][180/200]	Time 1.358 (2.600)	Data 0.000 (0.992)	Loss_ce 1.358 / 2.061	Loss_ce_soft 1.268 / 2.858	Loss_tri_soft 0.166 / 0.158	Loss_lf 4.717 / 2.982	Loss_lf_tri 0.253 / 0.212	Prec 91.60% / 85.07%	
Epoch: [23][200/200]	Time 1.414 (2.703)	Data 0.000 (1.119)	Loss_ce 1.367 / 2.045	Loss_ce_soft 1.281 / 2.854	Loss_tri_soft 0.168 / 0.160	Loss_lf 4.710 / 2.997	Loss_lf_tri 0.255 / 0.213	Prec 91.28% / 85.31%	
Extract Features: [50/302]	Time 0.169 (1.072)	Data 0.001 (0.899)	
Extract Features: [100/302]	Time 0.171 (0.622)	Data 0.001 (0.449)	
Extract Features: [150/302]	Time 0.166 (0.472)	Data 0.000 (0.300)	
Extract Features: [200/302]	Time 0.173 (0.397)	Data 0.000 (0.225)	
Extract Features: [250/302]	Time 0.165 (0.352)	Data 0.001 (0.180)	
Extract Features: [300/302]	Time 0.181 (0.322)	Data 0.001 (0.150)	
Mean AP: 65.3%

 * Finished phase   3 epoch  23  model no.1 mAP: 65.3%  best: 65.6%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.09343934059143
Clustering and labeling...

 Clustered into 146 classes 

###############################
Lamda for less forget is set to  22.869193252058544
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [24][20/200]	Time 1.356 (1.349)	Data 0.000 (0.000)	Loss_ce 1.400 / 2.017	Loss_ce_soft 1.240 / 2.877	Loss_tri_soft 0.177 / 0.213	Loss_lf 4.693 / 3.252	Loss_lf_tri 0.256 / 0.256	Prec 91.25% / 84.69%	
Epoch: [24][40/200]	Time 1.291 (2.410)	Data 0.000 (1.067)	Loss_ce 1.391 / 2.073	Loss_ce_soft 1.260 / 2.870	Loss_tri_soft 0.191 / 0.194	Loss_lf 4.716 / 3.174	Loss_lf_tri 0.266 / 0.246	Prec 90.62% / 83.59%	
Epoch: [24][60/200]	Time 1.323 (2.053)	Data 0.000 (0.712)	Loss_ce 1.383 / 2.051	Loss_ce_soft 1.273 / 2.843	Loss_tri_soft 0.177 / 0.176	Loss_lf 4.728 / 3.074	Loss_lf_tri 0.261 / 0.229	Prec 90.73% / 84.69%	
Epoch: [24][80/200]	Time 1.322 (2.412)	Data 0.001 (1.058)	Loss_ce 1.384 / 2.048	Loss_ce_soft 1.256 / 2.851	Loss_tri_soft 0.177 / 0.158	Loss_lf 4.716 / 2.985	Loss_lf_tri 0.264 / 0.213	Prec 91.17% / 85.08%	
Epoch: [24][100/200]	Time 1.354 (2.197)	Data 0.000 (0.846)	Loss_ce 1.376 / 2.067	Loss_ce_soft 1.258 / 2.849	Loss_tri_soft 0.181 / 0.159	Loss_lf 4.731 / 2.981	Loss_lf_tri 0.266 / 0.212	Prec 91.25% / 84.56%	
Epoch: [24][120/200]	Time 1.245 (2.407)	Data 0.000 (1.061)	Loss_ce 1.369 / 2.067	Loss_ce_soft 1.240 / 2.845	Loss_tri_soft 0.173 / 0.156	Loss_lf 4.732 / 2.970	Loss_lf_tri 0.261 / 0.208	Prec 91.30% / 84.74%	
Epoch: [24][140/200]	Time 1.352 (2.250)	Data 0.001 (0.910)	Loss_ce 1.383 / 2.050	Loss_ce_soft 1.257 / 2.820	Loss_tri_soft 0.171 / 0.150	Loss_lf 4.739 / 2.954	Loss_lf_tri 0.260 / 0.201	Prec 90.45% / 85.13%	
Epoch: [24][160/200]	Time 1.236 (2.391)	Data 0.000 (1.058)	Loss_ce 1.384 / 2.042	Loss_ce_soft 1.257 / 2.793	Loss_tri_soft 0.165 / 0.145	Loss_lf 4.739 / 2.933	Loss_lf_tri 0.255 / 0.196	Prec 90.55% / 85.31%	
Epoch: [24][180/200]	Time 1.371 (2.518)	Data 0.001 (0.940)	Loss_ce 1.382 / 2.029	Loss_ce_soft 1.258 / 2.780	Loss_tri_soft 0.162 / 0.142	Loss_lf 4.736 / 2.956	Loss_lf_tri 0.252 / 0.196	Prec 90.66% / 85.59%	
Epoch: [24][200/200]	Time 1.382 (2.620)	Data 0.001 (1.064)	Loss_ce 1.372 / 2.018	Loss_ce_soft 1.258 / 2.780	Loss_tri_soft 0.161 / 0.146	Loss_lf 4.736 / 2.960	Loss_lf_tri 0.253 / 0.198	Prec 90.97% / 85.91%	
Extract Features: [50/302]	Time 0.166 (1.071)	Data 0.000 (0.900)	
Extract Features: [100/302]	Time 0.171 (0.620)	Data 0.001 (0.450)	
Extract Features: [150/302]	Time 0.166 (0.469)	Data 0.000 (0.300)	
Extract Features: [200/302]	Time 0.179 (0.395)	Data 0.001 (0.225)	
Extract Features: [250/302]	Time 0.170 (0.350)	Data 0.000 (0.180)	
Extract Features: [300/302]	Time 0.179 (0.320)	Data 0.000 (0.150)	
Mean AP: 65.2%

 * Finished phase   3 epoch  24  model no.1 mAP: 65.2%  best: 65.6%

update proto_dataset
=> Loaded checkpoint 'logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase4_model_best.pth.tar'
mismatch: module.classifier.fc2.weight torch.Size([140, 2048]) torch.Size([146, 2048])
mismatch: module.classifier_max.fc2.weight torch.Size([140, 2048]) torch.Size([146, 2048])
missing keys in state_dict: {'module.classifier_max.fc2.weight', 'module.classifier.fc2.weight'}
Computing original distance...
Computing Jaccard distance...
Time cost: 11.087216138839722
Clustering and labeling...
phase 3 Clustered into 149 example classes 
delete by camera is 36
delete by cluster distance is 5,total num is 113
NMI of phase3 is 0.9463100710941655 
pickle into logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase3_proto.pkl


 phase 4 have 631 old classes
=> Loaded checkpoint 'logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase4_model_best.pth.tar'
phase:4 input id:100,input image:1549
Computing original distance...
Computing Jaccard distance...
Time cost: 9.06408977508545
eps for cluster: 0.048
Clustering and labeling...

 Clustered into 96 classes 

in_features: 2048 out_features1: 523 out_features2: 108
###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [0][20/200]	Time 1.354 (1.342)	Data 0.000 (0.000)	Loss_ce 1.380 / 1.879	Loss_ce_soft 1.191 / 3.125	Loss_tri_soft 0.215 / 0.177	Loss_lf 4.913 / 3.085	Loss_lf_tri 0.201 / 0.171	Prec 90.31% / 87.81%	
Epoch: [0][40/200]	Time 1.351 (2.487)	Data 0.001 (1.123)	Loss_ce 1.363 / 1.877	Loss_ce_soft 1.220 / 3.041	Loss_tri_soft 0.221 / 0.161	Loss_lf 4.884 / 2.903	Loss_lf_tri 0.216 / 0.154	Prec 90.78% / 88.59%	
Epoch: [0][60/200]	Time 1.385 (2.865)	Data 0.001 (1.497)	Loss_ce 1.337 / 1.926	Loss_ce_soft 1.248 / 3.061	Loss_tri_soft 0.219 / 0.177	Loss_lf 4.939 / 2.840	Loss_lf_tri 0.206 / 0.166	Prec 91.25% / 86.77%	
Epoch: [0][80/200]	Time 3.616 (3.089)	Data 0.000 (1.684)	Loss_ce 1.322 / 1.960	Loss_ce_soft 1.223 / 3.139	Loss_tri_soft 0.219 / 0.175	Loss_lf 4.929 / 2.844	Loss_lf_tri 0.203 / 0.165	Prec 91.25% / 86.95%	
Epoch: [0][100/200]	Time 1.431 (3.201)	Data 0.001 (1.798)	Loss_ce 1.315 / 1.986	Loss_ce_soft 1.232 / 3.176	Loss_tri_soft 0.222 / 0.177	Loss_lf 4.951 / 2.847	Loss_lf_tri 0.206 / 0.164	Prec 91.50% / 86.44%	
Epoch: [0][120/200]	Time 1.390 (2.901)	Data 0.000 (1.498)	Loss_ce 1.316 / 2.026	Loss_ce_soft 1.244 / 3.215	Loss_tri_soft 0.236 / 0.177	Loss_lf 4.957 / 2.876	Loss_lf_tri 0.215 / 0.165	Prec 91.35% / 85.89%	
Epoch: [0][140/200]	Time 1.393 (3.005)	Data 0.000 (1.605)	Loss_ce 1.307 / 2.070	Loss_ce_soft 1.233 / 3.264	Loss_tri_soft 0.230 / 0.177	Loss_lf 4.954 / 2.912	Loss_lf_tri 0.214 / 0.165	Prec 91.56% / 85.45%	
Epoch: [0][160/200]	Time 1.349 (3.080)	Data 0.000 (1.682)	Loss_ce 1.300 / 2.088	Loss_ce_soft 1.219 / 3.287	Loss_tri_soft 0.228 / 0.177	Loss_lf 4.961 / 2.934	Loss_lf_tri 0.213 / 0.164	Prec 91.45% / 84.88%	
Epoch: [0][180/200]	Time 1.450 (3.140)	Data 0.000 (1.741)	Loss_ce 1.297 / 2.112	Loss_ce_soft 1.217 / 3.304	Loss_tri_soft 0.223 / 0.178	Loss_lf 4.958 / 2.925	Loss_lf_tri 0.211 / 0.165	Prec 91.39% / 84.62%	
Epoch: [0][200/200]	Time 1.315 (3.408)	Data 0.000 (1.792)	Loss_ce 1.288 / 2.106	Loss_ce_soft 1.199 / 3.292	Loss_tri_soft 0.213 / 0.171	Loss_lf 4.964 / 2.921	Loss_lf_tri 0.205 / 0.160	Prec 91.44% / 84.50%	
Extract Features: [50/302]	Time 0.191 (1.236)	Data 0.000 (1.057)	
Extract Features: [100/302]	Time 0.172 (0.706)	Data 0.000 (0.529)	
Extract Features: [150/302]	Time 0.193 (0.530)	Data 0.000 (0.353)	
Extract Features: [200/302]	Time 0.190 (0.441)	Data 0.000 (0.264)	
Extract Features: [250/302]	Time 0.192 (0.389)	Data 0.001 (0.212)	
Extract Features: [300/302]	Time 0.178 (0.353)	Data 0.000 (0.176)	
Mean AP: 64.5%

 * Finished phase   4 epoch   0  model no.1 mAP: 64.5%  best: 64.5% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.277154922485352
Clustering and labeling...

 Clustered into 98 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [1][20/200]	Time 1.323 (1.271)	Data 0.000 (0.000)	Loss_ce 1.336 / 2.055	Loss_ce_soft 1.144 / 3.150	Loss_tri_soft 0.227 / 0.195	Loss_lf 5.027 / 2.944	Loss_lf_tri 0.214 / 0.181	Prec 89.06% / 83.75%	
Epoch: [1][40/200]	Time 1.362 (2.396)	Data 0.000 (1.087)	Loss_ce 1.320 / 2.065	Loss_ce_soft 1.091 / 3.181	Loss_tri_soft 0.194 / 0.169	Loss_lf 5.040 / 2.945	Loss_lf_tri 0.198 / 0.162	Prec 89.53% / 84.69%	
Epoch: [1][60/200]	Time 1.324 (2.780)	Data 0.001 (1.459)	Loss_ce 1.307 / 2.077	Loss_ce_soft 1.097 / 3.182	Loss_tri_soft 0.186 / 0.149	Loss_lf 5.015 / 2.994	Loss_lf_tri 0.192 / 0.148	Prec 90.31% / 84.06%	
Epoch: [1][80/200]	Time 1.334 (2.974)	Data 0.000 (1.646)	Loss_ce 1.285 / 2.131	Loss_ce_soft 1.088 / 3.253	Loss_tri_soft 0.187 / 0.151	Loss_lf 5.019 / 3.038	Loss_lf_tri 0.195 / 0.151	Prec 90.86% / 82.97%	
Epoch: [1][100/200]	Time 1.341 (3.087)	Data 0.000 (1.754)	Loss_ce 1.292 / 2.137	Loss_ce_soft 1.100 / 3.247	Loss_tri_soft 0.175 / 0.148	Loss_lf 5.012 / 3.025	Loss_lf_tri 0.185 / 0.150	Prec 91.00% / 83.25%	
Epoch: [1][120/200]	Time 1.363 (2.797)	Data 0.000 (1.462)	Loss_ce 1.284 / 2.176	Loss_ce_soft 1.101 / 3.260	Loss_tri_soft 0.172 / 0.151	Loss_lf 5.022 / 2.988	Loss_lf_tri 0.182 / 0.152	Prec 90.94% / 82.24%	
Epoch: [1][140/200]	Time 1.318 (2.903)	Data 0.000 (1.568)	Loss_ce 1.281 / 2.197	Loss_ce_soft 1.100 / 3.288	Loss_tri_soft 0.175 / 0.151	Loss_lf 5.021 / 2.988	Loss_lf_tri 0.184 / 0.151	Prec 91.16% / 82.32%	
Epoch: [1][160/200]	Time 1.299 (2.977)	Data 0.000 (1.641)	Loss_ce 1.275 / 2.211	Loss_ce_soft 1.089 / 3.301	Loss_tri_soft 0.169 / 0.154	Loss_lf 5.008 / 2.994	Loss_lf_tri 0.180 / 0.152	Prec 91.33% / 82.23%	
Epoch: [1][180/200]	Time 1.360 (3.050)	Data 0.000 (1.699)	Loss_ce 1.270 / 2.228	Loss_ce_soft 1.080 / 3.322	Loss_tri_soft 0.177 / 0.153	Loss_lf 5.013 / 3.028	Loss_lf_tri 0.185 / 0.153	Prec 91.53% / 82.12%	
Epoch: [1][200/200]	Time 1.325 (3.311)	Data 0.000 (1.740)	Loss_ce 1.268 / 2.214	Loss_ce_soft 1.078 / 3.313	Loss_tri_soft 0.178 / 0.151	Loss_lf 5.005 / 3.014	Loss_lf_tri 0.185 / 0.152	Prec 91.56% / 82.16%	
Extract Features: [50/302]	Time 0.165 (1.056)	Data 0.001 (0.887)	
Extract Features: [100/302]	Time 0.183 (0.613)	Data 0.001 (0.444)	
Extract Features: [150/302]	Time 0.164 (0.464)	Data 0.001 (0.296)	
Extract Features: [200/302]	Time 0.186 (0.391)	Data 0.000 (0.222)	
Extract Features: [250/302]	Time 0.165 (0.346)	Data 0.000 (0.178)	
Extract Features: [300/302]	Time 0.157 (0.316)	Data 0.000 (0.148)	
Mean AP: 64.9%

 * Finished phase   4 epoch   1  model no.1 mAP: 64.9%  best: 64.9% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.05209493637085
Clustering and labeling...

 Clustered into 100 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [2][20/200]	Time 1.397 (1.346)	Data 0.000 (0.000)	Loss_ce 1.317 / 2.144	Loss_ce_soft 1.089 / 3.368	Loss_tri_soft 0.214 / 0.138	Loss_lf 5.048 / 3.136	Loss_lf_tri 0.212 / 0.142	Prec 91.25% / 85.62%	
Epoch: [2][40/200]	Time 1.316 (2.430)	Data 0.000 (1.101)	Loss_ce 1.314 / 2.138	Loss_ce_soft 1.090 / 3.265	Loss_tri_soft 0.174 / 0.146	Loss_lf 5.032 / 3.074	Loss_lf_tri 0.180 / 0.152	Prec 91.41% / 85.16%	
Epoch: [2][60/200]	Time 1.313 (2.811)	Data 0.000 (1.478)	Loss_ce 1.296 / 2.151	Loss_ce_soft 1.102 / 3.255	Loss_tri_soft 0.179 / 0.133	Loss_lf 5.018 / 3.067	Loss_lf_tri 0.186 / 0.137	Prec 91.88% / 84.06%	
Epoch: [2][80/200]	Time 1.351 (2.992)	Data 0.000 (1.653)	Loss_ce 1.294 / 2.181	Loss_ce_soft 1.105 / 3.286	Loss_tri_soft 0.175 / 0.138	Loss_lf 5.036 / 3.133	Loss_lf_tri 0.184 / 0.138	Prec 91.72% / 83.83%	
Epoch: [2][100/200]	Time 1.356 (2.662)	Data 0.000 (1.323)	Loss_ce 1.281 / 2.207	Loss_ce_soft 1.100 / 3.269	Loss_tri_soft 0.195 / 0.129	Loss_lf 5.032 / 3.084	Loss_lf_tri 0.196 / 0.134	Prec 92.19% / 83.25%	
Epoch: [2][120/200]	Time 1.349 (2.807)	Data 0.000 (1.468)	Loss_ce 1.272 / 2.225	Loss_ce_soft 1.087 / 3.262	Loss_tri_soft 0.199 / 0.130	Loss_lf 5.019 / 3.079	Loss_lf_tri 0.198 / 0.136	Prec 92.34% / 82.45%	
Epoch: [2][140/200]	Time 1.351 (2.901)	Data 0.000 (1.563)	Loss_ce 1.262 / 2.229	Loss_ce_soft 1.084 / 3.275	Loss_tri_soft 0.195 / 0.128	Loss_lf 5.012 / 3.075	Loss_lf_tri 0.195 / 0.134	Prec 92.72% / 82.46%	
Epoch: [2][160/200]	Time 1.350 (2.967)	Data 0.000 (1.631)	Loss_ce 1.260 / 2.247	Loss_ce_soft 1.093 / 3.309	Loss_tri_soft 0.190 / 0.129	Loss_lf 5.015 / 3.068	Loss_lf_tri 0.193 / 0.133	Prec 92.77% / 82.54%	
Epoch: [2][180/200]	Time 1.367 (3.033)	Data 0.000 (1.695)	Loss_ce 1.252 / 2.259	Loss_ce_soft 1.084 / 3.306	Loss_tri_soft 0.191 / 0.131	Loss_lf 5.011 / 3.057	Loss_lf_tri 0.195 / 0.134	Prec 92.99% / 82.12%	
Epoch: [2][200/200]	Time 1.228 (3.072)	Data 0.001 (1.525)	Loss_ce 1.250 / 2.239	Loss_ce_soft 1.084 / 3.306	Loss_tri_soft 0.189 / 0.133	Loss_lf 5.010 / 3.062	Loss_lf_tri 0.194 / 0.136	Prec 93.09% / 82.75%	
Extract Features: [50/302]	Time 0.164 (1.014)	Data 0.001 (0.843)	
Extract Features: [100/302]	Time 0.166 (0.591)	Data 0.000 (0.422)	
Extract Features: [150/302]	Time 0.164 (0.450)	Data 0.000 (0.281)	
Extract Features: [200/302]	Time 0.163 (0.380)	Data 0.000 (0.211)	
Extract Features: [250/302]	Time 0.165 (0.338)	Data 0.000 (0.169)	
Extract Features: [300/302]	Time 0.157 (0.309)	Data 0.000 (0.141)	
Mean AP: 64.9%

 * Finished phase   4 epoch   2  model no.1 mAP: 64.9%  best: 64.9% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.012107372283936
Clustering and labeling...

 Clustered into 100 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [3][20/200]	Time 1.321 (1.315)	Data 0.000 (0.000)	Loss_ce 1.294 / 2.113	Loss_ce_soft 1.021 / 3.225	Loss_tri_soft 0.164 / 0.180	Loss_lf 4.995 / 3.056	Loss_lf_tri 0.172 / 0.173	Prec 92.50% / 83.44%	
Epoch: [3][40/200]	Time 1.363 (2.444)	Data 0.000 (1.068)	Loss_ce 1.302 / 2.208	Loss_ce_soft 1.092 / 3.338	Loss_tri_soft 0.175 / 0.172	Loss_lf 5.006 / 3.108	Loss_lf_tri 0.196 / 0.171	Prec 91.72% / 81.09%	
Epoch: [3][60/200]	Time 1.245 (2.776)	Data 0.000 (1.421)	Loss_ce 1.274 / 2.209	Loss_ce_soft 1.068 / 3.308	Loss_tri_soft 0.175 / 0.165	Loss_lf 5.034 / 3.070	Loss_lf_tri 0.193 / 0.168	Prec 91.98% / 82.29%	
Epoch: [3][80/200]	Time 1.384 (2.974)	Data 0.000 (1.629)	Loss_ce 1.272 / 2.221	Loss_ce_soft 1.082 / 3.309	Loss_tri_soft 0.167 / 0.166	Loss_lf 5.032 / 3.088	Loss_lf_tri 0.193 / 0.169	Prec 91.64% / 82.73%	
Epoch: [3][100/200]	Time 1.412 (2.648)	Data 0.000 (1.303)	Loss_ce 1.271 / 2.248	Loss_ce_soft 1.078 / 3.337	Loss_tri_soft 0.179 / 0.169	Loss_lf 5.034 / 3.124	Loss_lf_tri 0.199 / 0.170	Prec 91.62% / 82.25%	
Epoch: [3][120/200]	Time 1.347 (2.795)	Data 0.000 (1.450)	Loss_ce 1.274 / 2.247	Loss_ce_soft 1.084 / 3.330	Loss_tri_soft 0.185 / 0.162	Loss_lf 5.046 / 3.104	Loss_lf_tri 0.204 / 0.162	Prec 91.56% / 82.24%	
Epoch: [3][140/200]	Time 1.350 (2.898)	Data 0.000 (1.556)	Loss_ce 1.263 / 2.261	Loss_ce_soft 1.070 / 3.330	Loss_tri_soft 0.179 / 0.154	Loss_lf 5.019 / 3.099	Loss_lf_tri 0.199 / 0.157	Prec 91.79% / 82.10%	
Epoch: [3][160/200]	Time 1.335 (2.962)	Data 0.001 (1.625)	Loss_ce 1.257 / 2.256	Loss_ce_soft 1.071 / 3.312	Loss_tri_soft 0.174 / 0.150	Loss_lf 5.021 / 3.069	Loss_lf_tri 0.196 / 0.153	Prec 92.11% / 82.07%	
Epoch: [3][180/200]	Time 1.305 (3.030)	Data 0.000 (1.691)	Loss_ce 1.251 / 2.272	Loss_ce_soft 1.058 / 3.332	Loss_tri_soft 0.176 / 0.151	Loss_lf 5.023 / 3.061	Loss_lf_tri 0.197 / 0.154	Prec 92.29% / 82.01%	
Epoch: [3][200/200]	Time 1.315 (3.077)	Data 0.000 (1.522)	Loss_ce 1.253 / 2.248	Loss_ce_soft 1.069 / 3.317	Loss_tri_soft 0.176 / 0.155	Loss_lf 5.027 / 3.056	Loss_lf_tri 0.199 / 0.157	Prec 92.41% / 82.34%	
Extract Features: [50/302]	Time 0.166 (1.076)	Data 0.000 (0.905)	
Extract Features: [100/302]	Time 0.164 (0.621)	Data 0.001 (0.452)	
Extract Features: [150/302]	Time 0.171 (0.470)	Data 0.001 (0.302)	
Extract Features: [200/302]	Time 0.168 (0.395)	Data 0.000 (0.226)	
Extract Features: [250/302]	Time 0.166 (0.349)	Data 0.000 (0.181)	
Extract Features: [300/302]	Time 0.157 (0.319)	Data 0.000 (0.151)	
Mean AP: 64.9%

 * Finished phase   4 epoch   3  model no.1 mAP: 64.9%  best: 64.9%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.039096355438232
Clustering and labeling...

 Clustered into 97 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [4][20/200]	Time 1.375 (1.333)	Data 0.000 (0.000)	Loss_ce 1.305 / 2.188	Loss_ce_soft 1.042 / 3.380	Loss_tri_soft 0.149 / 0.160	Loss_lf 5.006 / 3.043	Loss_lf_tri 0.186 / 0.161	Prec 91.25% / 84.06%	
Epoch: [4][40/200]	Time 1.358 (2.361)	Data 0.000 (1.045)	Loss_ce 1.288 / 2.220	Loss_ce_soft 1.028 / 3.296	Loss_tri_soft 0.152 / 0.145	Loss_lf 5.014 / 3.077	Loss_lf_tri 0.189 / 0.155	Prec 91.25% / 81.72%	
Epoch: [4][60/200]	Time 1.215 (2.710)	Data 0.000 (1.400)	Loss_ce 1.277 / 2.250	Loss_ce_soft 1.016 / 3.319	Loss_tri_soft 0.161 / 0.149	Loss_lf 5.032 / 3.133	Loss_lf_tri 0.190 / 0.157	Prec 91.67% / 81.04%	
Epoch: [4][80/200]	Time 1.338 (2.881)	Data 0.001 (1.575)	Loss_ce 1.281 / 2.263	Loss_ce_soft 1.049 / 3.335	Loss_tri_soft 0.167 / 0.147	Loss_lf 5.030 / 3.144	Loss_lf_tri 0.190 / 0.158	Prec 91.48% / 81.48%	
Epoch: [4][100/200]	Time 1.307 (2.992)	Data 0.000 (1.682)	Loss_ce 1.300 / 2.268	Loss_ce_soft 1.083 / 3.315	Loss_tri_soft 0.168 / 0.152	Loss_lf 5.031 / 3.108	Loss_lf_tri 0.191 / 0.160	Prec 91.00% / 81.44%	
Epoch: [4][120/200]	Time 1.321 (2.728)	Data 0.000 (1.402)	Loss_ce 1.289 / 2.275	Loss_ce_soft 1.067 / 3.333	Loss_tri_soft 0.168 / 0.151	Loss_lf 5.029 / 3.106	Loss_lf_tri 0.193 / 0.159	Prec 91.20% / 81.56%	
Epoch: [4][140/200]	Time 1.300 (2.822)	Data 0.000 (1.501)	Loss_ce 1.279 / 2.301	Loss_ce_soft 1.059 / 3.356	Loss_tri_soft 0.161 / 0.145	Loss_lf 5.025 / 3.142	Loss_lf_tri 0.189 / 0.155	Prec 91.43% / 80.94%	
Epoch: [4][160/200]	Time 1.294 (2.899)	Data 0.000 (1.577)	Loss_ce 1.283 / 2.306	Loss_ce_soft 1.056 / 3.346	Loss_tri_soft 0.161 / 0.145	Loss_lf 5.038 / 3.131	Loss_lf_tri 0.190 / 0.156	Prec 91.25% / 81.13%	
Epoch: [4][180/200]	Time 1.313 (2.955)	Data 0.000 (1.636)	Loss_ce 1.269 / 2.327	Loss_ce_soft 1.043 / 3.366	Loss_tri_soft 0.161 / 0.143	Loss_lf 5.035 / 3.133	Loss_lf_tri 0.193 / 0.154	Prec 91.70% / 80.52%	
Epoch: [4][200/200]	Time 1.238 (3.215)	Data 0.001 (1.685)	Loss_ce 1.265 / 2.306	Loss_ce_soft 1.041 / 3.357	Loss_tri_soft 0.158 / 0.144	Loss_lf 5.037 / 3.106	Loss_lf_tri 0.190 / 0.154	Prec 91.66% / 81.00%	
Extract Features: [50/302]	Time 0.187 (1.045)	Data 0.001 (0.870)	
Extract Features: [100/302]	Time 0.169 (0.608)	Data 0.000 (0.435)	
Extract Features: [150/302]	Time 0.167 (0.463)	Data 0.000 (0.290)	
Extract Features: [200/302]	Time 0.166 (0.391)	Data 0.000 (0.218)	
Extract Features: [250/302]	Time 0.166 (0.347)	Data 0.000 (0.174)	
Extract Features: [300/302]	Time 0.163 (0.317)	Data 0.000 (0.145)	
Mean AP: 64.8%

 * Finished phase   4 epoch   4  model no.1 mAP: 64.8%  best: 64.9%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.265026807785034
Clustering and labeling...

 Clustered into 100 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [5][20/200]	Time 1.227 (1.291)	Data 0.000 (0.000)	Loss_ce 1.274 / 2.202	Loss_ce_soft 1.011 / 3.383	Loss_tri_soft 0.192 / 0.141	Loss_lf 5.097 / 3.110	Loss_lf_tri 0.206 / 0.145	Prec 91.88% / 82.50%	
Epoch: [5][40/200]	Time 1.309 (2.360)	Data 0.000 (1.076)	Loss_ce 1.252 / 2.151	Loss_ce_soft 1.034 / 3.202	Loss_tri_soft 0.184 / 0.136	Loss_lf 5.053 / 3.089	Loss_lf_tri 0.208 / 0.141	Prec 92.66% / 83.12%	
Epoch: [5][60/200]	Time 1.337 (2.718)	Data 0.000 (1.427)	Loss_ce 1.269 / 2.194	Loss_ce_soft 1.034 / 3.219	Loss_tri_soft 0.186 / 0.133	Loss_lf 5.027 / 3.024	Loss_lf_tri 0.212 / 0.141	Prec 92.29% / 83.12%	
Epoch: [5][80/200]	Time 1.307 (2.907)	Data 0.001 (1.608)	Loss_ce 1.279 / 2.229	Loss_ce_soft 1.058 / 3.270	Loss_tri_soft 0.179 / 0.144	Loss_lf 5.026 / 3.016	Loss_lf_tri 0.206 / 0.149	Prec 92.11% / 82.27%	
Epoch: [5][100/200]	Time 1.318 (2.585)	Data 0.000 (1.286)	Loss_ce 1.272 / 2.257	Loss_ce_soft 1.061 / 3.294	Loss_tri_soft 0.171 / 0.145	Loss_lf 5.033 / 3.059	Loss_lf_tri 0.201 / 0.150	Prec 92.06% / 81.94%	
Epoch: [5][120/200]	Time 1.262 (2.728)	Data 0.001 (1.433)	Loss_ce 1.270 / 2.275	Loss_ce_soft 1.066 / 3.292	Loss_tri_soft 0.178 / 0.146	Loss_lf 5.030 / 3.101	Loss_lf_tri 0.204 / 0.153	Prec 92.29% / 81.41%	
Epoch: [5][140/200]	Time 1.321 (2.823)	Data 0.000 (1.531)	Loss_ce 1.263 / 2.294	Loss_ce_soft 1.060 / 3.300	Loss_tri_soft 0.165 / 0.145	Loss_lf 5.021 / 3.073	Loss_lf_tri 0.195 / 0.154	Prec 92.59% / 80.98%	
Epoch: [5][160/200]	Time 1.319 (2.903)	Data 0.000 (1.607)	Loss_ce 1.260 / 2.312	Loss_ce_soft 1.063 / 3.323	Loss_tri_soft 0.168 / 0.144	Loss_lf 5.027 / 3.091	Loss_lf_tri 0.198 / 0.154	Prec 92.54% / 80.74%	
Epoch: [5][180/200]	Time 1.247 (2.970)	Data 0.000 (1.660)	Loss_ce 1.256 / 2.317	Loss_ce_soft 1.057 / 3.314	Loss_tri_soft 0.166 / 0.148	Loss_lf 5.020 / 3.094	Loss_lf_tri 0.196 / 0.155	Prec 92.53% / 80.66%	
Epoch: [5][200/200]	Time 1.229 (3.013)	Data 0.000 (1.494)	Loss_ce 1.246 / 2.311	Loss_ce_soft 1.053 / 3.328	Loss_tri_soft 0.168 / 0.152	Loss_lf 5.029 / 3.093	Loss_lf_tri 0.198 / 0.158	Prec 92.81% / 80.94%	
Extract Features: [50/302]	Time 0.166 (1.028)	Data 0.000 (0.854)	
Extract Features: [100/302]	Time 0.169 (0.600)	Data 0.000 (0.427)	
Extract Features: [150/302]	Time 0.170 (0.457)	Data 0.001 (0.285)	
Extract Features: [200/302]	Time 0.170 (0.386)	Data 0.001 (0.214)	
Extract Features: [250/302]	Time 0.166 (0.344)	Data 0.000 (0.171)	
Extract Features: [300/302]	Time 0.180 (0.316)	Data 0.000 (0.143)	
Mean AP: 64.8%

 * Finished phase   4 epoch   5  model no.1 mAP: 64.8%  best: 64.9%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.106076717376709
Clustering and labeling...

 Clustered into 105 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [6][20/200]	Time 1.300 (1.253)	Data 0.001 (0.000)	Loss_ce 1.310 / 2.150	Loss_ce_soft 1.091 / 3.262	Loss_tri_soft 0.136 / 0.173	Loss_lf 5.023 / 3.122	Loss_lf_tri 0.165 / 0.175	Prec 90.62% / 82.50%	
Epoch: [6][40/200]	Time 1.265 (2.312)	Data 0.000 (1.050)	Loss_ce 1.376 / 2.187	Loss_ce_soft 1.215 / 3.218	Loss_tri_soft 0.180 / 0.154	Loss_lf 5.025 / 3.062	Loss_lf_tri 0.205 / 0.166	Prec 89.53% / 81.09%	
Epoch: [6][60/200]	Time 1.261 (2.680)	Data 0.000 (1.417)	Loss_ce 1.343 / 2.253	Loss_ce_soft 1.196 / 3.311	Loss_tri_soft 0.172 / 0.142	Loss_lf 5.041 / 3.090	Loss_lf_tri 0.198 / 0.157	Prec 90.83% / 80.62%	
Epoch: [6][80/200]	Time 1.328 (2.863)	Data 0.001 (1.597)	Loss_ce 1.342 / 2.263	Loss_ce_soft 1.204 / 3.300	Loss_tri_soft 0.166 / 0.147	Loss_lf 5.026 / 3.111	Loss_lf_tri 0.195 / 0.163	Prec 90.39% / 81.09%	
Epoch: [6][100/200]	Time 1.325 (2.555)	Data 0.000 (1.277)	Loss_ce 1.335 / 2.267	Loss_ce_soft 1.203 / 3.287	Loss_tri_soft 0.161 / 0.149	Loss_lf 5.028 / 3.107	Loss_lf_tri 0.190 / 0.166	Prec 90.81% / 81.69%	
Epoch: [6][120/200]	Time 1.343 (2.699)	Data 0.000 (1.420)	Loss_ce 1.322 / 2.285	Loss_ce_soft 1.186 / 3.302	Loss_tri_soft 0.162 / 0.142	Loss_lf 5.026 / 3.100	Loss_lf_tri 0.190 / 0.160	Prec 91.30% / 81.56%	
Epoch: [6][140/200]	Time 1.238 (2.805)	Data 0.000 (1.527)	Loss_ce 1.311 / 2.314	Loss_ce_soft 1.167 / 3.321	Loss_tri_soft 0.157 / 0.141	Loss_lf 5.019 / 3.114	Loss_lf_tri 0.188 / 0.159	Prec 91.38% / 81.07%	
Epoch: [6][160/200]	Time 1.236 (2.880)	Data 0.000 (1.602)	Loss_ce 1.307 / 2.319	Loss_ce_soft 1.164 / 3.317	Loss_tri_soft 0.155 / 0.143	Loss_lf 5.011 / 3.109	Loss_lf_tri 0.185 / 0.160	Prec 91.60% / 81.21%	
Epoch: [6][180/200]	Time 1.349 (2.704)	Data 0.000 (1.424)	Loss_ce 1.309 / 2.337	Loss_ce_soft 1.169 / 3.338	Loss_tri_soft 0.160 / 0.145	Loss_lf 5.019 / 3.131	Loss_lf_tri 0.191 / 0.162	Prec 91.49% / 81.04%	
Epoch: [6][200/200]	Time 1.285 (2.983)	Data 0.000 (1.495)	Loss_ce 1.301 / 2.328	Loss_ce_soft 1.162 / 3.338	Loss_tri_soft 0.157 / 0.151	Loss_lf 5.014 / 3.141	Loss_lf_tri 0.188 / 0.166	Prec 91.62% / 81.22%	
Extract Features: [50/302]	Time 0.171 (1.033)	Data 0.000 (0.860)	
Extract Features: [100/302]	Time 0.167 (0.602)	Data 0.000 (0.430)	
Extract Features: [150/302]	Time 0.169 (0.459)	Data 0.000 (0.287)	
Extract Features: [200/302]	Time 0.168 (0.387)	Data 0.000 (0.215)	
Extract Features: [250/302]	Time 0.188 (0.344)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.185 (0.315)	Data 0.000 (0.144)	
Mean AP: 65.3%

 * Finished phase   4 epoch   6  model no.1 mAP: 65.3%  best: 65.3% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.014106512069702
Clustering and labeling...

 Clustered into 101 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [7][20/200]	Time 1.274 (1.403)	Data 0.000 (0.000)	Loss_ce 1.243 / 2.151	Loss_ce_soft 1.072 / 3.217	Loss_tri_soft 0.156 / 0.144	Loss_lf 5.017 / 2.862	Loss_lf_tri 0.186 / 0.157	Prec 94.06% / 81.88%	
Epoch: [7][40/200]	Time 1.310 (2.406)	Data 0.000 (1.068)	Loss_ce 1.259 / 2.162	Loss_ce_soft 1.088 / 3.199	Loss_tri_soft 0.161 / 0.104	Loss_lf 5.047 / 2.966	Loss_lf_tri 0.190 / 0.126	Prec 93.44% / 82.19%	
Epoch: [7][60/200]	Time 1.262 (2.733)	Data 0.000 (1.417)	Loss_ce 1.260 / 2.190	Loss_ce_soft 1.077 / 3.245	Loss_tri_soft 0.177 / 0.125	Loss_lf 5.041 / 3.050	Loss_lf_tri 0.204 / 0.145	Prec 92.92% / 82.60%	
Epoch: [7][80/200]	Time 1.287 (2.905)	Data 0.000 (1.600)	Loss_ce 1.257 / 2.200	Loss_ce_soft 1.079 / 3.247	Loss_tri_soft 0.167 / 0.126	Loss_lf 5.022 / 3.020	Loss_lf_tri 0.199 / 0.143	Prec 93.36% / 83.20%	
Epoch: [7][100/200]	Time 1.348 (2.576)	Data 0.000 (1.280)	Loss_ce 1.264 / 2.227	Loss_ce_soft 1.090 / 3.256	Loss_tri_soft 0.167 / 0.130	Loss_lf 5.025 / 3.034	Loss_lf_tri 0.197 / 0.146	Prec 93.00% / 82.94%	
Epoch: [7][120/200]	Time 1.253 (2.718)	Data 0.000 (1.425)	Loss_ce 1.265 / 2.250	Loss_ce_soft 1.093 / 3.259	Loss_tri_soft 0.170 / 0.130	Loss_lf 5.033 / 3.040	Loss_lf_tri 0.201 / 0.145	Prec 92.97% / 82.19%	
Epoch: [7][140/200]	Time 1.307 (2.815)	Data 0.000 (1.524)	Loss_ce 1.254 / 2.263	Loss_ce_soft 1.087 / 3.273	Loss_tri_soft 0.162 / 0.127	Loss_lf 5.028 / 3.078	Loss_lf_tri 0.194 / 0.144	Prec 93.44% / 82.14%	
Epoch: [7][160/200]	Time 1.337 (2.885)	Data 0.000 (1.600)	Loss_ce 1.253 / 2.278	Loss_ce_soft 1.077 / 3.283	Loss_tri_soft 0.158 / 0.130	Loss_lf 5.024 / 3.099	Loss_lf_tri 0.194 / 0.147	Prec 93.40% / 82.11%	
Epoch: [7][180/200]	Time 1.257 (2.949)	Data 0.001 (1.661)	Loss_ce 1.246 / 2.298	Loss_ce_soft 1.074 / 3.284	Loss_tri_soft 0.159 / 0.128	Loss_lf 5.018 / 3.107	Loss_lf_tri 0.195 / 0.145	Prec 93.47% / 81.94%	
Epoch: [7][200/200]	Time 1.251 (2.993)	Data 0.000 (1.495)	Loss_ce 1.253 / 2.283	Loss_ce_soft 1.084 / 3.281	Loss_tri_soft 0.167 / 0.128	Loss_lf 5.022 / 3.089	Loss_lf_tri 0.202 / 0.144	Prec 93.22% / 82.31%	
Extract Features: [50/302]	Time 0.166 (1.020)	Data 0.000 (0.848)	
Extract Features: [100/302]	Time 0.166 (0.596)	Data 0.000 (0.424)	
Extract Features: [150/302]	Time 0.166 (0.454)	Data 0.001 (0.283)	
Extract Features: [200/302]	Time 0.167 (0.384)	Data 0.000 (0.212)	
Extract Features: [250/302]	Time 0.166 (0.341)	Data 0.001 (0.170)	
Extract Features: [300/302]	Time 0.163 (0.313)	Data 0.000 (0.142)	
Mean AP: 65.4%

 * Finished phase   4 epoch   7  model no.1 mAP: 65.4%  best: 65.4% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.0151047706604
Clustering and labeling...

 Clustered into 102 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [8][20/200]	Time 1.275 (1.272)	Data 0.000 (0.000)	Loss_ce 1.296 / 2.053	Loss_ce_soft 1.037 / 2.954	Loss_tri_soft 0.137 / 0.113	Loss_lf 4.969 / 2.871	Loss_lf_tri 0.167 / 0.118	Prec 90.62% / 82.81%	
Epoch: [8][40/200]	Time 1.229 (2.329)	Data 0.000 (1.059)	Loss_ce 1.305 / 2.201	Loss_ce_soft 1.108 / 3.150	Loss_tri_soft 0.167 / 0.149	Loss_lf 5.044 / 3.081	Loss_lf_tri 0.198 / 0.153	Prec 91.56% / 82.03%	
Epoch: [8][60/200]	Time 1.306 (2.678)	Data 0.000 (1.405)	Loss_ce 1.286 / 2.226	Loss_ce_soft 1.076 / 3.175	Loss_tri_soft 0.136 / 0.131	Loss_lf 5.039 / 3.039	Loss_lf_tri 0.171 / 0.144	Prec 92.71% / 80.31%	
Epoch: [8][80/200]	Time 1.293 (2.856)	Data 0.000 (1.582)	Loss_ce 1.268 / 2.268	Loss_ce_soft 1.068 / 3.237	Loss_tri_soft 0.127 / 0.130	Loss_lf 5.022 / 3.102	Loss_lf_tri 0.167 / 0.146	Prec 93.12% / 80.31%	
Epoch: [8][100/200]	Time 1.255 (2.538)	Data 0.000 (1.266)	Loss_ce 1.262 / 2.281	Loss_ce_soft 1.062 / 3.248	Loss_tri_soft 0.133 / 0.135	Loss_lf 5.029 / 3.140	Loss_lf_tri 0.168 / 0.151	Prec 93.38% / 80.56%	
Epoch: [8][120/200]	Time 1.313 (2.723)	Data 0.000 (1.409)	Loss_ce 1.261 / 2.293	Loss_ce_soft 1.059 / 3.235	Loss_tri_soft 0.136 / 0.133	Loss_lf 5.020 / 3.119	Loss_lf_tri 0.171 / 0.151	Prec 93.23% / 80.00%	
Epoch: [8][140/200]	Time 1.292 (2.816)	Data 0.000 (1.506)	Loss_ce 1.249 / 2.306	Loss_ce_soft 1.048 / 3.255	Loss_tri_soft 0.134 / 0.131	Loss_lf 5.003 / 3.109	Loss_lf_tri 0.170 / 0.149	Prec 93.53% / 79.87%	
Epoch: [8][160/200]	Time 1.236 (2.887)	Data 0.000 (1.579)	Loss_ce 1.247 / 2.330	Loss_ce_soft 1.052 / 3.276	Loss_tri_soft 0.142 / 0.132	Loss_lf 5.002 / 3.120	Loss_lf_tri 0.177 / 0.150	Prec 93.48% / 79.10%	
Epoch: [8][180/200]	Time 1.298 (2.945)	Data 0.000 (1.640)	Loss_ce 1.237 / 2.359	Loss_ce_soft 1.038 / 3.313	Loss_tri_soft 0.138 / 0.139	Loss_lf 4.997 / 3.149	Loss_lf_tri 0.174 / 0.159	Prec 93.78% / 78.78%	
Epoch: [8][200/200]	Time 1.305 (2.987)	Data 0.001 (1.476)	Loss_ce 1.239 / 2.341	Loss_ce_soft 1.042 / 3.306	Loss_tri_soft 0.140 / 0.140	Loss_lf 5.000 / 3.128	Loss_lf_tri 0.176 / 0.159	Prec 93.59% / 79.28%	
Extract Features: [50/302]	Time 0.170 (1.027)	Data 0.001 (0.853)	
Extract Features: [100/302]	Time 0.166 (0.600)	Data 0.000 (0.427)	
Extract Features: [150/302]	Time 0.167 (0.458)	Data 0.000 (0.285)	
Extract Features: [200/302]	Time 0.168 (0.386)	Data 0.000 (0.213)	
Extract Features: [250/302]	Time 0.166 (0.343)	Data 0.000 (0.171)	
Extract Features: [300/302]	Time 0.180 (0.314)	Data 0.000 (0.142)	
Mean AP: 65.4%

 * Finished phase   4 epoch   8  model no.1 mAP: 65.4%  best: 65.4% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.057090520858765
Clustering and labeling...

 Clustered into 104 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [9][20/200]	Time 1.346 (1.321)	Data 0.000 (0.000)	Loss_ce 1.271 / 2.206	Loss_ce_soft 1.031 / 3.205	Loss_tri_soft 0.081 / 0.104	Loss_lf 5.052 / 2.852	Loss_lf_tri 0.130 / 0.130	Prec 91.88% / 83.44%	
Epoch: [9][40/200]	Time 1.317 (2.362)	Data 0.000 (1.049)	Loss_ce 1.260 / 2.216	Loss_ce_soft 1.013 / 3.189	Loss_tri_soft 0.118 / 0.123	Loss_lf 5.033 / 2.910	Loss_lf_tri 0.165 / 0.138	Prec 92.19% / 82.81%	
Epoch: [9][60/200]	Time 1.348 (2.725)	Data 0.000 (1.411)	Loss_ce 1.271 / 2.223	Loss_ce_soft 1.061 / 3.178	Loss_tri_soft 0.125 / 0.133	Loss_lf 5.046 / 2.981	Loss_lf_tri 0.174 / 0.147	Prec 92.08% / 81.98%	
Epoch: [9][80/200]	Time 1.251 (2.905)	Data 0.000 (1.589)	Loss_ce 1.291 / 2.241	Loss_ce_soft 1.081 / 3.171	Loss_tri_soft 0.136 / 0.123	Loss_lf 5.040 / 2.979	Loss_lf_tri 0.183 / 0.140	Prec 91.09% / 81.95%	
Epoch: [9][100/200]	Time 1.321 (2.589)	Data 0.000 (1.271)	Loss_ce 1.297 / 2.230	Loss_ce_soft 1.104 / 3.171	Loss_tri_soft 0.142 / 0.118	Loss_lf 5.059 / 3.008	Loss_lf_tri 0.194 / 0.137	Prec 91.12% / 81.56%	
Epoch: [9][120/200]	Time 1.286 (2.727)	Data 0.000 (1.415)	Loss_ce 1.293 / 2.267	Loss_ce_soft 1.093 / 3.213	Loss_tri_soft 0.150 / 0.120	Loss_lf 5.059 / 3.055	Loss_lf_tri 0.200 / 0.138	Prec 91.09% / 81.30%	
Epoch: [9][140/200]	Time 1.338 (2.829)	Data 0.000 (1.515)	Loss_ce 1.291 / 2.286	Loss_ce_soft 1.096 / 3.221	Loss_tri_soft 0.141 / 0.121	Loss_lf 5.061 / 3.064	Loss_lf_tri 0.191 / 0.142	Prec 91.21% / 81.12%	
Epoch: [9][160/200]	Time 1.374 (2.904)	Data 0.000 (1.590)	Loss_ce 1.286 / 2.307	Loss_ce_soft 1.089 / 3.236	Loss_tri_soft 0.139 / 0.118	Loss_lf 5.047 / 3.055	Loss_lf_tri 0.187 / 0.138	Prec 91.33% / 80.39%	
Epoch: [9][180/200]	Time 1.295 (2.737)	Data 0.000 (1.414)	Loss_ce 1.282 / 2.307	Loss_ce_soft 1.080 / 3.233	Loss_tri_soft 0.142 / 0.118	Loss_lf 5.048 / 3.092	Loss_lf_tri 0.188 / 0.139	Prec 91.49% / 80.28%	
Epoch: [9][200/200]	Time 1.228 (3.022)	Data 0.000 (1.487)	Loss_ce 1.283 / 2.295	Loss_ce_soft 1.087 / 3.241	Loss_tri_soft 0.140 / 0.120	Loss_lf 5.046 / 3.096	Loss_lf_tri 0.186 / 0.140	Prec 91.53% / 80.59%	
Extract Features: [50/302]	Time 0.169 (1.038)	Data 0.000 (0.864)	
Extract Features: [100/302]	Time 0.165 (0.604)	Data 0.000 (0.432)	
Extract Features: [150/302]	Time 0.166 (0.460)	Data 0.000 (0.288)	
Extract Features: [200/302]	Time 0.172 (0.388)	Data 0.001 (0.216)	
Extract Features: [250/302]	Time 0.188 (0.345)	Data 0.000 (0.173)	
Extract Features: [300/302]	Time 0.180 (0.316)	Data 0.000 (0.144)	
Mean AP: 65.2%

 * Finished phase   4 epoch   9  model no.1 mAP: 65.2%  best: 65.4%

Computing original distance...
Computing Jaccard distance...
Time cost: 8.986116170883179
Clustering and labeling...

 Clustered into 101 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [10][20/200]	Time 1.314 (1.292)	Data 0.001 (0.000)	Loss_ce 1.264 / 2.169	Loss_ce_soft 1.037 / 3.209	Loss_tri_soft 0.157 / 0.106	Loss_lf 5.089 / 2.957	Loss_lf_tri 0.203 / 0.132	Prec 94.69% / 82.81%	
Epoch: [10][40/200]	Time 1.318 (2.376)	Data 0.000 (1.070)	Loss_ce 1.253 / 2.174	Loss_ce_soft 1.046 / 3.165	Loss_tri_soft 0.152 / 0.155	Loss_lf 5.075 / 2.983	Loss_lf_tri 0.206 / 0.164	Prec 93.91% / 82.19%	
Epoch: [10][60/200]	Time 1.485 (2.739)	Data 0.001 (1.433)	Loss_ce 1.294 / 2.182	Loss_ce_soft 1.095 / 3.159	Loss_tri_soft 0.166 / 0.143	Loss_lf 5.055 / 3.004	Loss_lf_tri 0.214 / 0.155	Prec 91.88% / 82.19%	
Epoch: [10][80/200]	Time 1.300 (2.905)	Data 0.000 (1.597)	Loss_ce 1.286 / 2.202	Loss_ce_soft 1.098 / 3.156	Loss_tri_soft 0.161 / 0.132	Loss_lf 5.060 / 3.019	Loss_lf_tri 0.208 / 0.148	Prec 92.19% / 81.02%	
Epoch: [10][100/200]	Time 1.314 (2.577)	Data 0.000 (1.278)	Loss_ce 1.272 / 2.205	Loss_ce_soft 1.081 / 3.144	Loss_tri_soft 0.155 / 0.124	Loss_lf 5.048 / 3.005	Loss_lf_tri 0.200 / 0.143	Prec 92.56% / 81.62%	
Epoch: [10][120/200]	Time 1.292 (2.714)	Data 0.000 (1.416)	Loss_ce 1.264 / 2.229	Loss_ce_soft 1.074 / 3.158	Loss_tri_soft 0.149 / 0.126	Loss_lf 5.044 / 3.034	Loss_lf_tri 0.195 / 0.148	Prec 92.60% / 80.94%	
Epoch: [10][140/200]	Time 1.279 (2.810)	Data 0.000 (1.513)	Loss_ce 1.258 / 2.257	Loss_ce_soft 1.072 / 3.162	Loss_tri_soft 0.151 / 0.128	Loss_lf 5.041 / 3.039	Loss_lf_tri 0.198 / 0.147	Prec 92.86% / 80.22%	
Epoch: [10][160/200]	Time 1.294 (2.882)	Data 0.000 (1.587)	Loss_ce 1.252 / 2.274	Loss_ce_soft 1.067 / 3.187	Loss_tri_soft 0.149 / 0.128	Loss_lf 5.044 / 3.084	Loss_lf_tri 0.194 / 0.148	Prec 92.89% / 80.00%	
Epoch: [10][180/200]	Time 1.317 (2.940)	Data 0.001 (1.646)	Loss_ce 1.243 / 2.297	Loss_ce_soft 1.053 / 3.222	Loss_tri_soft 0.145 / 0.131	Loss_lf 5.040 / 3.098	Loss_lf_tri 0.193 / 0.150	Prec 93.02% / 79.86%	
Epoch: [10][200/200]	Time 1.295 (2.988)	Data 0.000 (1.481)	Loss_ce 1.241 / 2.279	Loss_ce_soft 1.047 / 3.212	Loss_tri_soft 0.142 / 0.131	Loss_lf 5.039 / 3.084	Loss_lf_tri 0.192 / 0.150	Prec 92.88% / 80.34%	
Extract Features: [50/302]	Time 0.174 (1.036)	Data 0.001 (0.861)	
Extract Features: [100/302]	Time 0.165 (0.603)	Data 0.000 (0.431)	
Extract Features: [150/302]	Time 0.166 (0.458)	Data 0.000 (0.287)	
Extract Features: [200/302]	Time 0.166 (0.386)	Data 0.000 (0.216)	
Extract Features: [250/302]	Time 0.185 (0.342)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.179 (0.314)	Data 0.000 (0.144)	
Mean AP: 65.5%

 * Finished phase   4 epoch  10  model no.1 mAP: 65.5%  best: 65.5% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.182052612304688
Clustering and labeling...

 Clustered into 107 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [11][20/200]	Time 1.285 (1.279)	Data 0.000 (0.000)	Loss_ce 1.300 / 2.165	Loss_ce_soft 1.023 / 3.187	Loss_tri_soft 0.150 / 0.093	Loss_lf 5.010 / 2.845	Loss_lf_tri 0.175 / 0.116	Prec 89.06% / 84.06%	
Epoch: [11][40/200]	Time 1.252 (2.392)	Data 0.000 (1.060)	Loss_ce 1.325 / 2.190	Loss_ce_soft 1.092 / 3.116	Loss_tri_soft 0.173 / 0.120	Loss_lf 5.050 / 3.031	Loss_lf_tri 0.214 / 0.144	Prec 89.06% / 82.19%	
Epoch: [11][60/200]	Time 1.234 (2.722)	Data 0.000 (1.411)	Loss_ce 1.312 / 2.224	Loss_ce_soft 1.095 / 3.132	Loss_tri_soft 0.151 / 0.139	Loss_lf 5.062 / 2.994	Loss_lf_tri 0.196 / 0.165	Prec 89.90% / 81.15%	
Epoch: [11][80/200]	Time 1.180 (2.899)	Data 0.000 (1.595)	Loss_ce 1.296 / 2.231	Loss_ce_soft 1.064 / 3.136	Loss_tri_soft 0.141 / 0.149	Loss_lf 5.047 / 3.064	Loss_lf_tri 0.187 / 0.169	Prec 90.16% / 81.41%	
Epoch: [11][100/200]	Time 1.322 (2.578)	Data 0.000 (1.276)	Loss_ce 1.286 / 2.258	Loss_ce_soft 1.058 / 3.177	Loss_tri_soft 0.138 / 0.152	Loss_lf 5.047 / 3.043	Loss_lf_tri 0.182 / 0.169	Prec 90.75% / 81.25%	
Epoch: [11][120/200]	Time 1.267 (2.715)	Data 0.000 (1.415)	Loss_ce 1.289 / 2.267	Loss_ce_soft 1.051 / 3.172	Loss_tri_soft 0.136 / 0.147	Loss_lf 5.042 / 3.086	Loss_lf_tri 0.181 / 0.166	Prec 90.78% / 80.68%	
Epoch: [11][140/200]	Time 1.298 (2.810)	Data 0.001 (1.514)	Loss_ce 1.289 / 2.300	Loss_ce_soft 1.046 / 3.188	Loss_tri_soft 0.133 / 0.138	Loss_lf 5.030 / 3.080	Loss_lf_tri 0.181 / 0.157	Prec 90.58% / 80.18%	
Epoch: [11][160/200]	Time 1.265 (2.889)	Data 0.001 (1.592)	Loss_ce 1.281 / 2.303	Loss_ce_soft 1.047 / 3.197	Loss_tri_soft 0.140 / 0.134	Loss_lf 5.030 / 3.090	Loss_lf_tri 0.186 / 0.156	Prec 90.90% / 80.35%	
Epoch: [11][180/200]	Time 1.341 (2.709)	Data 0.000 (1.415)	Loss_ce 1.290 / 2.308	Loss_ce_soft 1.067 / 3.198	Loss_tri_soft 0.149 / 0.134	Loss_lf 5.038 / 3.110	Loss_lf_tri 0.197 / 0.155	Prec 91.01% / 80.03%	
Epoch: [11][200/200]	Time 1.260 (2.992)	Data 0.000 (1.491)	Loss_ce 1.288 / 2.295	Loss_ce_soft 1.074 / 3.209	Loss_tri_soft 0.148 / 0.133	Loss_lf 5.034 / 3.115	Loss_lf_tri 0.196 / 0.155	Prec 91.09% / 80.66%	
Extract Features: [50/302]	Time 0.184 (1.038)	Data 0.000 (0.849)	
Extract Features: [100/302]	Time 0.181 (0.612)	Data 0.000 (0.425)	
Extract Features: [150/302]	Time 0.175 (0.469)	Data 0.000 (0.283)	
Extract Features: [200/302]	Time 0.194 (0.398)	Data 0.000 (0.212)	
Extract Features: [250/302]	Time 0.172 (0.355)	Data 0.000 (0.170)	
Extract Features: [300/302]	Time 0.181 (0.325)	Data 0.000 (0.142)	
Mean AP: 65.0%

 * Finished phase   4 epoch  11  model no.1 mAP: 65.0%  best: 65.5%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.106076002120972
Clustering and labeling...

 Clustered into 103 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [12][20/200]	Time 1.260 (1.257)	Data 0.001 (0.000)	Loss_ce 1.247 / 2.097	Loss_ce_soft 1.002 / 3.208	Loss_tri_soft 0.143 / 0.103	Loss_lf 4.997 / 3.235	Loss_lf_tri 0.197 / 0.118	Prec 93.75% / 83.44%	
Epoch: [12][40/200]	Time 1.276 (2.332)	Data 0.000 (1.071)	Loss_ce 1.244 / 2.135	Loss_ce_soft 1.003 / 3.177	Loss_tri_soft 0.142 / 0.108	Loss_lf 5.043 / 3.176	Loss_lf_tri 0.189 / 0.140	Prec 94.06% / 82.97%	
Epoch: [12][60/200]	Time 1.230 (2.698)	Data 0.000 (1.430)	Loss_ce 1.243 / 2.184	Loss_ce_soft 1.017 / 3.181	Loss_tri_soft 0.144 / 0.127	Loss_lf 5.045 / 3.123	Loss_lf_tri 0.192 / 0.158	Prec 93.75% / 82.71%	
Epoch: [12][80/200]	Time 1.314 (2.874)	Data 0.000 (1.606)	Loss_ce 1.262 / 2.235	Loss_ce_soft 1.064 / 3.195	Loss_tri_soft 0.156 / 0.127	Loss_lf 5.052 / 3.100	Loss_lf_tri 0.198 / 0.155	Prec 93.28% / 82.19%	
Epoch: [12][100/200]	Time 1.333 (2.565)	Data 0.000 (1.285)	Loss_ce 1.252 / 2.259	Loss_ce_soft 1.051 / 3.199	Loss_tri_soft 0.148 / 0.138	Loss_lf 5.057 / 3.135	Loss_lf_tri 0.193 / 0.164	Prec 93.31% / 81.75%	
Epoch: [12][120/200]	Time 1.235 (2.698)	Data 0.001 (1.419)	Loss_ce 1.259 / 2.280	Loss_ce_soft 1.068 / 3.217	Loss_tri_soft 0.153 / 0.136	Loss_lf 5.057 / 3.110	Loss_lf_tri 0.200 / 0.162	Prec 93.02% / 81.98%	
Epoch: [12][140/200]	Time 1.283 (2.796)	Data 0.001 (1.518)	Loss_ce 1.254 / 2.284	Loss_ce_soft 1.064 / 3.214	Loss_tri_soft 0.151 / 0.133	Loss_lf 5.054 / 3.126	Loss_lf_tri 0.202 / 0.158	Prec 93.04% / 81.96%	
Epoch: [12][160/200]	Time 1.298 (2.874)	Data 0.000 (1.597)	Loss_ce 1.249 / 2.303	Loss_ce_soft 1.055 / 3.219	Loss_tri_soft 0.149 / 0.127	Loss_lf 5.035 / 3.125	Loss_lf_tri 0.201 / 0.152	Prec 93.24% / 81.13%	
Epoch: [12][180/200]	Time 1.267 (2.957)	Data 0.001 (1.664)	Loss_ce 1.249 / 2.317	Loss_ce_soft 1.054 / 3.233	Loss_tri_soft 0.149 / 0.124	Loss_lf 5.045 / 3.134	Loss_lf_tri 0.203 / 0.149	Prec 93.12% / 80.87%	
Epoch: [12][200/200]	Time 1.255 (3.001)	Data 0.000 (1.497)	Loss_ce 1.243 / 2.294	Loss_ce_soft 1.051 / 3.216	Loss_tri_soft 0.144 / 0.122	Loss_lf 5.039 / 3.123	Loss_lf_tri 0.198 / 0.147	Prec 93.31% / 81.19%	
Extract Features: [50/302]	Time 0.186 (1.034)	Data 0.000 (0.863)	
Extract Features: [100/302]	Time 0.168 (0.603)	Data 0.000 (0.432)	
Extract Features: [150/302]	Time 0.184 (0.458)	Data 0.000 (0.288)	
Extract Features: [200/302]	Time 0.165 (0.387)	Data 0.000 (0.216)	
Extract Features: [250/302]	Time 0.166 (0.344)	Data 0.000 (0.173)	
Extract Features: [300/302]	Time 0.161 (0.315)	Data 0.000 (0.144)	
Mean AP: 65.4%

 * Finished phase   4 epoch  12  model no.1 mAP: 65.4%  best: 65.5%

Computing original distance...
Computing Jaccard distance...
Time cost: 8.964122533798218
Clustering and labeling...

 Clustered into 108 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [13][20/200]	Time 1.268 (1.260)	Data 0.000 (0.000)	Loss_ce 1.365 / 2.032	Loss_ce_soft 1.169 / 2.929	Loss_tri_soft 0.131 / 0.093	Loss_lf 5.096 / 3.038	Loss_lf_tri 0.175 / 0.127	Prec 90.31% / 85.94%	
Epoch: [13][40/200]	Time 1.345 (2.354)	Data 0.000 (1.072)	Loss_ce 1.315 / 2.000	Loss_ce_soft 1.075 / 2.861	Loss_tri_soft 0.131 / 0.120	Loss_lf 5.067 / 2.879	Loss_lf_tri 0.181 / 0.146	Prec 91.25% / 86.72%	
Epoch: [13][60/200]	Time 1.413 (2.783)	Data 0.000 (1.469)	Loss_ce 1.312 / 2.039	Loss_ce_soft 1.077 / 2.898	Loss_tri_soft 0.130 / 0.142	Loss_lf 5.066 / 2.970	Loss_lf_tri 0.175 / 0.159	Prec 90.62% / 85.62%	
Epoch: [13][80/200]	Time 1.390 (2.434)	Data 0.000 (1.101)	Loss_ce 1.305 / 2.120	Loss_ce_soft 1.069 / 2.972	Loss_tri_soft 0.128 / 0.145	Loss_lf 5.062 / 3.012	Loss_lf_tri 0.179 / 0.164	Prec 90.86% / 83.75%	
Epoch: [13][100/200]	Time 1.263 (2.649)	Data 0.000 (1.314)	Loss_ce 1.292 / 2.148	Loss_ce_soft 1.077 / 2.992	Loss_tri_soft 0.128 / 0.139	Loss_lf 5.075 / 3.005	Loss_lf_tri 0.183 / 0.158	Prec 91.50% / 83.38%	
Epoch: [13][120/200]	Time 1.289 (2.785)	Data 0.000 (1.457)	Loss_ce 1.285 / 2.185	Loss_ce_soft 1.068 / 3.039	Loss_tri_soft 0.137 / 0.136	Loss_lf 5.068 / 3.013	Loss_lf_tri 0.190 / 0.157	Prec 91.72% / 82.97%	
Epoch: [13][140/200]	Time 1.339 (2.877)	Data 0.001 (1.554)	Loss_ce 1.273 / 2.219	Loss_ce_soft 1.062 / 3.076	Loss_tri_soft 0.132 / 0.140	Loss_lf 5.056 / 3.020	Loss_lf_tri 0.185 / 0.162	Prec 92.28% / 82.50%	
Epoch: [13][160/200]	Time 1.330 (2.684)	Data 0.000 (1.360)	Loss_ce 1.268 / 2.259	Loss_ce_soft 1.061 / 3.119	Loss_tri_soft 0.133 / 0.140	Loss_lf 5.056 / 3.064	Loss_lf_tri 0.188 / 0.160	Prec 92.50% / 81.52%	
Epoch: [13][180/200]	Time 1.328 (2.769)	Data 0.000 (1.447)	Loss_ce 1.269 / 2.283	Loss_ce_soft 1.062 / 3.149	Loss_tri_soft 0.128 / 0.139	Loss_lf 5.049 / 3.073	Loss_lf_tri 0.182 / 0.160	Prec 92.22% / 81.46%	
Epoch: [13][200/200]	Time 1.292 (3.046)	Data 0.000 (1.511)	Loss_ce 1.262 / 2.283	Loss_ce_soft 1.057 / 3.159	Loss_tri_soft 0.127 / 0.137	Loss_lf 5.054 / 3.069	Loss_lf_tri 0.182 / 0.158	Prec 92.50% / 81.66%	
Extract Features: [50/302]	Time 0.170 (1.010)	Data 0.001 (0.837)	
Extract Features: [100/302]	Time 0.169 (0.590)	Data 0.000 (0.419)	
Extract Features: [150/302]	Time 0.173 (0.450)	Data 0.000 (0.279)	
Extract Features: [200/302]	Time 0.189 (0.380)	Data 0.000 (0.209)	
Extract Features: [250/302]	Time 0.187 (0.338)	Data 0.001 (0.168)	
Extract Features: [300/302]	Time 0.175 (0.311)	Data 0.000 (0.140)	
Mean AP: 65.6%

 * Finished phase   4 epoch  13  model no.1 mAP: 65.6%  best: 65.6% *

Computing original distance...
Computing Jaccard distance...
Time cost: 8.911139726638794
Clustering and labeling...

 Clustered into 110 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [14][20/200]	Time 1.234 (1.261)	Data 0.000 (0.000)	Loss_ce 1.299 / 2.136	Loss_ce_soft 1.136 / 3.080	Loss_tri_soft 0.132 / 0.157	Loss_lf 5.117 / 2.968	Loss_lf_tri 0.191 / 0.192	Prec 91.25% / 85.00%	
Epoch: [14][40/200]	Time 1.230 (2.315)	Data 0.000 (1.046)	Loss_ce 1.342 / 2.197	Loss_ce_soft 1.123 / 3.114	Loss_tri_soft 0.173 / 0.145	Loss_lf 5.075 / 2.959	Loss_lf_tri 0.219 / 0.179	Prec 88.59% / 83.91%	
Epoch: [14][60/200]	Time 1.286 (2.659)	Data 0.000 (1.388)	Loss_ce 1.288 / 2.240	Loss_ce_soft 1.048 / 3.170	Loss_tri_soft 0.159 / 0.135	Loss_lf 5.047 / 3.035	Loss_lf_tri 0.202 / 0.165	Prec 91.04% / 82.81%	
Epoch: [14][80/200]	Time 1.334 (2.314)	Data 0.000 (1.041)	Loss_ce 1.275 / 2.235	Loss_ce_soft 1.028 / 3.154	Loss_tri_soft 0.149 / 0.130	Loss_lf 5.026 / 3.065	Loss_lf_tri 0.195 / 0.161	Prec 91.80% / 83.12%	
Epoch: [14][100/200]	Time 1.325 (2.561)	Data 0.000 (1.254)	Loss_ce 1.290 / 2.245	Loss_ce_soft 1.060 / 3.134	Loss_tri_soft 0.151 / 0.119	Loss_lf 5.023 / 3.058	Loss_lf_tri 0.195 / 0.149	Prec 91.75% / 82.38%	
Epoch: [14][120/200]	Time 1.288 (2.705)	Data 0.000 (1.403)	Loss_ce 1.294 / 2.257	Loss_ce_soft 1.055 / 3.158	Loss_tri_soft 0.153 / 0.121	Loss_lf 5.024 / 3.065	Loss_lf_tri 0.202 / 0.149	Prec 91.61% / 81.98%	
Epoch: [14][140/200]	Time 1.289 (2.809)	Data 0.001 (1.510)	Loss_ce 1.298 / 2.275	Loss_ce_soft 1.062 / 3.186	Loss_tri_soft 0.153 / 0.121	Loss_lf 5.021 / 3.095	Loss_lf_tri 0.205 / 0.149	Prec 91.43% / 81.96%	
Epoch: [14][160/200]	Time 1.275 (2.619)	Data 0.000 (1.321)	Loss_ce 1.285 / 2.279	Loss_ce_soft 1.063 / 3.196	Loss_tri_soft 0.148 / 0.119	Loss_lf 5.020 / 3.122	Loss_lf_tri 0.203 / 0.151	Prec 91.76% / 82.07%	
Epoch: [14][180/200]	Time 1.351 (2.710)	Data 0.001 (1.410)	Loss_ce 1.282 / 2.288	Loss_ce_soft 1.055 / 3.207	Loss_tri_soft 0.150 / 0.120	Loss_lf 5.023 / 3.134	Loss_lf_tri 0.205 / 0.150	Prec 91.81% / 81.53%	
Epoch: [14][200/200]	Time 1.341 (2.989)	Data 0.000 (1.478)	Loss_ce 1.273 / 2.276	Loss_ce_soft 1.046 / 3.205	Loss_tri_soft 0.150 / 0.119	Loss_lf 5.021 / 3.132	Loss_lf_tri 0.204 / 0.149	Prec 91.97% / 82.06%	
Extract Features: [50/302]	Time 0.166 (1.030)	Data 0.000 (0.860)	
Extract Features: [100/302]	Time 0.166 (0.600)	Data 0.000 (0.430)	
Extract Features: [150/302]	Time 0.174 (0.457)	Data 0.000 (0.287)	
Extract Features: [200/302]	Time 0.167 (0.385)	Data 0.000 (0.215)	
Extract Features: [250/302]	Time 0.186 (0.342)	Data 0.001 (0.172)	
Extract Features: [300/302]	Time 0.163 (0.313)	Data 0.001 (0.144)	
Mean AP: 65.6%

 * Finished phase   4 epoch  14  model no.1 mAP: 65.6%  best: 65.6% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.090080976486206
Clustering and labeling...

 Clustered into 112 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [15][20/200]	Time 1.320 (1.278)	Data 0.001 (0.000)	Loss_ce 1.346 / 2.227	Loss_ce_soft 1.097 / 3.297	Loss_tri_soft 0.119 / 0.159	Loss_lf 5.068 / 3.107	Loss_lf_tri 0.180 / 0.174	Prec 91.56% / 81.25%	
Epoch: [15][40/200]	Time 1.232 (2.370)	Data 0.000 (1.084)	Loss_ce 1.314 / 2.225	Loss_ce_soft 1.091 / 3.265	Loss_tri_soft 0.123 / 0.135	Loss_lf 5.047 / 3.174	Loss_lf_tri 0.187 / 0.157	Prec 91.25% / 81.72%	
Epoch: [15][60/200]	Time 1.260 (2.717)	Data 0.000 (1.428)	Loss_ce 1.290 / 2.251	Loss_ce_soft 1.060 / 3.270	Loss_tri_soft 0.130 / 0.131	Loss_lf 5.045 / 3.219	Loss_lf_tri 0.187 / 0.155	Prec 92.60% / 80.94%	
Epoch: [15][80/200]	Time 1.285 (2.357)	Data 0.000 (1.071)	Loss_ce 1.282 / 2.264	Loss_ce_soft 1.071 / 3.268	Loss_tri_soft 0.132 / 0.128	Loss_lf 5.057 / 3.170	Loss_lf_tri 0.187 / 0.151	Prec 92.73% / 81.25%	
Epoch: [15][100/200]	Time 1.305 (2.573)	Data 0.000 (1.282)	Loss_ce 1.288 / 2.281	Loss_ce_soft 1.080 / 3.242	Loss_tri_soft 0.135 / 0.133	Loss_lf 5.034 / 3.170	Loss_lf_tri 0.193 / 0.152	Prec 92.75% / 81.19%	
Epoch: [15][120/200]	Time 1.242 (2.720)	Data 0.001 (1.423)	Loss_ce 1.278 / 2.301	Loss_ce_soft 1.093 / 3.257	Loss_tri_soft 0.128 / 0.141	Loss_lf 5.046 / 3.155	Loss_lf_tri 0.188 / 0.165	Prec 92.97% / 80.94%	
Epoch: [15][140/200]	Time 1.279 (2.511)	Data 0.001 (1.219)	Loss_ce 1.267 / 2.296	Loss_ce_soft 1.080 / 3.228	Loss_tri_soft 0.132 / 0.145	Loss_lf 5.036 / 3.142	Loss_lf_tri 0.191 / 0.167	Prec 93.08% / 81.56%	
Epoch: [15][160/200]	Time 1.359 (2.627)	Data 0.000 (1.332)	Loss_ce 1.266 / 2.314	Loss_ce_soft 1.072 / 3.234	Loss_tri_soft 0.131 / 0.140	Loss_lf 5.030 / 3.142	Loss_lf_tri 0.186 / 0.161	Prec 92.81% / 81.21%	
Epoch: [15][180/200]	Time 1.283 (2.718)	Data 0.000 (1.422)	Loss_ce 1.275 / 2.327	Loss_ce_soft 1.075 / 3.255	Loss_tri_soft 0.133 / 0.138	Loss_lf 5.025 / 3.158	Loss_lf_tri 0.188 / 0.160	Prec 92.64% / 81.01%	
Epoch: [15][200/200]	Time 1.236 (2.997)	Data 0.000 (1.492)	Loss_ce 1.268 / 2.305	Loss_ce_soft 1.069 / 3.235	Loss_tri_soft 0.129 / 0.135	Loss_lf 5.025 / 3.145	Loss_lf_tri 0.186 / 0.158	Prec 92.75% / 81.41%	
Extract Features: [50/302]	Time 0.185 (1.025)	Data 0.000 (0.850)	
Extract Features: [100/302]	Time 0.185 (0.598)	Data 0.000 (0.425)	
Extract Features: [150/302]	Time 0.164 (0.455)	Data 0.000 (0.284)	
Extract Features: [200/302]	Time 0.168 (0.396)	Data 0.001 (0.213)	
Extract Features: [250/302]	Time 0.185 (0.351)	Data 0.000 (0.170)	
Extract Features: [300/302]	Time 0.191 (0.321)	Data 0.000 (0.142)	
Mean AP: 65.3%

 * Finished phase   4 epoch  15  model no.1 mAP: 65.3%  best: 65.6%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.060091972351074
Clustering and labeling...

 Clustered into 110 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [16][20/200]	Time 1.248 (1.257)	Data 0.001 (0.000)	Loss_ce 1.315 / 2.215	Loss_ce_soft 1.123 / 3.214	Loss_tri_soft 0.153 / 0.116	Loss_lf 5.020 / 3.140	Loss_lf_tri 0.190 / 0.144	Prec 92.19% / 80.94%	
Epoch: [16][40/200]	Time 1.283 (2.315)	Data 0.000 (1.051)	Loss_ce 1.284 / 2.273	Loss_ce_soft 1.053 / 3.216	Loss_tri_soft 0.151 / 0.122	Loss_lf 5.016 / 3.050	Loss_lf_tri 0.206 / 0.146	Prec 92.66% / 78.75%	
Epoch: [16][60/200]	Time 1.301 (2.698)	Data 0.000 (1.419)	Loss_ce 1.260 / 2.264	Loss_ce_soft 1.044 / 3.190	Loss_tri_soft 0.148 / 0.123	Loss_lf 5.033 / 3.044	Loss_lf_tri 0.201 / 0.146	Prec 93.23% / 80.10%	
Epoch: [16][80/200]	Time 1.318 (2.347)	Data 0.000 (1.065)	Loss_ce 1.266 / 2.248	Loss_ce_soft 1.056 / 3.170	Loss_tri_soft 0.147 / 0.127	Loss_lf 5.026 / 3.038	Loss_lf_tri 0.206 / 0.144	Prec 93.12% / 81.25%	
Epoch: [16][100/200]	Time 1.346 (2.557)	Data 0.000 (1.275)	Loss_ce 1.252 / 2.288	Loss_ce_soft 1.063 / 3.193	Loss_tri_soft 0.139 / 0.128	Loss_lf 5.040 / 3.078	Loss_lf_tri 0.201 / 0.147	Prec 93.56% / 80.62%	
Epoch: [16][120/200]	Time 1.315 (2.703)	Data 0.000 (1.421)	Loss_ce 1.255 / 2.298	Loss_ce_soft 1.067 / 3.205	Loss_tri_soft 0.153 / 0.125	Loss_lf 5.039 / 3.118	Loss_lf_tri 0.209 / 0.144	Prec 93.44% / 80.78%	
Epoch: [16][140/200]	Time 1.270 (2.808)	Data 0.000 (1.524)	Loss_ce 1.255 / 2.325	Loss_ce_soft 1.063 / 3.235	Loss_tri_soft 0.155 / 0.130	Loss_lf 5.039 / 3.138	Loss_lf_tri 0.210 / 0.152	Prec 93.21% / 80.36%	
Epoch: [16][160/200]	Time 1.328 (2.622)	Data 0.000 (1.333)	Loss_ce 1.255 / 2.338	Loss_ce_soft 1.065 / 3.235	Loss_tri_soft 0.153 / 0.134	Loss_lf 5.035 / 3.156	Loss_lf_tri 0.207 / 0.154	Prec 92.93% / 79.77%	
Epoch: [16][180/200]	Time 1.347 (2.715)	Data 0.000 (1.424)	Loss_ce 1.246 / 2.348	Loss_ce_soft 1.062 / 3.236	Loss_tri_soft 0.153 / 0.133	Loss_lf 5.039 / 3.159	Loss_lf_tri 0.209 / 0.154	Prec 93.37% / 79.62%	
Epoch: [16][200/200]	Time 1.299 (3.001)	Data 0.000 (1.496)	Loss_ce 1.245 / 2.331	Loss_ce_soft 1.064 / 3.234	Loss_tri_soft 0.149 / 0.132	Loss_lf 5.040 / 3.153	Loss_lf_tri 0.205 / 0.153	Prec 93.44% / 79.97%	
Extract Features: [50/302]	Time 0.169 (1.018)	Data 0.000 (0.846)	
Extract Features: [100/302]	Time 0.168 (0.594)	Data 0.000 (0.423)	
Extract Features: [150/302]	Time 0.168 (0.452)	Data 0.001 (0.282)	
Extract Features: [200/302]	Time 0.166 (0.382)	Data 0.001 (0.212)	
Extract Features: [250/302]	Time 0.165 (0.339)	Data 0.000 (0.169)	
Extract Features: [300/302]	Time 0.186 (0.311)	Data 0.000 (0.141)	
Mean AP: 66.0%

 * Finished phase   4 epoch  16  model no.1 mAP: 66.0%  best: 66.0% *

Computing original distance...
Computing Jaccard distance...
Time cost: 9.182052373886108
Clustering and labeling...

 Clustered into 108 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [17][20/200]	Time 1.401 (1.342)	Data 0.000 (0.000)	Loss_ce 1.307 / 2.108	Loss_ce_soft 1.049 / 3.061	Loss_tri_soft 0.134 / 0.095	Loss_lf 4.999 / 3.071	Loss_lf_tri 0.165 / 0.126	Prec 92.19% / 83.12%	
Epoch: [17][40/200]	Time 1.295 (2.375)	Data 0.000 (1.051)	Loss_ce 1.279 / 2.133	Loss_ce_soft 1.041 / 3.119	Loss_tri_soft 0.102 / 0.096	Loss_lf 5.055 / 3.075	Loss_lf_tri 0.157 / 0.132	Prec 92.19% / 83.59%	
Epoch: [17][60/200]	Time 1.257 (2.721)	Data 0.000 (1.402)	Loss_ce 1.338 / 2.250	Loss_ce_soft 1.109 / 3.217	Loss_tri_soft 0.123 / 0.104	Loss_lf 5.079 / 3.124	Loss_lf_tri 0.177 / 0.140	Prec 90.83% / 80.94%	
Epoch: [17][80/200]	Time 1.235 (2.358)	Data 0.000 (1.052)	Loss_ce 1.313 / 2.232	Loss_ce_soft 1.084 / 3.148	Loss_tri_soft 0.131 / 0.096	Loss_lf 5.060 / 3.123	Loss_lf_tri 0.192 / 0.129	Prec 91.56% / 81.41%	
Epoch: [17][100/200]	Time 1.301 (2.566)	Data 0.000 (1.267)	Loss_ce 1.300 / 2.236	Loss_ce_soft 1.070 / 3.142	Loss_tri_soft 0.127 / 0.102	Loss_lf 5.041 / 3.071	Loss_lf_tri 0.187 / 0.132	Prec 91.94% / 82.38%	
Epoch: [17][120/200]	Time 1.288 (2.709)	Data 0.000 (1.410)	Loss_ce 1.290 / 2.259	Loss_ce_soft 1.072 / 3.165	Loss_tri_soft 0.124 / 0.103	Loss_lf 5.040 / 3.087	Loss_lf_tri 0.184 / 0.132	Prec 92.29% / 82.08%	
Epoch: [17][140/200]	Time 1.259 (2.827)	Data 0.000 (1.513)	Loss_ce 1.282 / 2.274	Loss_ce_soft 1.054 / 3.172	Loss_tri_soft 0.127 / 0.115	Loss_lf 5.035 / 3.092	Loss_lf_tri 0.187 / 0.142	Prec 92.32% / 81.52%	
Epoch: [17][160/200]	Time 1.260 (2.633)	Data 0.000 (1.324)	Loss_ce 1.272 / 2.288	Loss_ce_soft 1.044 / 3.177	Loss_tri_soft 0.130 / 0.108	Loss_lf 5.042 / 3.078	Loss_lf_tri 0.190 / 0.137	Prec 92.58% / 81.45%	
Epoch: [17][180/200]	Time 1.257 (2.717)	Data 0.000 (1.411)	Loss_ce 1.262 / 2.311	Loss_ce_soft 1.037 / 3.203	Loss_tri_soft 0.130 / 0.109	Loss_lf 5.036 / 3.113	Loss_lf_tri 0.192 / 0.139	Prec 92.81% / 81.11%	
Epoch: [17][200/200]	Time 1.281 (3.002)	Data 0.000 (1.485)	Loss_ce 1.257 / 2.288	Loss_ce_soft 1.041 / 3.190	Loss_tri_soft 0.130 / 0.106	Loss_lf 5.045 / 3.093	Loss_lf_tri 0.190 / 0.136	Prec 92.94% / 81.41%	
Extract Features: [50/302]	Time 0.184 (1.043)	Data 0.000 (0.870)	
Extract Features: [100/302]	Time 0.168 (0.608)	Data 0.000 (0.435)	
Extract Features: [150/302]	Time 0.185 (0.463)	Data 0.000 (0.290)	
Extract Features: [200/302]	Time 0.168 (0.390)	Data 0.000 (0.218)	
Extract Features: [250/302]	Time 0.167 (0.346)	Data 0.001 (0.174)	
Extract Features: [300/302]	Time 0.163 (0.317)	Data 0.000 (0.145)	
Mean AP: 65.0%

 * Finished phase   4 epoch  17  model no.1 mAP: 65.0%  best: 66.0%

Computing original distance...
Computing Jaccard distance...
Time cost: 10.095762968063354
Clustering and labeling...

 Clustered into 120 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [18][20/200]	Time 1.299 (1.252)	Data 0.001 (0.000)	Loss_ce 1.305 / 2.205	Loss_ce_soft 1.206 / 3.189	Loss_tri_soft 0.116 / 0.119	Loss_lf 5.107 / 3.014	Loss_lf_tri 0.187 / 0.148	Prec 91.25% / 81.56%	
Epoch: [18][40/200]	Time 1.321 (2.356)	Data 0.000 (1.078)	Loss_ce 1.311 / 2.169	Loss_ce_soft 1.172 / 3.116	Loss_tri_soft 0.118 / 0.150	Loss_lf 5.041 / 3.131	Loss_lf_tri 0.179 / 0.176	Prec 92.66% / 82.66%	
Epoch: [18][60/200]	Time 1.292 (2.014)	Data 0.000 (0.718)	Loss_ce 1.316 / 2.201	Loss_ce_soft 1.159 / 3.143	Loss_tri_soft 0.138 / 0.145	Loss_lf 5.054 / 3.139	Loss_lf_tri 0.196 / 0.170	Prec 92.29% / 83.23%	
Epoch: [18][80/200]	Time 1.260 (2.365)	Data 0.000 (1.076)	Loss_ce 1.301 / 2.198	Loss_ce_soft 1.125 / 3.127	Loss_tri_soft 0.132 / 0.151	Loss_lf 5.051 / 3.127	Loss_lf_tri 0.187 / 0.172	Prec 92.58% / 83.59%	
Epoch: [18][100/200]	Time 1.227 (2.572)	Data 0.000 (1.288)	Loss_ce 1.305 / 2.244	Loss_ce_soft 1.114 / 3.175	Loss_tri_soft 0.128 / 0.153	Loss_lf 5.048 / 3.133	Loss_lf_tri 0.184 / 0.173	Prec 92.31% / 82.25%	
Epoch: [18][120/200]	Time 1.343 (2.363)	Data 0.000 (1.074)	Loss_ce 1.301 / 2.266	Loss_ce_soft 1.109 / 3.177	Loss_tri_soft 0.127 / 0.149	Loss_lf 5.036 / 3.146	Loss_lf_tri 0.183 / 0.168	Prec 92.14% / 81.98%	
Epoch: [18][140/200]	Time 1.318 (2.518)	Data 0.000 (1.227)	Loss_ce 1.302 / 2.271	Loss_ce_soft 1.120 / 3.177	Loss_tri_soft 0.127 / 0.140	Loss_lf 5.041 / 3.153	Loss_lf_tri 0.187 / 0.163	Prec 91.92% / 81.74%	
Epoch: [18][160/200]	Time 1.348 (2.633)	Data 0.001 (1.341)	Loss_ce 1.316 / 2.285	Loss_ce_soft 1.130 / 3.186	Loss_tri_soft 0.135 / 0.141	Loss_lf 5.050 / 3.146	Loss_lf_tri 0.194 / 0.164	Prec 91.88% / 81.52%	
Epoch: [18][180/200]	Time 1.294 (2.487)	Data 0.000 (1.192)	Loss_ce 1.310 / 2.292	Loss_ce_soft 1.112 / 3.187	Loss_tri_soft 0.131 / 0.137	Loss_lf 5.043 / 3.143	Loss_lf_tri 0.190 / 0.160	Prec 91.84% / 81.25%	
Epoch: [18][200/200]	Time 1.251 (2.801)	Data 0.000 (1.289)	Loss_ce 1.299 / 2.281	Loss_ce_soft 1.104 / 3.192	Loss_tri_soft 0.132 / 0.134	Loss_lf 5.040 / 3.117	Loss_lf_tri 0.191 / 0.156	Prec 92.16% / 81.72%	
Extract Features: [50/302]	Time 0.168 (1.031)	Data 0.001 (0.855)	
Extract Features: [100/302]	Time 0.167 (0.602)	Data 0.000 (0.428)	
Extract Features: [150/302]	Time 0.167 (0.458)	Data 0.000 (0.285)	
Extract Features: [200/302]	Time 0.167 (0.386)	Data 0.001 (0.214)	
Extract Features: [250/302]	Time 0.171 (0.344)	Data 0.000 (0.171)	
Extract Features: [300/302]	Time 0.180 (0.315)	Data 0.000 (0.143)	
Mean AP: 65.5%

 * Finished phase   4 epoch  18  model no.1 mAP: 65.5%  best: 66.0%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.069088697433472
Clustering and labeling...

 Clustered into 110 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [19][20/200]	Time 1.262 (1.268)	Data 0.000 (0.000)	Loss_ce 1.263 / 2.196	Loss_ce_soft 1.023 / 3.056	Loss_tri_soft 0.122 / 0.151	Loss_lf 5.078 / 3.082	Loss_lf_tri 0.175 / 0.172	Prec 92.19% / 81.25%	
Epoch: [19][40/200]	Time 1.386 (2.330)	Data 0.000 (1.039)	Loss_ce 1.281 / 2.235	Loss_ce_soft 1.032 / 3.127	Loss_tri_soft 0.135 / 0.150	Loss_lf 5.108 / 2.997	Loss_lf_tri 0.193 / 0.171	Prec 92.81% / 82.03%	
Epoch: [19][60/200]	Time 1.239 (2.694)	Data 0.000 (1.397)	Loss_ce 1.279 / 2.235	Loss_ce_soft 1.058 / 3.171	Loss_tri_soft 0.117 / 0.124	Loss_lf 5.091 / 3.052	Loss_lf_tri 0.177 / 0.147	Prec 93.12% / 82.81%	
Epoch: [19][80/200]	Time 1.261 (2.338)	Data 0.000 (1.047)	Loss_ce 1.280 / 2.212	Loss_ce_soft 1.077 / 3.126	Loss_tri_soft 0.126 / 0.121	Loss_lf 5.072 / 3.085	Loss_lf_tri 0.188 / 0.144	Prec 92.97% / 82.73%	
Epoch: [19][100/200]	Time 1.348 (2.544)	Data 0.000 (1.252)	Loss_ce 1.283 / 2.226	Loss_ce_soft 1.084 / 3.128	Loss_tri_soft 0.120 / 0.133	Loss_lf 5.052 / 3.091	Loss_lf_tri 0.181 / 0.154	Prec 93.06% / 82.12%	
Epoch: [19][120/200]	Time 1.395 (2.698)	Data 0.000 (1.400)	Loss_ce 1.272 / 2.233	Loss_ce_soft 1.066 / 3.129	Loss_tri_soft 0.125 / 0.131	Loss_lf 5.051 / 3.089	Loss_lf_tri 0.185 / 0.151	Prec 93.28% / 82.45%	
Epoch: [19][140/200]	Time 1.283 (2.806)	Data 0.000 (1.503)	Loss_ce 1.270 / 2.245	Loss_ce_soft 1.056 / 3.140	Loss_tri_soft 0.123 / 0.123	Loss_lf 5.056 / 3.104	Loss_lf_tri 0.184 / 0.144	Prec 92.90% / 82.37%	
Epoch: [19][160/200]	Time 1.323 (2.615)	Data 0.001 (1.315)	Loss_ce 1.265 / 2.270	Loss_ce_soft 1.043 / 3.159	Loss_tri_soft 0.119 / 0.123	Loss_lf 5.053 / 3.118	Loss_lf_tri 0.181 / 0.145	Prec 92.70% / 81.84%	
Epoch: [19][180/200]	Time 1.311 (2.699)	Data 0.001 (1.400)	Loss_ce 1.269 / 2.274	Loss_ce_soft 1.050 / 3.170	Loss_tri_soft 0.130 / 0.118	Loss_lf 5.054 / 3.116	Loss_lf_tri 0.192 / 0.142	Prec 92.60% / 82.05%	
Epoch: [19][200/200]	Time 1.233 (2.983)	Data 0.000 (1.474)	Loss_ce 1.264 / 2.248	Loss_ce_soft 1.049 / 3.155	Loss_tri_soft 0.133 / 0.115	Loss_lf 5.051 / 3.103	Loss_lf_tri 0.194 / 0.138	Prec 92.72% / 82.56%	
Extract Features: [50/302]	Time 0.173 (1.045)	Data 0.000 (0.870)	
Extract Features: [100/302]	Time 0.169 (0.608)	Data 0.000 (0.435)	
Extract Features: [150/302]	Time 0.166 (0.462)	Data 0.000 (0.290)	
Extract Features: [200/302]	Time 0.189 (0.391)	Data 0.000 (0.218)	
Extract Features: [250/302]	Time 0.167 (0.347)	Data 0.000 (0.174)	
Extract Features: [300/302]	Time 0.180 (0.318)	Data 0.000 (0.145)	
Mean AP: 65.7%

 * Finished phase   4 epoch  19  model no.1 mAP: 65.7%  best: 66.0%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.095081090927124
Clustering and labeling...

 Clustered into 117 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [20][20/200]	Time 1.235 (1.258)	Data 0.001 (0.000)	Loss_ce 1.355 / 2.176	Loss_ce_soft 1.167 / 3.134	Loss_tri_soft 0.191 / 0.130	Loss_lf 5.114 / 2.912	Loss_lf_tri 0.242 / 0.174	Prec 90.62% / 81.25%	
Epoch: [20][40/200]	Time 1.341 (2.326)	Data 0.001 (1.056)	Loss_ce 1.371 / 2.242	Loss_ce_soft 1.212 / 3.193	Loss_tri_soft 0.179 / 0.114	Loss_lf 5.088 / 3.062	Loss_lf_tri 0.231 / 0.154	Prec 90.62% / 80.94%	
Epoch: [20][60/200]	Time 1.281 (2.691)	Data 0.000 (1.406)	Loss_ce 1.343 / 2.287	Loss_ce_soft 1.167 / 3.243	Loss_tri_soft 0.166 / 0.129	Loss_lf 5.085 / 3.142	Loss_lf_tri 0.214 / 0.172	Prec 91.15% / 79.69%	
Epoch: [20][80/200]	Time 1.298 (2.336)	Data 0.000 (1.054)	Loss_ce 1.333 / 2.266	Loss_ce_soft 1.158 / 3.222	Loss_tri_soft 0.154 / 0.120	Loss_lf 5.080 / 3.118	Loss_lf_tri 0.209 / 0.158	Prec 91.41% / 81.09%	
Epoch: [20][100/200]	Time 1.314 (2.548)	Data 0.001 (1.258)	Loss_ce 1.315 / 2.276	Loss_ce_soft 1.155 / 3.227	Loss_tri_soft 0.152 / 0.135	Loss_lf 5.091 / 3.178	Loss_lf_tri 0.205 / 0.169	Prec 91.75% / 81.56%	
Epoch: [20][120/200]	Time 1.357 (2.697)	Data 0.000 (1.401)	Loss_ce 1.302 / 2.292	Loss_ce_soft 1.141 / 3.245	Loss_tri_soft 0.144 / 0.133	Loss_lf 5.076 / 3.180	Loss_lf_tri 0.200 / 0.165	Prec 92.03% / 81.56%	
Epoch: [20][140/200]	Time 1.317 (2.500)	Data 0.000 (1.201)	Loss_ce 1.291 / 2.308	Loss_ce_soft 1.126 / 3.242	Loss_tri_soft 0.147 / 0.129	Loss_lf 5.076 / 3.187	Loss_lf_tri 0.205 / 0.162	Prec 92.28% / 81.52%	
Epoch: [20][160/200]	Time 1.297 (2.629)	Data 0.000 (1.312)	Loss_ce 1.280 / 2.307	Loss_ce_soft 1.128 / 3.231	Loss_tri_soft 0.148 / 0.122	Loss_lf 5.077 / 3.163	Loss_lf_tri 0.205 / 0.154	Prec 92.62% / 81.91%	
Epoch: [20][180/200]	Time 1.253 (2.714)	Data 0.001 (1.404)	Loss_ce 1.272 / 2.307	Loss_ce_soft 1.117 / 3.217	Loss_tri_soft 0.143 / 0.123	Loss_lf 5.074 / 3.152	Loss_lf_tri 0.202 / 0.154	Prec 92.85% / 81.84%	
Epoch: [20][200/200]	Time 1.233 (2.782)	Data 0.001 (1.264)	Loss_ce 1.267 / 2.295	Loss_ce_soft 1.110 / 3.216	Loss_tri_soft 0.139 / 0.122	Loss_lf 5.069 / 3.162	Loss_lf_tri 0.198 / 0.154	Prec 93.09% / 81.78%	
Extract Features: [50/302]	Time 0.171 (1.036)	Data 0.000 (0.863)	
Extract Features: [100/302]	Time 0.166 (0.605)	Data 0.000 (0.431)	
Extract Features: [150/302]	Time 0.170 (0.461)	Data 0.001 (0.288)	
Extract Features: [200/302]	Time 0.175 (0.389)	Data 0.000 (0.216)	
Extract Features: [250/302]	Time 0.174 (0.346)	Data 0.001 (0.173)	
Extract Features: [300/302]	Time 0.189 (0.317)	Data 0.001 (0.144)	
Mean AP: 65.0%

 * Finished phase   4 epoch  20  model no.1 mAP: 65.0%  best: 66.0%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.661897897720337
Clustering and labeling...

 Clustered into 117 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [21][20/200]	Time 1.316 (1.347)	Data 0.000 (0.000)	Loss_ce 1.366 / 2.214	Loss_ce_soft 1.182 / 3.198	Loss_tri_soft 0.123 / 0.183	Loss_lf 5.085 / 3.072	Loss_lf_tri 0.173 / 0.190	Prec 92.50% / 79.69%	
Epoch: [21][40/200]	Time 1.302 (2.412)	Data 0.000 (1.071)	Loss_ce 1.337 / 2.284	Loss_ce_soft 1.156 / 3.218	Loss_tri_soft 0.143 / 0.167	Loss_lf 5.076 / 3.214	Loss_lf_tri 0.198 / 0.172	Prec 92.19% / 79.69%	
Epoch: [21][60/200]	Time 1.348 (2.744)	Data 0.000 (1.406)	Loss_ce 1.299 / 2.273	Loss_ce_soft 1.111 / 3.210	Loss_tri_soft 0.127 / 0.150	Loss_lf 5.078 / 3.164	Loss_lf_tri 0.183 / 0.161	Prec 92.81% / 80.94%	
Epoch: [21][80/200]	Time 1.312 (2.383)	Data 0.000 (1.054)	Loss_ce 1.301 / 2.272	Loss_ce_soft 1.117 / 3.184	Loss_tri_soft 0.121 / 0.140	Loss_lf 5.052 / 3.160	Loss_lf_tri 0.181 / 0.156	Prec 92.58% / 81.48%	
Epoch: [21][100/200]	Time 1.254 (2.587)	Data 0.000 (1.264)	Loss_ce 1.296 / 2.252	Loss_ce_soft 1.088 / 3.143	Loss_tri_soft 0.114 / 0.141	Loss_lf 5.027 / 3.170	Loss_lf_tri 0.174 / 0.160	Prec 92.56% / 81.75%	
Epoch: [21][120/200]	Time 1.286 (2.726)	Data 0.001 (1.409)	Loss_ce 1.295 / 2.258	Loss_ce_soft 1.093 / 3.143	Loss_tri_soft 0.113 / 0.135	Loss_lf 5.023 / 3.131	Loss_lf_tri 0.172 / 0.157	Prec 92.24% / 81.67%	
Epoch: [21][140/200]	Time 1.322 (2.527)	Data 0.000 (1.208)	Loss_ce 1.290 / 2.273	Loss_ce_soft 1.094 / 3.166	Loss_tri_soft 0.114 / 0.139	Loss_lf 5.026 / 3.133	Loss_lf_tri 0.173 / 0.160	Prec 92.37% / 81.92%	
Epoch: [21][160/200]	Time 1.261 (2.634)	Data 0.000 (1.318)	Loss_ce 1.292 / 2.266	Loss_ce_soft 1.095 / 3.147	Loss_tri_soft 0.117 / 0.134	Loss_lf 5.031 / 3.131	Loss_lf_tri 0.175 / 0.158	Prec 92.19% / 82.27%	
Epoch: [21][180/200]	Time 1.232 (2.721)	Data 0.000 (1.411)	Loss_ce 1.301 / 2.266	Loss_ce_soft 1.108 / 3.142	Loss_tri_soft 0.120 / 0.129	Loss_lf 5.026 / 3.101	Loss_lf_tri 0.180 / 0.152	Prec 91.88% / 82.26%	
Epoch: [21][200/200]	Time 1.250 (2.789)	Data 0.000 (1.270)	Loss_ce 1.287 / 2.260	Loss_ce_soft 1.090 / 3.132	Loss_tri_soft 0.119 / 0.130	Loss_lf 5.025 / 3.110	Loss_lf_tri 0.178 / 0.154	Prec 92.22% / 82.06%	
Extract Features: [50/302]	Time 0.168 (1.031)	Data 0.000 (0.858)	
Extract Features: [100/302]	Time 0.165 (0.601)	Data 0.000 (0.429)	
Extract Features: [150/302]	Time 0.166 (0.457)	Data 0.000 (0.286)	
Extract Features: [200/302]	Time 0.169 (0.385)	Data 0.000 (0.215)	
Extract Features: [250/302]	Time 0.169 (0.342)	Data 0.000 (0.172)	
Extract Features: [300/302]	Time 0.166 (0.313)	Data 0.001 (0.143)	
Mean AP: 65.0%

 * Finished phase   4 epoch  21  model no.1 mAP: 65.0%  best: 66.0%

Computing original distance...
Computing Jaccard distance...
Time cost: 11.039452314376831
Clustering and labeling...

 Clustered into 111 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [22][20/200]	Time 1.325 (1.313)	Data 0.000 (0.000)	Loss_ce 1.281 / 2.218	Loss_ce_soft 1.110 / 3.111	Loss_tri_soft 0.097 / 0.124	Loss_lf 5.025 / 2.975	Loss_lf_tri 0.177 / 0.164	Prec 93.75% / 83.44%	
Epoch: [22][40/200]	Time 1.370 (2.467)	Data 0.000 (1.078)	Loss_ce 1.276 / 2.164	Loss_ce_soft 1.071 / 3.070	Loss_tri_soft 0.117 / 0.102	Loss_lf 5.028 / 2.993	Loss_lf_tri 0.182 / 0.142	Prec 93.59% / 84.22%	
Epoch: [22][60/200]	Time 1.289 (2.796)	Data 0.000 (1.427)	Loss_ce 1.265 / 2.183	Loss_ce_soft 1.035 / 3.049	Loss_tri_soft 0.112 / 0.097	Loss_lf 5.013 / 3.046	Loss_lf_tri 0.178 / 0.137	Prec 93.23% / 82.81%	
Epoch: [22][80/200]	Time 1.300 (2.416)	Data 0.000 (1.070)	Loss_ce 1.274 / 2.222	Loss_ce_soft 1.037 / 3.096	Loss_tri_soft 0.119 / 0.097	Loss_lf 5.029 / 3.078	Loss_lf_tri 0.182 / 0.136	Prec 92.58% / 81.80%	
Epoch: [22][100/200]	Time 1.287 (2.623)	Data 0.000 (1.284)	Loss_ce 1.267 / 2.231	Loss_ce_soft 1.026 / 3.089	Loss_tri_soft 0.114 / 0.108	Loss_lf 5.031 / 3.094	Loss_lf_tri 0.177 / 0.145	Prec 92.69% / 82.06%	
Epoch: [22][120/200]	Time 1.338 (2.754)	Data 0.000 (1.420)	Loss_ce 1.265 / 2.251	Loss_ce_soft 1.027 / 3.119	Loss_tri_soft 0.116 / 0.106	Loss_lf 5.024 / 3.085	Loss_lf_tri 0.183 / 0.141	Prec 92.92% / 81.56%	
Epoch: [22][140/200]	Time 1.343 (2.864)	Data 0.000 (1.527)	Loss_ce 1.258 / 2.264	Loss_ce_soft 1.016 / 3.132	Loss_tri_soft 0.112 / 0.102	Loss_lf 5.029 / 3.087	Loss_lf_tri 0.179 / 0.138	Prec 93.08% / 81.70%	
Epoch: [22][160/200]	Time 1.412 (2.676)	Data 0.000 (1.336)	Loss_ce 1.253 / 2.279	Loss_ce_soft 1.017 / 3.151	Loss_tri_soft 0.113 / 0.105	Loss_lf 5.033 / 3.097	Loss_lf_tri 0.179 / 0.139	Prec 93.24% / 81.95%	
Epoch: [22][180/200]	Time 1.353 (2.773)	Data 0.000 (1.432)	Loss_ce 1.253 / 2.292	Loss_ce_soft 1.020 / 3.161	Loss_tri_soft 0.119 / 0.108	Loss_lf 5.027 / 3.104	Loss_lf_tri 0.183 / 0.142	Prec 92.95% / 81.91%	
Epoch: [22][200/200]	Time 1.364 (3.056)	Data 0.000 (1.503)	Loss_ce 1.246 / 2.258	Loss_ce_soft 1.020 / 3.129	Loss_tri_soft 0.122 / 0.109	Loss_lf 5.035 / 3.098	Loss_lf_tri 0.184 / 0.142	Prec 93.12% / 82.75%	
Extract Features: [50/302]	Time 0.166 (1.042)	Data 0.000 (0.869)	
Extract Features: [100/302]	Time 0.169 (0.607)	Data 0.000 (0.435)	
Extract Features: [150/302]	Time 0.170 (0.461)	Data 0.000 (0.290)	
Extract Features: [200/302]	Time 0.166 (0.388)	Data 0.000 (0.217)	
Extract Features: [250/302]	Time 0.187 (0.345)	Data 0.000 (0.174)	
Extract Features: [300/302]	Time 0.180 (0.316)	Data 0.000 (0.145)	
Mean AP: 66.1%

 * Finished phase   4 epoch  22  model no.1 mAP: 66.1%  best: 66.1% *

Computing original distance...
Computing Jaccard distance...
Time cost: 8.828166007995605
Clustering and labeling...

 Clustered into 119 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [23][20/200]	Time 1.381 (1.287)	Data 0.000 (0.000)	Loss_ce 1.279 / 2.169	Loss_ce_soft 1.120 / 3.163	Loss_tri_soft 0.130 / 0.114	Loss_lf 5.052 / 3.060	Loss_lf_tri 0.175 / 0.151	Prec 95.31% / 82.50%	
Epoch: [23][40/200]	Time 1.374 (2.418)	Data 0.000 (1.108)	Loss_ce 1.266 / 2.185	Loss_ce_soft 1.058 / 3.170	Loss_tri_soft 0.111 / 0.145	Loss_lf 5.032 / 3.128	Loss_lf_tri 0.174 / 0.172	Prec 95.00% / 84.38%	
Epoch: [23][60/200]	Time 1.233 (2.757)	Data 0.000 (1.457)	Loss_ce 1.283 / 2.158	Loss_ce_soft 1.078 / 3.106	Loss_tri_soft 0.108 / 0.133	Loss_lf 5.018 / 3.191	Loss_lf_tri 0.175 / 0.163	Prec 93.44% / 84.69%	
Epoch: [23][80/200]	Time 1.366 (2.399)	Data 0.000 (1.093)	Loss_ce 1.286 / 2.171	Loss_ce_soft 1.080 / 3.141	Loss_tri_soft 0.114 / 0.127	Loss_lf 5.016 / 3.158	Loss_lf_tri 0.178 / 0.157	Prec 92.89% / 84.84%	
Epoch: [23][100/200]	Time 1.166 (2.606)	Data 0.000 (1.315)	Loss_ce 1.284 / 2.192	Loss_ce_soft 1.089 / 3.114	Loss_tri_soft 0.125 / 0.110	Loss_lf 5.019 / 3.152	Loss_lf_tri 0.186 / 0.143	Prec 92.81% / 83.81%	
Epoch: [23][120/200]	Time 1.370 (2.743)	Data 0.001 (1.464)	Loss_ce 1.281 / 2.216	Loss_ce_soft 1.091 / 3.145	Loss_tri_soft 0.130 / 0.107	Loss_lf 5.034 / 3.147	Loss_lf_tri 0.187 / 0.139	Prec 93.07% / 83.65%	
Epoch: [23][140/200]	Time 1.352 (2.544)	Data 0.000 (1.255)	Loss_ce 1.269 / 2.224	Loss_ce_soft 1.073 / 3.154	Loss_tri_soft 0.124 / 0.109	Loss_lf 5.034 / 3.149	Loss_lf_tri 0.180 / 0.142	Prec 93.44% / 83.35%	
Epoch: [23][160/200]	Time 1.336 (2.694)	Data 0.000 (1.381)	Loss_ce 1.269 / 2.251	Loss_ce_soft 1.063 / 3.172	Loss_tri_soft 0.117 / 0.120	Loss_lf 5.033 / 3.151	Loss_lf_tri 0.174 / 0.152	Prec 93.32% / 82.93%	
Epoch: [23][180/200]	Time 1.339 (2.782)	Data 0.000 (1.465)	Loss_ce 1.272 / 2.263	Loss_ce_soft 1.060 / 3.179	Loss_tri_soft 0.115 / 0.117	Loss_lf 5.034 / 3.138	Loss_lf_tri 0.173 / 0.149	Prec 92.99% / 82.67%	
Epoch: [23][200/200]	Time 1.305 (2.859)	Data 0.000 (1.319)	Loss_ce 1.269 / 2.257	Loss_ce_soft 1.063 / 3.174	Loss_tri_soft 0.114 / 0.124	Loss_lf 5.038 / 3.143	Loss_lf_tri 0.172 / 0.151	Prec 93.03% / 82.69%	
Extract Features: [50/302]	Time 0.166 (1.065)	Data 0.001 (0.898)	
Extract Features: [100/302]	Time 0.165 (0.617)	Data 0.001 (0.449)	
Extract Features: [150/302]	Time 0.172 (0.467)	Data 0.001 (0.299)	
Extract Features: [200/302]	Time 0.168 (0.393)	Data 0.000 (0.225)	
Extract Features: [250/302]	Time 0.185 (0.348)	Data 0.000 (0.180)	
Extract Features: [300/302]	Time 0.160 (0.318)	Data 0.001 (0.150)	
Mean AP: 65.0%

 * Finished phase   4 epoch  23  model no.1 mAP: 65.0%  best: 66.1%

Computing original distance...
Computing Jaccard distance...
Time cost: 9.017105340957642
Clustering and labeling...

 Clustered into 107 classes 

###############################
Lamda for less forget is set to  25.11971337416094
###############################
lr of module.classifier.fc1.weight is 0
lr of module.classifier_max.fc1.weight is 0
Epoch: [24][20/200]	Time 1.300 (1.304)	Data 0.000 (0.000)	Loss_ce 1.268 / 2.080	Loss_ce_soft 0.989 / 3.170	Loss_tri_soft 0.157 / 0.110	Loss_lf 4.950 / 2.907	Loss_lf_tri 0.221 / 0.149	Prec 92.81% / 86.88%	
Epoch: [24][40/200]	Time 1.340 (2.443)	Data 0.000 (1.116)	Loss_ce 1.283 / 2.114	Loss_ce_soft 1.024 / 3.112	Loss_tri_soft 0.125 / 0.122	Loss_lf 4.988 / 3.056	Loss_lf_tri 0.197 / 0.166	Prec 92.19% / 85.31%	
Epoch: [24][60/200]	Time 1.345 (2.831)	Data 0.000 (1.493)	Loss_ce 1.270 / 2.198	Loss_ce_soft 1.003 / 3.191	Loss_tri_soft 0.125 / 0.126	Loss_lf 5.011 / 3.101	Loss_lf_tri 0.192 / 0.159	Prec 92.50% / 83.44%	
Epoch: [24][80/200]	Time 1.312 (2.992)	Data 0.000 (1.655)	Loss_ce 1.275 / 2.217	Loss_ce_soft 0.997 / 3.185	Loss_tri_soft 0.120 / 0.132	Loss_lf 5.005 / 3.112	Loss_lf_tri 0.192 / 0.162	Prec 92.19% / 83.12%	
Epoch: [24][100/200]	Time 1.345 (2.664)	Data 0.000 (1.324)	Loss_ce 1.282 / 2.220	Loss_ce_soft 1.002 / 3.165	Loss_tri_soft 0.124 / 0.137	Loss_lf 4.996 / 3.049	Loss_lf_tri 0.191 / 0.161	Prec 91.94% / 82.62%	
Epoch: [24][120/200]	Time 1.406 (2.817)	Data 0.000 (1.474)	Loss_ce 1.274 / 2.219	Loss_ce_soft 0.999 / 3.162	Loss_tri_soft 0.124 / 0.131	Loss_lf 5.002 / 3.058	Loss_lf_tri 0.193 / 0.157	Prec 92.29% / 83.18%	
Epoch: [24][140/200]	Time 1.413 (2.924)	Data 0.000 (1.578)	Loss_ce 1.270 / 2.247	Loss_ce_soft 1.001 / 3.171	Loss_tri_soft 0.135 / 0.123	Loss_lf 5.003 / 3.066	Loss_lf_tri 0.199 / 0.151	Prec 92.72% / 82.90%	
Epoch: [24][160/200]	Time 1.344 (3.001)	Data 0.001 (1.654)	Loss_ce 1.268 / 2.267	Loss_ce_soft 1.015 / 3.176	Loss_tri_soft 0.138 / 0.124	Loss_lf 4.999 / 3.092	Loss_lf_tri 0.203 / 0.151	Prec 92.85% / 82.54%	
Epoch: [24][180/200]	Time 1.393 (2.817)	Data 0.000 (1.470)	Loss_ce 1.273 / 2.286	Loss_ce_soft 1.023 / 3.187	Loss_tri_soft 0.138 / 0.136	Loss_lf 5.000 / 3.120	Loss_lf_tri 0.203 / 0.158	Prec 92.64% / 81.77%	
Epoch: [24][200/200]	Time 1.318 (3.115)	Data 0.000 (1.545)	Loss_ce 1.264 / 2.263	Loss_ce_soft 1.008 / 3.171	Loss_tri_soft 0.142 / 0.135	Loss_lf 4.992 / 3.095	Loss_lf_tri 0.207 / 0.157	Prec 92.78% / 82.41%	
Extract Features: [50/302]	Time 0.166 (1.070)	Data 0.000 (0.900)	
Extract Features: [100/302]	Time 0.169 (0.620)	Data 0.000 (0.450)	
Extract Features: [150/302]	Time 0.166 (0.469)	Data 0.000 (0.300)	
Extract Features: [200/302]	Time 0.165 (0.394)	Data 0.000 (0.225)	
Extract Features: [250/302]	Time 0.168 (0.349)	Data 0.000 (0.180)	
Extract Features: [300/302]	Time 0.157 (0.319)	Data 0.000 (0.150)	
Mean AP: 65.6%

 * Finished phase   4 epoch  24  model no.1 mAP: 65.6%  best: 66.1%

update proto_dataset
=> Loaded checkpoint 'logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase5_model_best.pth.tar'
mismatch: module.classifier.fc2.weight torch.Size([111, 2048]) torch.Size([107, 2048])
mismatch: module.classifier_max.fc2.weight torch.Size([111, 2048]) torch.Size([107, 2048])
missing keys in state_dict: {'module.classifier_max.fc2.weight', 'module.classifier.fc2.weight'}
Computing original distance...
Computing Jaccard distance...
Time cost: 9.185049533843994
Clustering and labeling...
phase 4 Clustered into 119 example classes 
delete by camera is 27
delete by cluster distance is 4,total num is 92
NMI of phase4 is 0.9544674977157264 
pickle into logs/dukemtmcTOmarket1501/resnet50_AB-ABMT-IC3/phase4_proto.pkl
